{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient decent method    \n",
    "The general framework for a gradient method for finding a minimizer of a function $f$ is defined by\n",
    "\\begin{align*} \n",
    "x^{k+1} = x^{(k)} -\\alpha_k \\nabla f(x_k),\\ \\ \\ \\ \\ \\ \\  k=0,1,2,...\n",
    "\\end{align*} \n",
    "where $\\alpha_k \\gt 0$ is known as the step size or learning rate.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression, the critical point has a closed-form solution. But that's not always the case especially for complicated functions. Thus, gradient decent is often used in machine learning.     \n",
    "\n",
    "From notebook 01, the gradient (first order derivative) is\n",
    "$$\n",
    "\\nabla_\\beta L(\\beta) = -2 X^T y + 2X^TX\\beta,\n",
    "$$\n",
    "\n",
    "The weight vector can be iteratively updated by doing:\n",
    "$$\n",
    "\\hat{\\beta}^{(k+1)} = \\hat{\\beta}^{(k)} - \\eta \\nabla_{\\beta} L(\\hat{\\beta}^{(k)}).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate some synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(8964)\n",
    "\n",
    "n = 200                                                       # number of samples\n",
    "# dimension of problem\n",
    "p = 6\n",
    "# noise standard deviation\n",
    "sigma = 0.2\n",
    "X = np.random.normal(0, 1, size=(n, p))                        # design matrix\n",
    "beta_star = np.random.randint(-4, 2, p)                       # true beta\n",
    "noise = np.random.normal(0, sigma, size=(n))\n",
    "y = X @ beta_star + noise                                     # y values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight vector $\\beta$ closed-form solution     \n",
    "From notebook 01:\n",
    "$$\n",
    "\\hat{\\beta} = (X^TX)^{-1}X^T y.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta hat using numpy:   [-1.0202166  -2.99123646 -1.00450705 -1.97897081 -1.00582651 -2.01509766]\n",
      "           true beta:   [-1 -3 -1 -2 -1 -2]\n"
     ]
    }
   ],
   "source": [
    "beta_hat_np = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(\"beta hat using numpy:  \", beta_hat_np)\n",
    "print(\"           true beta:  \", beta_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solve with `sklearn`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta hat using sklearn:   [-1.0202166  -2.99123646 -1.00450705 -1.97897081 -1.00582651 -2.01509766]\n",
      "             true beta:   [-1 -3 -1 -2 -1 -2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "beta_hat_sk = LinearRegression(fit_intercept=False).fit(X, y).coef_\n",
    "print(\"beta hat using sklearn:  \", beta_hat_sk)\n",
    "print(\"             true beta:  \", beta_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient decent(GD)    \n",
    "Take $\\eta=0.001$ as the step size (learning rate), and run GD T=50 iterations.\n",
    "\n",
    "As mentioned before, the gradient is\n",
    "$$\n",
    "\\nabla_\\beta L(\\beta) = -2 X^T y + 2X^TX\\beta,\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta hat using GD by hand:   [-1.02021658 -2.99123644 -1.00450708 -1.97897079 -1.00582652 -2.01509767]\n",
      "                true beta:   [-1 -3 -1 -2 -1 -2]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.001\n",
    "T = 50\n",
    "\n",
    "\n",
    "def grad_loss(b, X, y):\n",
    "    return -2 * X.T @ y + 2 * X.T @ X @ b\n",
    "\n",
    "\n",
    "# store the beta vec at each iteration\n",
    "betas = np.zeros(shape=(T, p))\n",
    "\n",
    "for t in range(1, T):\n",
    "    betas[t, :] = betas[t-1, :] - eta * grad_loss(betas[t-1, :], X, y)\n",
    "\n",
    "print(\"beta hat using GD by hand:  \", betas[T-1, :])\n",
    "print(\"                true beta:  \", beta_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GD with `PyTorch`    \n",
    "PyTorch is technically an `automatic differentiation (autodiff)` library, which means that it is able to compute gradients of functions numerically. PyTorch is primarily used for deep learning (which requires a form of differentiation called 'back propagation'). \n",
    "\n",
    "In `PyTorch` tensors are used rather than `NumPy` arrays. You can think of tensors as you do arrays, the only difference is that tensors allow you to compute gradients. It is important to note here that in the following code, there is no equivalent of the `grad_loss` function, which is taken care of automatically (by the `autodiff`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta hat using PyTorch:   tensor([-1.0200, -2.9910, -1.0046, -1.9788, -1.0057, -2.0151])\n",
      "             true beta:   [-1 -3 -1 -2 -1 -2]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# X data tensor\n",
    "X_tensor = torch.from_numpy(X).float()\n",
    "# y data tensor\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "# this is our parameter vector beta\n",
    "beta_tensor = torch.zeros(p, requires_grad=True)\n",
    "eta = 0.1\n",
    "T = 50\n",
    "\n",
    "for _ in range(T):\n",
    "    yhat = X_tensor @ beta_tensor             # prediction of model\n",
    "    residual = yhat - y_tensor                # residual tensor (errors)\n",
    "    loss = (residual**2).mean()               # mean squared error tensor\n",
    "\n",
    "    # this call computes gradients of `loss` w.r.t. any params (beta)tensor\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # we do not want this calculation to be part of the gradient computation\n",
    "        beta_tensor -= eta * beta_tensor.grad\n",
    "\n",
    "    # remove current gradients from being stored (important if you want to recompute gradients)\n",
    "    beta_tensor.grad.zero_()\n",
    "\n",
    "print(\"beta hat using PyTorch:  \", beta_tensor.data)\n",
    "print(\"             true beta:  \", beta_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GD with `PyTorch` a better way    \n",
    "Make use of more `PyTorch` functionality.   \n",
    "\n",
    "General procedures:\n",
    "1. For any problem you wish to solve using `PyTorch`, you should first specify a `model` which inherits from the `Module` class and defines the model of computation you wish to work with. Importantly, this class always contains a `forward` function that describes how an input should make its way through the computational graph.\n",
    "\n",
    "2. `PyTorch` already has a host of useful machine learning functions that can be imported. Many of these live in the `nn` module, which is short for `neural nets`.\n",
    "\n",
    "3. `PyTorch` already has a large number of optimizers (other numerical methods apart from GD) that we can use. These live in the `optim` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta hat using PyTorch w model:   tensor([-1.0200, -2.9910, -1.0046, -1.9788, -1.0057, -2.0151])\n",
      "                     true beta:   [-1 -3 -1 -2 -1 -2]\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# create the linear regression model class\n",
    "\n",
    "\n",
    "class LinearRegressionTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # need to wrap the params of the model in nn.Parameter\n",
    "        self.beta_tensor = nn.Parameter(torch.zeros(p, requires_grad=True))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X @ self.beta_tensor\n",
    "\n",
    "\n",
    "model = LinearRegressionTorch()                        # create a model instance\n",
    "loss_func = nn.MSELoss()                               # choose MSE loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=eta)      # choose Stochastic GD\n",
    "\n",
    "for _ in range(50):\n",
    "    yhat = model.forward(X_tensor)\n",
    "    loss = loss_func(y_tensor, yhat)\n",
    "    loss.backward()\n",
    "    optimizer.step()                                   # no need to manually update\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(\"beta hat using PyTorch w model:  \", model.beta_tensor.data)\n",
    "print(\"                     true beta:  \", beta_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient based optimization on Ridge model     \n",
    "Consider optimizing the following loss function of ridge model (w.r.t.  $x$):\n",
    "$$\n",
    "f(x)=\\frac{1}{2}\\|Ax-b\\|_2^2+\\frac{\\gamma}{2}\\|x\\|^2_2\n",
    "$$\n",
    "and where $A\\in \\mathbb{R}^{m\\times n}, b\\in \\mathbb{R}^m$ are defined as\n",
    "$$\n",
    "A=\\left [\n",
    "    \\begin{array}{cccc}\n",
    "    1&2&1&-1 \\\\\n",
    "    -1&1&0&2 \\\\\n",
    "    0&-1&-2&1 \\\\\n",
    "    \\end{array}\n",
    "\\right ]\n",
    ",\\ \\ \\ \\ \\ \\ \n",
    "b = \\left [\n",
    "    \\begin{array}{c}\n",
    "    3\\\\ 2\\\\ -2\\\\\n",
    "    \\end{array}\n",
    "\\right ]\n",
    "$$\n",
    "$\\gamma$ is a positive constant.    \n",
    "Run gradient decent on $f$ with step size $\\alpha = 0.1$, $\\gamma = 0.2$, and starting point of $x^{(0)}=(1,1,1,1)$. \n",
    "\n",
    "Terminate the algorithm when $\\|\\nabla f(x^{(k)})\\|_2 \\lt 0.001$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is \n",
    "\\begin{align*}\n",
    "\\nabla f(x) &= \\frac{\\partial}{\\partial x}f(x) = A^TAx-A^Tb+\\gamma x\n",
    "\\end{align*}\n",
    "So the GD steps are\n",
    "\\begin{align*}\n",
    "x^{(k+1)} &= x^{(k)} - \\alpha_k(A^TAx^{(k)}-A^Tb+\\gamma x^{(k)}) \\\\ \n",
    "&= x^{(k)} - \\alpha_k(A^T(Ax^{(k)}-b)+\\gamma x^{(k)}) \\\\ \n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 iterations: \n",
      "k=0,\t[1 1 1 1]\n",
      "k=1,\t[0.98 0.98 0.98 0.98]\n",
      "k=2,\t[0.9624 0.9804 0.9744 0.9584]\n",
      "k=3,\t[0.942712 0.982392 0.966832 0.943272]\n",
      "k=4,\t[0.92337456 0.98663216 0.95983296 0.92954576]\n",
      "k=5,\t[0.90444937 0.99160416 0.95259323 0.91685286]\n",
      "Last 5 iterations: \n",
      "k=272,\t[0.06662967 1.33658172 0.49283021 0.32508074]\n",
      "k=273,\t[0.06655417 1.33661408 0.49278707 0.32502682]\n",
      "k=274,\t[0.06648018 1.33664579 0.49274479 0.32497397]\n",
      "k=275,\t[0.06640768 1.33667686 0.49270336 0.32492218]\n",
      "k=276,\t[0.06633662 1.33670732 0.49266275 0.32487142]\n"
     ]
    }
   ],
   "source": [
    "# Solve manually\n",
    "A = np.array([\n",
    "    [1, 2, 1, -1],\n",
    "    [-1, 1, 0, 2],\n",
    "    [0, -1, -2, 1]\n",
    "])\n",
    "b = np.array([3, 2, -2])\n",
    "gamma = 0.2\n",
    "alpha = 0.1\n",
    "tol = 0.001\n",
    "\n",
    "\n",
    "def grad_f(x):\n",
    "    return A.T @ (A@x - b) + gamma * x\n",
    "\n",
    "\n",
    "x = np.array([1, 1, 1, 1])\n",
    "k = 0\n",
    "x_k = [x]\n",
    "while np.linalg.norm(grad_f(x)) > tol:\n",
    "    x = x - alpha * grad_f(x)\n",
    "    k += 1\n",
    "    x_k.append(x)\n",
    "\n",
    "print(\"First 5 iterations: \")\n",
    "for i in range(6):\n",
    "    print(f\"k={i},\\t{x_k[i]}\")\n",
    "\n",
    "print(\"Last 5 iterations: \")\n",
    "for i in range(5):\n",
    "    print(f\"k={k - 4 + i},\\t{x_k[k - 4 + i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0,\ttensor([1., 1., 1., 1.])\n",
      "k=20,\ttensor([0.6772, 1.0758, 0.8405, 0.7608])\n",
      "k=40,\ttensor([0.4726, 1.1626, 0.7248, 0.6150])\n",
      "k=60,\ttensor([0.3364, 1.2210, 0.6470, 0.5177])\n",
      "k=80,\ttensor([0.2455, 1.2599, 0.5950, 0.4528])\n",
      "k=100,\ttensor([0.1848, 1.2860, 0.5603, 0.4095])\n",
      "k=120,\ttensor([0.1442, 1.3033, 0.5372, 0.3805])\n",
      "k=140,\ttensor([0.1172, 1.3149, 0.5217, 0.3612])\n",
      "k=160,\ttensor([0.0991, 1.3227, 0.5114, 0.3483])\n",
      "k=180,\ttensor([0.0871, 1.3278, 0.5045, 0.3397])\n",
      "k=200,\ttensor([0.0790, 1.3313, 0.4999, 0.3339])\n",
      "k=220,\ttensor([0.0736, 1.3336, 0.4968, 0.3301])\n",
      "k=240,\ttensor([0.0701, 1.3351, 0.4948, 0.3275])\n",
      "k=260,\ttensor([0.0677, 1.3361, 0.4934, 0.3258])\n",
      "k=277,\ttensor([0.0663, 1.3367, 0.4926, 0.3248])\n"
     ]
    }
   ],
   "source": [
    "# Solve with PyTorch\n",
    "from turtle import forward\n",
    "\n",
    "\n",
    "A_tensor = torch.from_numpy(A).float()\n",
    "b_tensor = torch.from_numpy(b).float()\n",
    "\n",
    "\n",
    "class RidgeModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.x = nn.Parameter(torch.ones(4, requires_grad=True))\n",
    "\n",
    "    def forward(self, A):\n",
    "        return A @ self.x\n",
    "\n",
    "\n",
    "model = RidgeModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=alpha)\n",
    "\n",
    "k = 0\n",
    "while True:\n",
    "    if k % 20 == 0:\n",
    "        print(f\"{k=},\\t{model.x.data}\")\n",
    "    y = model.forward(A_tensor)\n",
    "    loss = 0.5 * torch.linalg.norm(y - b_tensor, ord=2)**2 + \\\n",
    "        gamma * 0.5 * torch.linalg.norm(model.x, ord=2)**2\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    k += 1\n",
    "    if torch.linalg.norm(model.x.grad) <= tol:\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(f\"k={k},\\t{model.x.data}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('COMP9417')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2d5407234b8938a089e01a60a45d2aae9d8d06e85dfd57bc61d76669c88389c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
