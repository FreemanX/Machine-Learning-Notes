{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch basics   \n",
    "**Tensor** is a fundamental structure in PyTorch which is very similar to an array or matrix. Tensors are used to encode the inputs and outputs of a model, as well as the modelâ€™s parameters. \n",
    "##### Create Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2, 3,)\n",
    "x = torch.rand(shape)\n",
    "y = torch.rand(shape)\n",
    "z = torch.zeros(shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = :\n",
      "tensor([[0.1506, 0.9966, 0.0523],\n",
      "        [0.2807, 0.2023, 0.9067]])\n",
      "y = :\n",
      "tensor([[0.5453, 0.2876, 0.7285],\n",
      "        [0.3665, 0.1064, 0.8783]])\n",
      "\n",
      "\n",
      "x + y = :\n",
      "tensor([[0.6960, 1.2842, 0.7808],\n",
      "        [0.6472, 0.3087, 1.7850]])\n"
     ]
    }
   ],
   "source": [
    "print(\"x = :\")\n",
    "print(x)\n",
    "print(\"y = :\")\n",
    "print(y)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"x + y = :\")\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6960, 1.2842, 0.7808],\n",
      "        [0.6472, 0.3087, 1.7850]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0.6960, 1.2842],\n",
      "        [0.7808, 0.6472],\n",
      "        [0.3087, 1.7850]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(z)\n",
    "print(z.shape)\n",
    "\n",
    "z = z.reshape([3, 2])\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1675, 0.5401, 0.4135],\n",
      "        [0.8770, 0.6943, 0.0997]])\n",
      "torch.Size([2, 3])\n",
      "\n",
      "\n",
      "z and its shape after the flatten operation:\n",
      "tensor([0.1675, 0.5401, 0.4135, 0.8770, 0.6943, 0.0997])\n",
      "torch.Size([6])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand([2, 3])\n",
    "\n",
    "print(z)\n",
    "print(z.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "z = torch.flatten(z)\n",
    "print(\"z and its shape after the flatten operation:\")\n",
    "print(z)\n",
    "print(z.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4091, 0.8527, 0.5423],\n",
      "        [0.8766, 0.4200, 0.8596]])\n",
      "torch.Size([2, 3])\n",
      "\n",
      "\n",
      "z and its shape after the transpose operation:\n",
      "tensor([[0.4091, 0.8766],\n",
      "        [0.8527, 0.4200],\n",
      "        [0.5423, 0.8596]])\n",
      "torch.Size([3, 2])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand([2, 3])\n",
    "print(z)\n",
    "print(z.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "z = torch.transpose(z, 0, 1)\n",
    "print(\"z and its shape after the transpose operation:\")\n",
    "print(z)\n",
    "print(z.shape)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4926],\n",
      "         [0.5932],\n",
      "         [0.9819],\n",
      "         [0.1368],\n",
      "         [0.4672]],\n",
      "\n",
      "        [[0.8629],\n",
      "         [0.8223],\n",
      "         [0.9654],\n",
      "         [0.2195],\n",
      "         [0.9839]],\n",
      "\n",
      "        [[0.8768],\n",
      "         [0.5181],\n",
      "         [0.9418],\n",
      "         [0.0854],\n",
      "         [0.0494]]])\n",
      "torch.Size([3, 5, 1])\n",
      "z and its shape after permutation\n",
      "tensor([[[0.4926, 0.5932, 0.9819, 0.1368, 0.4672],\n",
      "         [0.8629, 0.8223, 0.9654, 0.2195, 0.9839],\n",
      "         [0.8768, 0.5181, 0.9418, 0.0854, 0.0494]]])\n",
      "torch.Size([1, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Permute the dimensions of z according to the specified order (exchanging axes).\n",
    "\"\"\"\n",
    "# Create a new tensor\n",
    "shape = (3,5,1)\n",
    "z = torch.rand(shape)\n",
    "order = [2,0,1]\n",
    "print(z)\n",
    "print(z.shape)\n",
    "\n",
    "z = torch.permute(z, (2, 0, 1))  #  Returns a view of the original tensor input with its dimensions permuted.\n",
    "\n",
    "print(\"z and its shape after permutation\")\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product of u and v:\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Create two vectors\n",
    "v = torch.tensor([2, 3])\n",
    "u = torch.tensor([2, 1])\n",
    "\n",
    "result = v @ u\n",
    "\n",
    "print(\"The dot product of u and v:\")\n",
    "print(result.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6335, 0.7177, 0.3232],\n",
      "        [0.4099, 0.0371, 0.5935]])\n",
      "tensor([[0.4223, 0.0444, 0.9044],\n",
      "        [0.4609, 0.6270, 0.7179]])\n",
      "The Concatenated tensor z of (x, y)\n",
      "tensor([[0.6335, 0.7177, 0.3232, 0.4223, 0.0444, 0.9044],\n",
      "        [0.4099, 0.0371, 0.5935, 0.4609, 0.6270, 0.7179]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "x = torch.rand(shape)\n",
    "y = torch.rand(shape)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "z = torch.concat((x, y), dim=1)\n",
    "\n",
    "print(\"The Concatenated tensor z of (x, y)\")\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Structure of a PyTorch Program\n",
    "\n",
    "```python\n",
    "# Create neural network according to model specification\n",
    "net = MyModel().to(device) # CPU or GPU\n",
    "\n",
    "# Prepare to load the training and test data\n",
    "train_loader = torch.utils.data.DataLoader(...)\n",
    "test_loader = torch.utils.data.DataLoader(...)\n",
    "\n",
    "# Choose an optimizer: SGD, Adam, or others\n",
    "optimizer = torch.optim.SGD(net.parameters, ...)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs):\n",
    "    train(params, net, device, train_loader, optimizer)\n",
    "    # Periodically evaluate the network on the test data\n",
    "    if epoch % 10 == 0:\n",
    "        test(params, net, device, test_loader)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a model\n",
    "\n",
    "```python\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # define the structure of the network here\n",
    "\n",
    "    def forward(self, input):\n",
    "        # apply network and return output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Custom Model    \n",
    "\n",
    "Example: define a function $f(x, y) \\rightarrow Ax\\log(y) + By^2$\n",
    "\n",
    "```python\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.A = nn.Parameter(torch.randn((1), requires_grad=True))\n",
    "        self.B = nn.Parameter(torch.randn((1), requires_grad=True))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.A * input[:, 0] * torch.log(input[:, 1]) \\\n",
    "            + self.B * input[:, 1] * input[:, 1]\n",
    "        return output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Net from Individual Components\n",
    "\n",
    "```python\n",
    "class MyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.in_to_hid = torch.nn.Linear(2,2)\n",
    "        self.hid_to_out = torch.nn.Linear(2,1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hid_sum = self.in_to_hid(input)\n",
    "        hidden = torch.tanh(hid_sum)\n",
    "        out_sum = self.hid_to_out(hidden)\n",
    "        output = torch.sigmoid(out_sum)\n",
    "        return output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Sequential Network\n",
    "\n",
    "```python\n",
    "class MyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_input, num_hid, num_out):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(num_input, num_hid),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hid, num_out),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "```\n",
    "\n",
    "#### Sequential Components\n",
    "\n",
    "Network Layers:\n",
    "* `nn.Linear()`\n",
    "* `nn.Conv2d()`\n",
    "\n",
    "Intermediate Operators:\n",
    "* `nn.Dropout()`\n",
    "* `nn.BatchNorm()`\n",
    "\n",
    "Activation Functions:\n",
    "* `nn.Sigmoid()`\n",
    "* `nn.Tanh()`\n",
    "* `nn.ReLU()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring Data Explicitly\n",
    "\n",
    "```python\n",
    "import torch.utils.data\n",
    "\n",
    "# input and target values for the XOR task \n",
    "input = torch.Tensor([[0,0],[0,1],[1,0],[1,1]])\n",
    "target = torch.Tensor([[0],[1],[1],[0]])\n",
    "\n",
    "xdata = torch.utils.data.TensorDataset(input, target)\n",
    "train_loader = torch.utils.data.DataLoader(xdata, batch_size=4)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from a .csv File\n",
    "\n",
    "```python\n",
    "import pandas as pd \n",
    "\n",
    "pd = pd.read_csv('sonar.all-data.csv')\n",
    "# pre-process raw data\n",
    "df = df.replace('R', 0)\n",
    "df = df.replace('M', 1)\n",
    "# Convert to torch tensor\n",
    "data = torch.tensor(df.values, dtype=torch.float32)\n",
    "# Split feature and target\n",
    "num_input = data.shape[1] - 1\n",
    "features = data[:, 0:num_input]\n",
    "target = data[:, num_input:num_input+1]\n",
    "# Create dataset\n",
    "dataset = torch.utils.data.TensorDataset(features, target)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Custom Datasets\n",
    "\n",
    "```python\n",
    "from data import ImageFolder\n",
    "# load images from a specified directory\n",
    "dataset = ImageFolder(folder, transform)\n",
    "\n",
    "from torchvision import datasets\n",
    "# download popular image datasets remotely\n",
    "mnist = datasets.MNIST(...)\n",
    "cifar10 = datasets.CIFAR10(...)\n",
    "celebA = datasets.CelebA(...)\n",
    "... \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing an Optimizer\n",
    "\n",
    "```python\n",
    "# Stochastic Gradient Descent\n",
    "optimizer = torch.optim.SGD(\n",
    "    net.parameters(),\n",
    "    lr=0.01,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0001\n",
    ")\n",
    "# Adam = Adaptive Moment Estimation\n",
    "optimizer = torch.optim.Adam(\n",
    "    net.parameters(), \n",
    "    eps=0.000001,\n",
    "    lr=0.01,\n",
    "    betas=(0.5, 0.999),\n",
    "    weight_decay=0.0001 \n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "```python\n",
    "import torch.nn.functional as F\n",
    "def train(args, net, device, train_loader, optimizer):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # zero gradients\n",
    "        output = net(data)     # apply network\n",
    "        loss = ...             # calculate loss\n",
    "        loss.backward()        # update gradients\n",
    "        optimizer.step()       # update weights \n",
    "```\n",
    "\n",
    "#### Common Loss Functions\n",
    "\n",
    "* `loss = torch.sum((output-target)*(output-target))`\n",
    "* `loss = F.nll_loss(output,target)`             \n",
    "* `loss = F.binary_cross_entropy(output,target) `\n",
    "* `loss = F.softmax(output,dim=1)              `\n",
    "\n",
    "* `loss = F.log_softmax(output,dim=1)          `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graphs\n",
    "\n",
    "PyTorch automatically builds a computational graph, enabling it to backpropagate derivatives.\n",
    "\n",
    "Every parameter has `.data` and `.grad` components, e.g.:\n",
    "\n",
    "```python\n",
    "A.data\n",
    "A.grad\n",
    "```\n",
    "\n",
    "`optimizer.zero_grad()` sets all `.grad` components to zero.\n",
    "`loss.backward()` updates the `.grad` component of all parameters by backpropagating gradients through the computational graph.\n",
    "`optimizer.step()` updates the `.data` components.\n",
    "\n",
    "### Controlling the Computational Graph\n",
    "\n",
    "If we need to stop the gradients from being backpropagated through a certain variable (or expression) A, we can exclude it from the computational graph by using `A.detach()`\n",
    "\n",
    "By default, `loss.backward()` discards the computational graph after computing the gradients. If needed, we can force it to keep the computational graph by `loss.backward(retain_graph=True)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "```python\n",
    "def test(args, net, device, test_loader):\n",
    "    with torch.no_grad():       # don't calculate gradients\n",
    "        net.eval()              # toggle dropout, batch norm\n",
    "        test_loss = 0\n",
    "        for data, target in test_loader:\n",
    "            output = net(data)\n",
    "            test_loss += ...\n",
    "        print(test_loss)\n",
    "        net.train()             # toggle dropout, batch norm to training mode \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PyTorch  \n",
    "\n",
    "Toy example 1: train $f(x)=Ax$ s.t. $f(1) = 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.A = torch.nn.Parameter(torch.zeros(1, requires_grad=True))\n",
    "\n",
    "    def forward(self, X):\n",
    "        pred = self.A * X\n",
    "        return pred\n",
    "\n",
    "\n",
    "X = torch.Tensor([[1]])\n",
    "Y = torch.Tensor([[1]])\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.01  \tepoch=998  \tm.A.data=tensor([1.0000])\n",
      "lr=0.1  \tepoch=97  \tm.A.data=tensor([1.0000])\n",
      "lr=0.5  \tepoch=16  \tm.A.data=tensor([1.0000])\n",
      "lr=1.0  \tepoch=2  \tm.A.data=tensor([1.])\n",
      "lr=1.5  \tepoch=16  \tm.A.data=tensor([1.0000])\n",
      "lr=1.9  \tepoch=97  \tm.A.data=tensor([1.0000])\n",
      "lr=2.0  \tepoch=1000  \tm.A.data=tensor([0.])\n",
      "lr=2.1  \tepoch=927  \tm.A.data=tensor([nan])\n"
     ]
    }
   ],
   "source": [
    "def train(m, optimizer, epochs, verbose=False):\n",
    "    epoch = 0\n",
    "    for _ in range(epochs):\n",
    "        epoch += 1\n",
    "        for batch_id, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            pred = m(x)\n",
    "            loss = 0.5 * torch.mean((pred - y)*(pred - y))\n",
    "            if verbose:\n",
    "                if type(m.A.grad) == type(None):\n",
    "                    print('Ep%3d: zero_grad(): A.grad=  None  A.data=%7.4f loss=%7.4f'\n",
    "                          % (epoch, m.A.data, loss))\n",
    "                else:\n",
    "                    print('Ep%3d: zero_grad(): A.grad=%7.4f A.data=%7.4f loss=%7.4f'\n",
    "                          % (epoch, m.A.grad, m.A.data, loss))\n",
    "            loss.backward()       # compute gradients\n",
    "            optimizer.step()      # update weights\n",
    "            if verbose:\n",
    "                print('            step(): A.grad=%7.4f A.data=%7.4f'\n",
    "                      % (m.A.grad, m.A.data))\n",
    "            if loss < 0.000000001 or np.isnan(loss.data):\n",
    "                return epoch\n",
    "    return epoch\n",
    "\n",
    "\n",
    "mom = 0\n",
    "lr_lst = [0.01, 0.1, 0.5, 1.0, 1.5, 1.9, 2.0, 2.1]\n",
    "# lr_lst = [1.0]\n",
    "epochs = 1000\n",
    "\n",
    "for lr in lr_lst:\n",
    "    m = MyModel().to('cpu')\n",
    "    optimizer = torch.optim.SGD(m.parameters(), lr=lr, momentum=mom)\n",
    "    epoch = train(m, optimizer, epochs)\n",
    "    print(f'{lr=}  \\t{epoch=}  \\t{m.A.data=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mom=0.1  \tepoch=25  \tm.A.data=tensor([1.0000])\n",
      "mom=0.2  \tepoch=14  \tm.A.data=tensor([1.0000])\n",
      "mom=0.3  \tepoch=13  \tm.A.data=tensor([1.0005])\n",
      "mom=0.4  \tepoch=22  \tm.A.data=tensor([0.9999])\n",
      "mom=0.5  \tepoch=30  \tm.A.data=tensor([1.0000])\n",
      "mom=0.6  \tepoch=37  \tm.A.data=tensor([1.0001])\n",
      "mom=0.7  \tepoch=48  \tm.A.data=tensor([0.9997])\n",
      "mom=0.8  \tepoch=71  \tm.A.data=tensor([1.0005])\n",
      "mom=0.9  \tepoch=192  \tm.A.data=tensor([1.0000])\n"
     ]
    }
   ],
   "source": [
    "lr = 1.9\n",
    "mom_lst = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "epochs = 1000\n",
    "\n",
    "for mom in mom_lst:\n",
    "    m = MyModel().to('cpu')\n",
    "    optimizer = torch.optim.SGD(m.parameters(), lr=lr, momentum=mom)\n",
    "    epoch = train(m, optimizer, epochs)\n",
    "    print(f'{mom=}  \\t{epoch=}  \\t{m.A.data=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep  1: zero_grad(): A.grad=  None  A.data= 0.0000 loss= 0.5000\n",
      "            step(): A.grad=-1.0000 A.data= 1.9000\n",
      "Ep  2: zero_grad(): A.grad= 0.0000 A.data= 1.9000 loss= 0.4050\n",
      "            step(): A.grad= 0.9000 A.data= 2.0900\n",
      "Ep  3: zero_grad(): A.grad= 0.0000 A.data= 2.0900 loss= 0.5940\n",
      "            step(): A.grad= 1.0900 A.data= 0.2090\n",
      "Ep  4: zero_grad(): A.grad= 0.0000 A.data= 0.2090 loss= 0.3128\n",
      "            step(): A.grad=-0.7910 A.data=-0.1691\n",
      "Ep  5: zero_grad(): A.grad= 0.0000 A.data=-0.1691 loss= 0.6834\n",
      "            step(): A.grad=-1.1691 A.data= 1.6741\n",
      "Ep  6: zero_grad(): A.grad= 0.0000 A.data= 1.6741 loss= 0.2272\n",
      "            step(): A.grad= 0.6741 A.data= 2.2365\n",
      "Ep  7: zero_grad(): A.grad= 0.0000 A.data= 2.2365 loss= 0.7645\n",
      "            step(): A.grad= 1.2365 A.data= 0.4496\n",
      "Ep  8: zero_grad(): A.grad= 0.0000 A.data= 0.4496 loss= 0.1515\n",
      "            step(): A.grad=-0.5504 A.data=-0.2916\n",
      "Ep  9: zero_grad(): A.grad= 0.0000 A.data=-0.2916 loss= 0.8341\n",
      "            step(): A.grad=-1.2916 A.data= 1.4213\n",
      "Ep 10: zero_grad(): A.grad= 0.0000 A.data= 1.4213 loss= 0.0887\n",
      "            step(): A.grad= 0.4213 A.data= 2.3337\n",
      "Ep 11: zero_grad(): A.grad= 0.0000 A.data= 2.3337 loss= 0.8894\n",
      "            step(): A.grad= 1.3337 A.data= 0.7121\n",
      "Ep 12: zero_grad(): A.grad= 0.0000 A.data= 0.7121 loss= 0.0414\n",
      "            step(): A.grad=-0.2879 A.data=-0.3625\n",
      "Ep 13: zero_grad(): A.grad= 0.0000 A.data=-0.3625 loss= 0.9282\n",
      "            step(): A.grad=-1.3625 A.data= 1.1517\n",
      "Ep 14: zero_grad(): A.grad= 0.0000 A.data= 1.1517 loss= 0.0115\n",
      "            step(): A.grad= 0.1517 A.data= 2.3776\n",
      "Ep 15: zero_grad(): A.grad= 0.0000 A.data= 2.3776 loss= 0.9489\n",
      "            step(): A.grad= 1.3776 A.data= 0.9861\n",
      "Ep 16: zero_grad(): A.grad= 0.0000 A.data= 0.9861 loss= 0.0001\n",
      "            step(): A.grad=-0.0139 A.data=-0.3790\n",
      "Ep 17: zero_grad(): A.grad= 0.0000 A.data=-0.3790 loss= 0.9509\n",
      "            step(): A.grad=-1.3790 A.data= 0.8760\n",
      "Ep 18: zero_grad(): A.grad= 0.0000 A.data= 0.8760 loss= 0.0077\n",
      "            step(): A.grad=-0.1240 A.data= 2.3666\n",
      "Ep 19: zero_grad(): A.grad= 0.0000 A.data= 2.3666 loss= 0.9338\n",
      "            step(): A.grad= 1.3666 A.data= 1.2607\n",
      "Ep 20: zero_grad(): A.grad= 0.0000 A.data= 1.2607 loss= 0.0340\n",
      "            step(): A.grad= 0.2607 A.data=-0.3406\n",
      "Ep 21: zero_grad(): A.grad= 0.0000 A.data=-0.3406 loss= 0.8986\n",
      "            step(): A.grad=-1.3406 A.data= 0.6053\n",
      "Ep 22: zero_grad(): A.grad= 0.0000 A.data= 0.6053 loss= 0.0779\n",
      "            step(): A.grad=-0.3947 A.data= 2.3011\n",
      "Ep 23: zero_grad(): A.grad= 0.0000 A.data= 2.3011 loss= 0.8464\n",
      "            step(): A.grad= 1.3011 A.data= 1.5248\n",
      "Ep 24: zero_grad(): A.grad= 0.0000 A.data= 1.5248 loss= 0.1377\n",
      "            step(): A.grad= 0.5248 A.data=-0.2486\n",
      "Ep 25: zero_grad(): A.grad= 0.0000 A.data=-0.2486 loss= 0.7795\n",
      "            step(): A.grad=-1.2486 A.data= 0.3503\n",
      "Ep 26: zero_grad(): A.grad= 0.0000 A.data= 0.3503 loss= 0.2110\n",
      "            step(): A.grad=-0.6497 A.data= 2.1836\n",
      "Ep 27: zero_grad(): A.grad= 0.0000 A.data= 2.1836 loss= 0.7005\n",
      "            step(): A.grad= 1.1836 A.data= 1.7681\n",
      "Ep 28: zero_grad(): A.grad= 0.0000 A.data= 1.7681 loss= 0.2950\n",
      "            step(): A.grad= 0.7681 A.data=-0.1068\n",
      "Ep 29: zero_grad(): A.grad= 0.0000 A.data=-0.1068 loss= 0.6125\n",
      "            step(): A.grad=-1.1068 A.data= 0.1213\n",
      "Ep 30: zero_grad(): A.grad= 0.0000 A.data= 0.1213 loss= 0.3861\n",
      "            step(): A.grad=-0.8787 A.data= 2.0190\n",
      "Ep 31: zero_grad(): A.grad= 0.0000 A.data= 2.0190 loss= 0.5191\n",
      "            step(): A.grad= 1.0190 A.data= 1.9806\n",
      "Ep 32: zero_grad(): A.grad= 0.0000 A.data= 1.9806 loss= 0.4808\n",
      "            step(): A.grad= 0.9806 A.data= 0.0791\n",
      "Ep 33: zero_grad(): A.grad= 0.0000 A.data= 0.0791 loss= 0.4240\n",
      "            step(): A.grad=-0.9209 A.data=-0.0727\n",
      "Ep 34: zero_grad(): A.grad= 0.0000 A.data=-0.0727 loss= 0.5754\n",
      "            step(): A.grad=-1.0727 A.data= 1.8136\n",
      "Ep 35: zero_grad(): A.grad= 0.0000 A.data= 1.8136 loss= 0.3310\n",
      "            step(): A.grad= 0.8136 A.data= 2.1541\n",
      "Ep 36: zero_grad(): A.grad= 0.0000 A.data= 2.1541 loss= 0.6660\n",
      "            step(): A.grad= 1.1541 A.data= 0.3018\n",
      "Ep 37: zero_grad(): A.grad= 0.0000 A.data= 0.3018 loss= 0.2438\n",
      "            step(): A.grad=-0.6982 A.data=-0.2239\n",
      "Ep 38: zero_grad(): A.grad= 0.0000 A.data=-0.2239 loss= 0.7490\n",
      "            step(): A.grad=-1.2239 A.data= 1.5758\n",
      "Ep 39: zero_grad(): A.grad= 0.0000 A.data= 1.5758 loss= 0.1658\n",
      "            step(): A.grad= 0.5758 A.data= 2.2815\n",
      "Ep 40: zero_grad(): A.grad= 0.0000 A.data= 2.2815 loss= 0.8211\n",
      "            step(): A.grad= 1.2815 A.data= 0.5523\n",
      "Ep 41: zero_grad(): A.grad= 0.0000 A.data= 0.5523 loss= 0.1002\n",
      "            step(): A.grad=-0.4477 A.data=-0.3263\n",
      "Ep 42: zero_grad(): A.grad= 0.0000 A.data=-0.3263 loss= 0.8795\n",
      "            step(): A.grad=-1.3263 A.data= 1.3151\n",
      "Ep 43: zero_grad(): A.grad= 0.0000 A.data= 1.3151 loss= 0.0496\n",
      "            step(): A.grad= 0.3151 A.data= 2.3578\n",
      "Ep 44: zero_grad(): A.grad= 0.0000 A.data= 2.3578 loss= 0.9218\n",
      "            step(): A.grad= 1.3578 A.data= 0.8207\n",
      "Ep 45: zero_grad(): A.grad= 0.0000 A.data= 0.8207 loss= 0.0161\n",
      "            step(): A.grad=-0.1793 A.data=-0.3757\n",
      "Ep 46: zero_grad(): A.grad= 0.0000 A.data=-0.3757 loss= 0.9463\n",
      "            step(): A.grad=-1.3757 A.data= 1.0417\n",
      "Ep 47: zero_grad(): A.grad= 0.0000 A.data= 1.0417 loss= 0.0009\n",
      "            step(): A.grad= 0.0417 A.data= 2.3799\n",
      "Ep 48: zero_grad(): A.grad= 0.0000 A.data= 2.3799 loss= 0.9520\n",
      "            step(): A.grad= 1.3799 A.data= 1.0963\n",
      "Ep 49: zero_grad(): A.grad= 0.0000 A.data= 1.0963 loss= 0.0046\n",
      "            step(): A.grad= 0.0963 A.data=-0.3702\n",
      "Ep 50: zero_grad(): A.grad= 0.0000 A.data=-0.3702 loss= 0.9388\n",
      "            step(): A.grad=-1.3702 A.data= 0.7667\n",
      "Ep 51: zero_grad(): A.grad= 0.0000 A.data= 0.7667 loss= 0.0272\n",
      "            step(): A.grad=-0.2333 A.data= 2.3469\n",
      "Ep 52: zero_grad(): A.grad= 0.0000 A.data= 2.3469 loss= 0.9071\n",
      "            step(): A.grad= 1.3469 A.data= 1.3680\n",
      "Ep 53: zero_grad(): A.grad= 0.0000 A.data= 1.3680 loss= 0.0677\n",
      "            step(): A.grad= 0.3680 A.data=-0.3101\n",
      "Ep 54: zero_grad(): A.grad= 0.0000 A.data=-0.3101 loss= 0.8582\n",
      "            step(): A.grad=-1.3101 A.data= 0.5010\n",
      "Ep 55: zero_grad(): A.grad= 0.0000 A.data= 0.5010 loss= 0.1245\n",
      "            step(): A.grad=-0.4990 A.data= 2.2602\n",
      "Ep 56: zero_grad(): A.grad= 0.0000 A.data= 2.2602 loss= 0.7941\n",
      "            step(): A.grad= 1.2602 A.data= 1.6250\n",
      "Ep 57: zero_grad(): A.grad= 0.0000 A.data= 1.6250 loss= 0.1953\n",
      "            step(): A.grad= 0.6250 A.data=-0.1977\n",
      "Ep 58: zero_grad(): A.grad= 0.0000 A.data=-0.1977 loss= 0.7172\n",
      "            step(): A.grad=-1.1977 A.data= 0.2552\n",
      "Ep 59: zero_grad(): A.grad= 0.0000 A.data= 0.2552 loss= 0.2774\n",
      "            step(): A.grad=-0.7448 A.data= 2.1232\n",
      "Ep 60: zero_grad(): A.grad= 0.0000 A.data= 2.1232 loss= 0.6308\n",
      "            step(): A.grad= 1.1232 A.data= 1.8571\n",
      "Ep 61: zero_grad(): A.grad= 0.0000 A.data= 1.8571 loss= 0.3673\n",
      "            step(): A.grad= 0.8571 A.data=-0.0375\n",
      "Ep 62: zero_grad(): A.grad= 0.0000 A.data=-0.0375 loss= 0.5382\n",
      "            step(): A.grad=-1.0375 A.data= 0.0391\n",
      "Ep 63: zero_grad(): A.grad= 0.0000 A.data= 0.0391 loss= 0.4616\n",
      "            step(): A.grad=-0.9609 A.data= 1.9414\n",
      "Ep 64: zero_grad(): A.grad= 0.0000 A.data= 1.9414 loss= 0.4431\n",
      "            step(): A.grad= 0.9414 A.data= 2.0550\n",
      "Ep 65: zero_grad(): A.grad= 0.0000 A.data= 2.0550 loss= 0.5565\n",
      "            step(): A.grad= 1.0550 A.data= 0.1641\n",
      "Ep 66: zero_grad(): A.grad= 0.0000 A.data= 0.1641 loss= 0.3494\n",
      "            step(): A.grad=-0.8359 A.data=-0.1386\n",
      "Ep 67: zero_grad(): A.grad= 0.0000 A.data=-0.1386 loss= 0.6482\n",
      "            step(): A.grad=-1.1386 A.data= 1.7221\n",
      "Ep 68: zero_grad(): A.grad= 0.0000 A.data= 1.7221 loss= 0.2607\n",
      "            step(): A.grad= 0.7221 A.data= 2.2108\n",
      "Ep 69: zero_grad(): A.grad= 0.0000 A.data= 2.2108 loss= 0.7330\n",
      "            step(): A.grad= 1.2108 A.data= 0.3990\n",
      "Ep 70: zero_grad(): A.grad= 0.0000 A.data= 0.3990 loss= 0.1806\n",
      "            step(): A.grad=-0.6010 A.data=-0.2709\n",
      "Ep 71: zero_grad(): A.grad= 0.0000 A.data=-0.2709 loss= 0.8076\n",
      "            step(): A.grad=-1.2709 A.data= 1.4739\n",
      "Ep 72: zero_grad(): A.grad= 0.0000 A.data= 1.4739 loss= 0.1123\n",
      "            step(): A.grad= 0.4739 A.data= 2.3183\n",
      "Ep 73: zero_grad(): A.grad= 0.0000 A.data= 2.3183 loss= 0.8689\n",
      "            step(): A.grad= 1.3183 A.data= 0.6579\n",
      "Ep 74: zero_grad(): A.grad= 0.0000 A.data= 0.6579 loss= 0.0585\n",
      "            step(): A.grad=-0.3421 A.data=-0.3525\n",
      "Ep 75: zero_grad(): A.grad= 0.0000 A.data=-0.3525 loss= 0.9146\n",
      "            step(): A.grad=-1.3525 A.data= 1.2068\n",
      "Ep 76: zero_grad(): A.grad= 0.0000 A.data= 1.2068 loss= 0.0214\n",
      "            step(): A.grad= 0.2068 A.data= 2.3732\n",
      "Ep 77: zero_grad(): A.grad= 0.0000 A.data= 2.3732 loss= 0.9428\n",
      "            step(): A.grad= 1.3732 A.data= 0.9305\n",
      "Ep 78: zero_grad(): A.grad= 0.0000 A.data= 0.9305 loss= 0.0024\n",
      "            step(): A.grad=-0.0695 A.data=-0.3801\n",
      "Ep 79: zero_grad(): A.grad= 0.0000 A.data=-0.3801 loss= 0.9524\n",
      "            step(): A.grad=-1.3801 A.data= 0.9315\n",
      "Ep 80: zero_grad(): A.grad= 0.0000 A.data= 0.9315 loss= 0.0023\n",
      "            step(): A.grad=-0.0685 A.data= 2.3733\n",
      "Ep 81: zero_grad(): A.grad= 0.0000 A.data= 2.3733 loss= 0.9429\n",
      "            step(): A.grad= 1.3733 A.data= 1.2058\n",
      "Ep 82: zero_grad(): A.grad= 0.0000 A.data= 1.2058 loss= 0.0212\n",
      "            step(): A.grad= 0.2058 A.data=-0.3527\n",
      "Ep 83: zero_grad(): A.grad= 0.0000 A.data=-0.3527 loss= 0.9149\n",
      "            step(): A.grad=-1.3527 A.data= 0.6589\n",
      "Ep 84: zero_grad(): A.grad= 0.0000 A.data= 0.6589 loss= 0.0582\n",
      "            step(): A.grad=-0.3411 A.data= 2.3186\n",
      "Ep 85: zero_grad(): A.grad= 0.0000 A.data= 2.3186 loss= 0.8693\n",
      "            step(): A.grad= 1.3186 A.data= 1.4730\n",
      "Ep 86: zero_grad(): A.grad= 0.0000 A.data= 1.4730 loss= 0.1119\n",
      "            step(): A.grad= 0.4730 A.data=-0.2713\n",
      "Ep 87: zero_grad(): A.grad= 0.0000 A.data=-0.2713 loss= 0.8081\n",
      "            step(): A.grad=-1.2713 A.data= 0.3999\n",
      "Ep 88: zero_grad(): A.grad= 0.0000 A.data= 0.3999 loss= 0.1801\n",
      "            step(): A.grad=-0.6001 A.data= 2.2113\n",
      "Ep 89: zero_grad(): A.grad= 0.0000 A.data= 2.2113 loss= 0.7336\n",
      "            step(): A.grad= 1.2113 A.data= 1.7212\n",
      "Ep 90: zero_grad(): A.grad= 0.0000 A.data= 1.7212 loss= 0.2601\n",
      "            step(): A.grad= 0.7212 A.data=-0.1392\n",
      "Ep 91: zero_grad(): A.grad= 0.0000 A.data=-0.1392 loss= 0.6488\n",
      "            step(): A.grad=-1.1392 A.data= 0.1649\n",
      "Ep 92: zero_grad(): A.grad= 0.0000 A.data= 0.1649 loss= 0.3487\n",
      "            step(): A.grad=-0.8351 A.data= 2.0556\n",
      "Ep 93: zero_grad(): A.grad= 0.0000 A.data= 2.0556 loss= 0.5572\n",
      "            step(): A.grad= 1.0556 A.data= 1.9407\n",
      "Ep 94: zero_grad(): A.grad= 0.0000 A.data= 1.9407 loss= 0.4425\n",
      "            step(): A.grad= 0.9407 A.data= 0.0384\n",
      "Ep 95: zero_grad(): A.grad= 0.0000 A.data= 0.0384 loss= 0.4623\n",
      "            step(): A.grad=-0.9616 A.data=-0.0369\n",
      "Ep 96: zero_grad(): A.grad= 0.0000 A.data=-0.0369 loss= 0.5375\n",
      "            step(): A.grad=-1.0369 A.data= 1.8579\n",
      "Ep 97: zero_grad(): A.grad= 0.0000 A.data= 1.8579 loss= 0.3680\n",
      "            step(): A.grad= 0.8579 A.data= 2.1227\n",
      "Ep 98: zero_grad(): A.grad= 0.0000 A.data= 2.1227 loss= 0.6302\n",
      "            step(): A.grad= 1.1227 A.data= 0.2544\n",
      "Ep 99: zero_grad(): A.grad= 0.0000 A.data= 0.2544 loss= 0.2780\n",
      "            step(): A.grad=-0.7456 A.data=-0.1972\n",
      "Ep100: zero_grad(): A.grad= 0.0000 A.data=-0.1972 loss= 0.7167\n",
      "            step(): A.grad=-1.1972 A.data= 1.6259\n",
      "Ep101: zero_grad(): A.grad= 0.0000 A.data= 1.6259 loss= 0.1959\n",
      "            step(): A.grad= 0.6259 A.data= 2.2598\n",
      "Ep102: zero_grad(): A.grad= 0.0000 A.data= 2.2598 loss= 0.7936\n",
      "            step(): A.grad= 1.2598 A.data= 0.5001\n",
      "Ep103: zero_grad(): A.grad= 0.0000 A.data= 0.5001 loss= 0.1250\n",
      "            step(): A.grad=-0.4999 A.data=-0.3098\n",
      "Ep104: zero_grad(): A.grad= 0.0000 A.data=-0.3098 loss= 0.8578\n",
      "            step(): A.grad=-1.3098 A.data= 1.3689\n",
      "Ep105: zero_grad(): A.grad= 0.0000 A.data= 1.3689 loss= 0.0681\n",
      "            step(): A.grad= 0.3689 A.data= 2.3467\n",
      "Ep106: zero_grad(): A.grad= 0.0000 A.data= 2.3467 loss= 0.9068\n",
      "            step(): A.grad= 1.3467 A.data= 0.7657\n",
      "Ep107: zero_grad(): A.grad= 0.0000 A.data= 0.7657 loss= 0.0274\n",
      "            step(): A.grad=-0.2343 A.data=-0.3701\n",
      "Ep108: zero_grad(): A.grad= 0.0000 A.data=-0.3701 loss= 0.9386\n",
      "            step(): A.grad=-1.3701 A.data= 1.0973\n",
      "Ep109: zero_grad(): A.grad= 0.0000 A.data= 1.0973 loss= 0.0047\n",
      "            step(): A.grad= 0.0973 A.data= 2.3798\n",
      "Ep110: zero_grad(): A.grad= 0.0000 A.data= 2.3798 loss= 0.9520\n",
      "            step(): A.grad= 1.3798 A.data= 1.0407\n",
      "Ep111: zero_grad(): A.grad= 0.0000 A.data= 1.0407 loss= 0.0008\n",
      "            step(): A.grad= 0.0407 A.data=-0.3758\n",
      "Ep112: zero_grad(): A.grad= 0.0000 A.data=-0.3758 loss= 0.9464\n",
      "            step(): A.grad=-1.3758 A.data= 0.8217\n",
      "Ep113: zero_grad(): A.grad= 0.0000 A.data= 0.8217 loss= 0.0159\n",
      "            step(): A.grad=-0.1783 A.data= 2.3579\n",
      "Ep114: zero_grad(): A.grad= 0.0000 A.data= 2.3579 loss= 0.9220\n",
      "            step(): A.grad= 1.3579 A.data= 1.3141\n",
      "Ep115: zero_grad(): A.grad= 0.0000 A.data= 1.3141 loss= 0.0493\n",
      "            step(): A.grad= 0.3141 A.data=-0.3265\n",
      "Ep116: zero_grad(): A.grad= 0.0000 A.data=-0.3265 loss= 0.8798\n",
      "            step(): A.grad=-1.3265 A.data= 0.5532\n",
      "Ep117: zero_grad(): A.grad= 0.0000 A.data= 0.5532 loss= 0.0998\n",
      "            step(): A.grad=-0.4468 A.data= 2.2819\n",
      "Ep118: zero_grad(): A.grad= 0.0000 A.data= 2.2819 loss= 0.8216\n",
      "            step(): A.grad= 1.2819 A.data= 1.5749\n",
      "Ep119: zero_grad(): A.grad= 0.0000 A.data= 1.5749 loss= 0.1653\n",
      "            step(): A.grad= 0.5749 A.data=-0.2244\n",
      "Ep120: zero_grad(): A.grad= 0.0000 A.data=-0.2244 loss= 0.7495\n",
      "            step(): A.grad=-1.2244 A.data= 0.3026\n",
      "Ep121: zero_grad(): A.grad= 0.0000 A.data= 0.3026 loss= 0.2432\n",
      "            step(): A.grad=-0.6974 A.data= 2.1546\n",
      "Ep122: zero_grad(): A.grad= 0.0000 A.data= 2.1546 loss= 0.6666\n",
      "            step(): A.grad= 1.1546 A.data= 1.8128\n",
      "Ep123: zero_grad(): A.grad= 0.0000 A.data= 1.8128 loss= 0.3304\n",
      "            step(): A.grad= 0.8128 A.data=-0.0733\n",
      "Ep124: zero_grad(): A.grad= 0.0000 A.data=-0.0733 loss= 0.5760\n",
      "            step(): A.grad=-1.0733 A.data= 0.0798\n",
      "Ep125: zero_grad(): A.grad= 0.0000 A.data= 0.0798 loss= 0.4234\n",
      "            step(): A.grad=-0.9202 A.data= 1.9813\n",
      "Ep126: zero_grad(): A.grad= 0.0000 A.data= 1.9813 loss= 0.4815\n",
      "            step(): A.grad= 0.9813 A.data= 2.0183\n",
      "Ep127: zero_grad(): A.grad= 0.0000 A.data= 2.0183 loss= 0.5185\n",
      "            step(): A.grad= 1.0183 A.data= 0.1205\n",
      "Ep128: zero_grad(): A.grad= 0.0000 A.data= 0.1205 loss= 0.3867\n",
      "            step(): A.grad=-0.8795 A.data=-0.1063\n",
      "Ep129: zero_grad(): A.grad= 0.0000 A.data=-0.1063 loss= 0.6119\n",
      "            step(): A.grad=-1.1063 A.data= 1.7689\n",
      "Ep130: zero_grad(): A.grad= 0.0000 A.data= 1.7689 loss= 0.2956\n",
      "            step(): A.grad= 0.7689 A.data= 2.1831\n",
      "Ep131: zero_grad(): A.grad= 0.0000 A.data= 2.1831 loss= 0.6999\n",
      "            step(): A.grad= 1.1831 A.data= 0.3495\n",
      "Ep132: zero_grad(): A.grad= 0.0000 A.data= 0.3495 loss= 0.2116\n",
      "            step(): A.grad=-0.6505 A.data=-0.2482\n",
      "Ep133: zero_grad(): A.grad= 0.0000 A.data=-0.2482 loss= 0.7790\n",
      "            step(): A.grad=-1.2482 A.data= 1.5257\n",
      "Ep134: zero_grad(): A.grad= 0.0000 A.data= 1.5257 loss= 0.1382\n",
      "            step(): A.grad= 0.5257 A.data= 2.3008\n",
      "Ep135: zero_grad(): A.grad= 0.0000 A.data= 2.3008 loss= 0.8460\n",
      "            step(): A.grad= 1.3008 A.data= 0.6043\n",
      "Ep136: zero_grad(): A.grad= 0.0000 A.data= 0.6043 loss= 0.0783\n",
      "            step(): A.grad=-0.3957 A.data=-0.3403\n",
      "Ep137: zero_grad(): A.grad= 0.0000 A.data=-0.3403 loss= 0.8982\n",
      "            step(): A.grad=-1.3403 A.data= 1.2616\n",
      "Ep138: zero_grad(): A.grad= 0.0000 A.data= 1.2616 loss= 0.0342\n",
      "            step(): A.grad= 0.2616 A.data= 2.3665\n",
      "Ep139: zero_grad(): A.grad= 0.0000 A.data= 2.3665 loss= 0.9337\n",
      "            step(): A.grad= 1.3665 A.data= 0.8750\n",
      "Ep140: zero_grad(): A.grad= 0.0000 A.data= 0.8750 loss= 0.0078\n",
      "            step(): A.grad=-0.1250 A.data=-0.3790\n",
      "Ep141: zero_grad(): A.grad= 0.0000 A.data=-0.3790 loss= 0.9508\n",
      "            step(): A.grad=-1.3790 A.data= 0.9871\n",
      "Ep142: zero_grad(): A.grad= 0.0000 A.data= 0.9871 loss= 0.0001\n",
      "            step(): A.grad=-0.0129 A.data= 2.3777\n",
      "Ep143: zero_grad(): A.grad= 0.0000 A.data= 2.3777 loss= 0.9490\n",
      "            step(): A.grad= 1.3777 A.data= 1.1507\n",
      "Ep144: zero_grad(): A.grad= 0.0000 A.data= 1.1507 loss= 0.0114\n",
      "            step(): A.grad= 0.1507 A.data=-0.3626\n",
      "Ep145: zero_grad(): A.grad= 0.0000 A.data=-0.3626 loss= 0.9284\n",
      "            step(): A.grad=-1.3626 A.data= 0.7130\n",
      "Ep146: zero_grad(): A.grad= 0.0000 A.data= 0.7130 loss= 0.0412\n",
      "            step(): A.grad=-0.2870 A.data= 2.3339\n",
      "Ep147: zero_grad(): A.grad= 0.0000 A.data= 2.3339 loss= 0.8897\n",
      "            step(): A.grad= 1.3339 A.data= 1.4204\n",
      "Ep148: zero_grad(): A.grad= 0.0000 A.data= 1.4204 loss= 0.0883\n",
      "            step(): A.grad= 0.4204 A.data=-0.2919\n",
      "Ep149: zero_grad(): A.grad= 0.0000 A.data=-0.2919 loss= 0.8345\n",
      "            step(): A.grad=-1.2919 A.data= 0.4505\n",
      "Ep150: zero_grad(): A.grad= 0.0000 A.data= 0.4505 loss= 0.1510\n",
      "            step(): A.grad=-0.5495 A.data= 2.2369\n",
      "Ep151: zero_grad(): A.grad= 0.0000 A.data= 2.2369 loss= 0.7650\n",
      "            step(): A.grad= 1.2369 A.data= 1.6732\n",
      "Ep152: zero_grad(): A.grad= 0.0000 A.data= 1.6732 loss= 0.2266\n",
      "            step(): A.grad= 0.6732 A.data=-0.1696\n",
      "Ep153: zero_grad(): A.grad= 0.0000 A.data=-0.1696 loss= 0.6840\n",
      "            step(): A.grad=-1.1696 A.data= 0.2098\n",
      "Ep154: zero_grad(): A.grad= 0.0000 A.data= 0.2098 loss= 0.3122\n",
      "            step(): A.grad=-0.7902 A.data= 2.0906\n",
      "Ep155: zero_grad(): A.grad= 0.0000 A.data= 2.0906 loss= 0.5947\n",
      "            step(): A.grad= 1.0906 A.data= 1.8993\n",
      "Ep156: zero_grad(): A.grad= 0.0000 A.data= 1.8993 loss= 0.4043\n",
      "            step(): A.grad= 0.8993 A.data=-0.0007\n",
      "Ep157: zero_grad(): A.grad= 0.0000 A.data=-0.0007 loss= 0.5007\n",
      "            step(): A.grad=-1.0007 A.data= 0.0007\n",
      "Ep158: zero_grad(): A.grad= 0.0000 A.data= 0.0007 loss= 0.4993\n",
      "            step(): A.grad=-0.9993 A.data= 1.9007\n",
      "Ep159: zero_grad(): A.grad= 0.0000 A.data= 1.9007 loss= 0.4057\n",
      "            step(): A.grad= 0.9007 A.data= 2.0894\n",
      "Ep160: zero_grad(): A.grad= 0.0000 A.data= 2.0894 loss= 0.5934\n",
      "            step(): A.grad= 1.0894 A.data= 0.2082\n",
      "Ep161: zero_grad(): A.grad= 0.0000 A.data= 0.2082 loss= 0.3135\n",
      "            step(): A.grad=-0.7918 A.data=-0.1686\n",
      "Ep162: zero_grad(): A.grad= 0.0000 A.data=-0.1686 loss= 0.6828\n",
      "            step(): A.grad=-1.1686 A.data= 1.6749\n",
      "Ep163: zero_grad(): A.grad= 0.0000 A.data= 1.6749 loss= 0.2278\n",
      "            step(): A.grad= 0.6749 A.data= 2.2361\n",
      "Ep164: zero_grad(): A.grad= 0.0000 A.data= 2.2361 loss= 0.7639\n",
      "            step(): A.grad= 1.2361 A.data= 0.4487\n",
      "Ep165: zero_grad(): A.grad= 0.0000 A.data= 0.4487 loss= 0.1520\n",
      "            step(): A.grad=-0.5513 A.data=-0.2912\n",
      "Ep166: zero_grad(): A.grad= 0.0000 A.data=-0.2912 loss= 0.8336\n",
      "            step(): A.grad=-1.2912 A.data= 1.4222\n",
      "Ep167: zero_grad(): A.grad= 0.0000 A.data= 1.4222 loss= 0.0891\n",
      "            step(): A.grad= 0.4222 A.data= 2.3334\n",
      "Ep168: zero_grad(): A.grad= 0.0000 A.data= 2.3334 loss= 0.8890\n",
      "            step(): A.grad= 1.3334 A.data= 0.7111\n",
      "Ep169: zero_grad(): A.grad= 0.0000 A.data= 0.7111 loss= 0.0417\n",
      "            step(): A.grad=-0.2889 A.data=-0.3623\n",
      "Ep170: zero_grad(): A.grad= 0.0000 A.data=-0.3623 loss= 0.9280\n",
      "            step(): A.grad=-1.3623 A.data= 1.1526\n",
      "Ep171: zero_grad(): A.grad= 0.0000 A.data= 1.1526 loss= 0.0116\n",
      "            step(): A.grad= 0.1526 A.data= 2.3776\n",
      "Ep172: zero_grad(): A.grad= 0.0000 A.data= 2.3776 loss= 0.9489\n",
      "            step(): A.grad= 1.3776 A.data= 0.9851\n",
      "Ep173: zero_grad(): A.grad= 0.0000 A.data= 0.9851 loss= 0.0001\n",
      "            step(): A.grad=-0.0149 A.data=-0.3791\n",
      "Ep174: zero_grad(): A.grad= 0.0000 A.data=-0.3791 loss= 0.9509\n",
      "            step(): A.grad=-1.3791 A.data= 0.8770\n",
      "Ep175: zero_grad(): A.grad= 0.0000 A.data= 0.8770 loss= 0.0076\n",
      "            step(): A.grad=-0.1230 A.data= 2.3668\n",
      "Ep176: zero_grad(): A.grad= 0.0000 A.data= 2.3668 loss= 0.9340\n",
      "            step(): A.grad= 1.3668 A.data= 1.2597\n",
      "Ep177: zero_grad(): A.grad= 0.0000 A.data= 1.2597 loss= 0.0337\n",
      "            step(): A.grad= 0.2597 A.data=-0.3408\n",
      "Ep178: zero_grad(): A.grad= 0.0000 A.data=-0.3408 loss= 0.8989\n",
      "            step(): A.grad=-1.3408 A.data= 0.6062\n",
      "Ep179: zero_grad(): A.grad= 0.0000 A.data= 0.6062 loss= 0.0775\n",
      "            step(): A.grad=-0.3938 A.data= 2.3014\n",
      "Ep180: zero_grad(): A.grad= 0.0000 A.data= 2.3014 loss= 0.8468\n",
      "            step(): A.grad= 1.3014 A.data= 1.5239\n",
      "Ep181: zero_grad(): A.grad= 0.0000 A.data= 1.5239 loss= 0.1372\n",
      "            step(): A.grad= 0.5239 A.data=-0.2490\n",
      "Ep182: zero_grad(): A.grad= 0.0000 A.data=-0.2490 loss= 0.7800\n",
      "            step(): A.grad=-1.2490 A.data= 0.3512\n",
      "Ep183: zero_grad(): A.grad= 0.0000 A.data= 0.3512 loss= 0.2105\n",
      "            step(): A.grad=-0.6488 A.data= 2.1841\n",
      "Ep184: zero_grad(): A.grad= 0.0000 A.data= 2.1841 loss= 0.7011\n",
      "            step(): A.grad= 1.1841 A.data= 1.7672\n",
      "Ep185: zero_grad(): A.grad= 0.0000 A.data= 1.7672 loss= 0.2943\n",
      "            step(): A.grad= 0.7672 A.data=-0.1074\n",
      "Ep186: zero_grad(): A.grad= 0.0000 A.data=-0.1074 loss= 0.6132\n",
      "            step(): A.grad=-1.1074 A.data= 0.1220\n",
      "Ep187: zero_grad(): A.grad= 0.0000 A.data= 0.1220 loss= 0.3854\n",
      "            step(): A.grad=-0.8780 A.data= 2.0196\n",
      "Ep188: zero_grad(): A.grad= 0.0000 A.data= 2.0196 loss= 0.5198\n",
      "            step(): A.grad= 1.0196 A.data= 1.9799\n",
      "Ep189: zero_grad(): A.grad= 0.0000 A.data= 1.9799 loss= 0.4801\n",
      "            step(): A.grad= 0.9799 A.data= 0.0784\n",
      "Ep190: zero_grad(): A.grad= 0.0000 A.data= 0.0784 loss= 0.4247\n",
      "            step(): A.grad=-0.9216 A.data=-0.0721\n",
      "Ep191: zero_grad(): A.grad= 0.0000 A.data=-0.0721 loss= 0.5747\n",
      "            step(): A.grad=-1.0721 A.data= 1.8144\n",
      "Ep192: zero_grad(): A.grad= 0.0000 A.data= 1.8144 loss= 0.3316\n",
      "            step(): A.grad= 0.8144 A.data= 2.1535\n",
      "Ep193: zero_grad(): A.grad= 0.0000 A.data= 2.1535 loss= 0.6653\n",
      "            step(): A.grad= 1.1535 A.data= 0.3009\n",
      "Ep194: zero_grad(): A.grad= 0.0000 A.data= 0.3009 loss= 0.2443\n",
      "            step(): A.grad=-0.6991 A.data=-0.2235\n",
      "Ep195: zero_grad(): A.grad= 0.0000 A.data=-0.2235 loss= 0.7484\n",
      "            step(): A.grad=-1.2235 A.data= 1.5767\n",
      "Ep196: zero_grad(): A.grad= 0.0000 A.data= 1.5767 loss= 0.1663\n",
      "            step(): A.grad= 0.5767 A.data= 2.2811\n",
      "Ep197: zero_grad(): A.grad= 0.0000 A.data= 2.2811 loss= 0.8206\n",
      "            step(): A.grad= 1.2811 A.data= 0.5514\n",
      "Ep198: zero_grad(): A.grad= 0.0000 A.data= 0.5514 loss= 0.1006\n",
      "            step(): A.grad=-0.4486 A.data=-0.3260\n",
      "Ep199: zero_grad(): A.grad= 0.0000 A.data=-0.3260 loss= 0.8791\n",
      "            step(): A.grad=-1.3260 A.data= 1.3160\n",
      "Ep200: zero_grad(): A.grad= 0.0000 A.data= 1.3160 loss= 0.0499\n",
      "            step(): A.grad= 0.3160 A.data= 2.3576\n",
      "Ep201: zero_grad(): A.grad= 0.0000 A.data= 2.3576 loss= 0.9215\n",
      "            step(): A.grad= 1.3576 A.data= 0.8198\n",
      "Ep202: zero_grad(): A.grad= 0.0000 A.data= 0.8198 loss= 0.0162\n",
      "            step(): A.grad=-0.1802 A.data=-0.3756\n",
      "Ep203: zero_grad(): A.grad= 0.0000 A.data=-0.3756 loss= 0.9462\n",
      "            step(): A.grad=-1.3756 A.data= 1.0427\n",
      "Ep204: zero_grad(): A.grad= 0.0000 A.data= 1.0427 loss= 0.0009\n",
      "            step(): A.grad= 0.0427 A.data= 2.3799\n",
      "Ep205: zero_grad(): A.grad= 0.0000 A.data= 2.3799 loss= 0.9520\n",
      "            step(): A.grad= 1.3799 A.data= 1.0953\n",
      "Ep206: zero_grad(): A.grad= 0.0000 A.data= 1.0953 loss= 0.0045\n",
      "            step(): A.grad= 0.0953 A.data=-0.3703\n",
      "Ep207: zero_grad(): A.grad= 0.0000 A.data=-0.3703 loss= 0.9389\n",
      "            step(): A.grad=-1.3703 A.data= 0.7677\n",
      "Ep208: zero_grad(): A.grad= 0.0000 A.data= 0.7677 loss= 0.0270\n",
      "            step(): A.grad=-0.2323 A.data= 2.3471\n",
      "Ep209: zero_grad(): A.grad= 0.0000 A.data= 2.3471 loss= 0.9074\n",
      "            step(): A.grad= 1.3471 A.data= 1.3671\n",
      "Ep210: zero_grad(): A.grad= 0.0000 A.data= 1.3671 loss= 0.0674\n",
      "            step(): A.grad= 0.3671 A.data=-0.3104\n",
      "Ep211: zero_grad(): A.grad= 0.0000 A.data=-0.3104 loss= 0.8586\n",
      "            step(): A.grad=-1.3104 A.data= 0.5019\n",
      "Ep212: zero_grad(): A.grad= 0.0000 A.data= 0.5019 loss= 0.1240\n",
      "            step(): A.grad=-0.4981 A.data= 2.2606\n",
      "Ep213: zero_grad(): A.grad= 0.0000 A.data= 2.2606 loss= 0.7946\n",
      "            step(): A.grad= 1.2606 A.data= 1.6242\n",
      "Ep214: zero_grad(): A.grad= 0.0000 A.data= 1.6242 loss= 0.1948\n",
      "            step(): A.grad= 0.6242 A.data=-0.1982\n",
      "Ep215: zero_grad(): A.grad= 0.0000 A.data=-0.1982 loss= 0.7178\n",
      "            step(): A.grad=-1.1982 A.data= 0.2560\n",
      "Ep216: zero_grad(): A.grad= 0.0000 A.data= 0.2560 loss= 0.2767\n",
      "            step(): A.grad=-0.7440 A.data= 2.1238\n",
      "Ep217: zero_grad(): A.grad= 0.0000 A.data= 2.1238 loss= 0.6314\n",
      "            step(): A.grad= 1.1238 A.data= 1.8564\n",
      "Ep218: zero_grad(): A.grad= 0.0000 A.data= 1.8564 loss= 0.3667\n",
      "            step(): A.grad= 0.8564 A.data=-0.0382\n",
      "Ep219: zero_grad(): A.grad= 0.0000 A.data=-0.0382 loss= 0.5389\n",
      "            step(): A.grad=-1.0382 A.data= 0.0398\n",
      "Ep220: zero_grad(): A.grad= 0.0000 A.data= 0.0398 loss= 0.4610\n",
      "            step(): A.grad=-0.9602 A.data= 1.9421\n",
      "Ep221: zero_grad(): A.grad= 0.0000 A.data= 1.9421 loss= 0.4438\n",
      "            step(): A.grad= 0.9421 A.data= 2.0544\n",
      "Ep222: zero_grad(): A.grad= 0.0000 A.data= 2.0544 loss= 0.5559\n",
      "            step(): A.grad= 1.0544 A.data= 0.1633\n",
      "Ep223: zero_grad(): A.grad= 0.0000 A.data= 0.1633 loss= 0.3500\n",
      "            step(): A.grad=-0.8367 A.data=-0.1380\n",
      "Ep224: zero_grad(): A.grad= 0.0000 A.data=-0.1380 loss= 0.6476\n",
      "            step(): A.grad=-1.1380 A.data= 1.7229\n",
      "Ep225: zero_grad(): A.grad= 0.0000 A.data= 1.7229 loss= 0.2613\n",
      "            step(): A.grad= 0.7229 A.data= 2.2103\n",
      "Ep226: zero_grad(): A.grad= 0.0000 A.data= 2.2103 loss= 0.7325\n",
      "            step(): A.grad= 1.2103 A.data= 0.3981\n",
      "Ep227: zero_grad(): A.grad= 0.0000 A.data= 0.3981 loss= 0.1811\n",
      "            step(): A.grad=-0.6019 A.data=-0.2705\n",
      "Ep228: zero_grad(): A.grad= 0.0000 A.data=-0.2705 loss= 0.8071\n",
      "            step(): A.grad=-1.2705 A.data= 1.4748\n",
      "Ep229: zero_grad(): A.grad= 0.0000 A.data= 1.4748 loss= 0.1127\n",
      "            step(): A.grad= 0.4748 A.data= 2.3180\n",
      "Ep230: zero_grad(): A.grad= 0.0000 A.data= 2.3180 loss= 0.8686\n",
      "            step(): A.grad= 1.3180 A.data= 0.6570\n",
      "Ep231: zero_grad(): A.grad= 0.0000 A.data= 0.6570 loss= 0.0588\n",
      "            step(): A.grad=-0.3430 A.data=-0.3523\n",
      "Ep232: zero_grad(): A.grad= 0.0000 A.data=-0.3523 loss= 0.9144\n",
      "            step(): A.grad=-1.3523 A.data= 1.2078\n",
      "Ep233: zero_grad(): A.grad= 0.0000 A.data= 1.2078 loss= 0.0216\n",
      "            step(): A.grad= 0.2078 A.data= 2.3731\n",
      "Ep234: zero_grad(): A.grad= 0.0000 A.data= 2.3731 loss= 0.9427\n",
      "            step(): A.grad= 1.3731 A.data= 0.9295\n",
      "Ep235: zero_grad(): A.grad= 0.0000 A.data= 0.9295 loss= 0.0025\n",
      "            step(): A.grad=-0.0705 A.data=-0.3801\n",
      "Ep236: zero_grad(): A.grad= 0.0000 A.data=-0.3801 loss= 0.9524\n",
      "            step(): A.grad=-1.3801 A.data= 0.9325\n",
      "Ep237: zero_grad(): A.grad= 0.0000 A.data= 0.9325 loss= 0.0023\n",
      "            step(): A.grad=-0.0675 A.data= 2.3734\n",
      "Ep238: zero_grad(): A.grad= 0.0000 A.data= 2.3734 loss= 0.9431\n",
      "            step(): A.grad= 1.3734 A.data= 1.2049\n",
      "Ep239: zero_grad(): A.grad= 0.0000 A.data= 1.2049 loss= 0.0210\n",
      "            step(): A.grad= 0.2049 A.data=-0.3529\n",
      "Ep240: zero_grad(): A.grad= 0.0000 A.data=-0.3529 loss= 0.9151\n",
      "            step(): A.grad=-1.3529 A.data= 0.6598\n",
      "Ep241: zero_grad(): A.grad= 0.0000 A.data= 0.6598 loss= 0.0579\n",
      "            step(): A.grad=-0.3402 A.data= 2.3189\n",
      "Ep242: zero_grad(): A.grad= 0.0000 A.data= 2.3189 loss= 0.8697\n",
      "            step(): A.grad= 1.3189 A.data= 1.4721\n",
      "Ep243: zero_grad(): A.grad= 0.0000 A.data= 1.4721 loss= 0.1114\n",
      "            step(): A.grad= 0.4721 A.data=-0.2717\n",
      "Ep244: zero_grad(): A.grad= 0.0000 A.data=-0.2717 loss= 0.8086\n",
      "            step(): A.grad=-1.2717 A.data= 0.4008\n",
      "Ep245: zero_grad(): A.grad= 0.0000 A.data= 0.4008 loss= 0.1795\n",
      "            step(): A.grad=-0.5992 A.data= 2.2117\n",
      "Ep246: zero_grad(): A.grad= 0.0000 A.data= 2.2117 loss= 0.7342\n",
      "            step(): A.grad= 1.2117 A.data= 1.7204\n",
      "Ep247: zero_grad(): A.grad= 0.0000 A.data= 1.7204 loss= 0.2595\n",
      "            step(): A.grad= 0.7204 A.data=-0.1397\n",
      "Ep248: zero_grad(): A.grad= 0.0000 A.data=-0.1397 loss= 0.6495\n",
      "            step(): A.grad=-1.1397 A.data= 0.1656\n",
      "Ep249: zero_grad(): A.grad= 0.0000 A.data= 0.1656 loss= 0.3481\n",
      "            step(): A.grad=-0.8344 A.data= 2.0563\n",
      "Ep250: zero_grad(): A.grad= 0.0000 A.data= 2.0563 loss= 0.5578\n",
      "            step(): A.grad= 1.0563 A.data= 1.9400\n",
      "Ep251: zero_grad(): A.grad= 0.0000 A.data= 1.9400 loss= 0.4418\n",
      "            step(): A.grad= 0.9400 A.data= 0.0377\n",
      "Ep252: zero_grad(): A.grad= 0.0000 A.data= 0.0377 loss= 0.4630\n",
      "            step(): A.grad=-0.9623 A.data=-0.0362\n",
      "Ep253: zero_grad(): A.grad= 0.0000 A.data=-0.0362 loss= 0.5369\n",
      "            step(): A.grad=-1.0362 A.data= 1.8586\n",
      "Ep254: zero_grad(): A.grad= 0.0000 A.data= 1.8586 loss= 0.3686\n",
      "            step(): A.grad= 0.8586 A.data= 2.1221\n",
      "Ep255: zero_grad(): A.grad= 0.0000 A.data= 2.1221 loss= 0.6295\n",
      "            step(): A.grad= 1.1221 A.data= 0.2536\n",
      "Ep256: zero_grad(): A.grad= 0.0000 A.data= 0.2536 loss= 0.2786\n",
      "            step(): A.grad=-0.7464 A.data=-0.1967\n",
      "Ep257: zero_grad(): A.grad= 0.0000 A.data=-0.1967 loss= 0.7161\n",
      "            step(): A.grad=-1.1967 A.data= 1.6268\n",
      "Ep258: zero_grad(): A.grad= 0.0000 A.data= 1.6268 loss= 0.1964\n",
      "            step(): A.grad= 0.6268 A.data= 2.2594\n",
      "Ep259: zero_grad(): A.grad= 0.0000 A.data= 2.2594 loss= 0.7931\n",
      "            step(): A.grad= 1.2594 A.data= 0.4992\n",
      "Ep260: zero_grad(): A.grad= 0.0000 A.data= 0.4992 loss= 0.1254\n",
      "            step(): A.grad=-0.5008 A.data=-0.3095\n",
      "Ep261: zero_grad(): A.grad= 0.0000 A.data=-0.3095 loss= 0.8574\n",
      "            step(): A.grad=-1.3095 A.data= 1.3699\n",
      "Ep262: zero_grad(): A.grad= 0.0000 A.data= 1.3699 loss= 0.0684\n",
      "            step(): A.grad= 0.3699 A.data= 2.3465\n",
      "Ep263: zero_grad(): A.grad= 0.0000 A.data= 2.3465 loss= 0.9065\n",
      "            step(): A.grad= 1.3465 A.data= 0.7648\n",
      "Ep264: zero_grad(): A.grad= 0.0000 A.data= 0.7648 loss= 0.0277\n",
      "            step(): A.grad=-0.2352 A.data=-0.3700\n",
      "Ep265: zero_grad(): A.grad= 0.0000 A.data=-0.3700 loss= 0.9384\n",
      "            step(): A.grad=-1.3700 A.data= 1.0982\n",
      "Ep266: zero_grad(): A.grad= 0.0000 A.data= 1.0982 loss= 0.0048\n",
      "            step(): A.grad= 0.0982 A.data= 2.3798\n",
      "Ep267: zero_grad(): A.grad= 0.0000 A.data= 2.3798 loss= 0.9520\n",
      "            step(): A.grad= 1.3798 A.data= 1.0398\n",
      "Ep268: zero_grad(): A.grad= 0.0000 A.data= 1.0398 loss= 0.0008\n",
      "            step(): A.grad= 0.0398 A.data=-0.3758\n",
      "Ep269: zero_grad(): A.grad= 0.0000 A.data=-0.3758 loss= 0.9465\n",
      "            step(): A.grad=-1.3758 A.data= 0.8227\n",
      "Ep270: zero_grad(): A.grad= 0.0000 A.data= 0.8227 loss= 0.0157\n",
      "            step(): A.grad=-0.1773 A.data= 2.3581\n",
      "Ep271: zero_grad(): A.grad= 0.0000 A.data= 2.3581 loss= 0.9222\n",
      "            step(): A.grad= 1.3581 A.data= 1.3132\n",
      "Ep272: zero_grad(): A.grad= 0.0000 A.data= 1.3132 loss= 0.0490\n",
      "            step(): A.grad= 0.3132 A.data=-0.3268\n",
      "Ep273: zero_grad(): A.grad= 0.0000 A.data=-0.3268 loss= 0.8802\n",
      "            step(): A.grad=-1.3268 A.data= 0.5542\n",
      "Ep274: zero_grad(): A.grad= 0.0000 A.data= 0.5542 loss= 0.0994\n",
      "            step(): A.grad=-0.4458 A.data= 2.2822\n",
      "Ep275: zero_grad(): A.grad= 0.0000 A.data= 2.2822 loss= 0.8220\n",
      "            step(): A.grad= 1.2822 A.data= 1.5741\n",
      "Ep276: zero_grad(): A.grad= 0.0000 A.data= 1.5741 loss= 0.1648\n",
      "            step(): A.grad= 0.5741 A.data=-0.2248\n",
      "Ep277: zero_grad(): A.grad= 0.0000 A.data=-0.2248 loss= 0.7501\n",
      "            step(): A.grad=-1.2248 A.data= 0.3035\n",
      "Ep278: zero_grad(): A.grad= 0.0000 A.data= 0.3035 loss= 0.2426\n",
      "            step(): A.grad=-0.6965 A.data= 2.1552\n",
      "Ep279: zero_grad(): A.grad= 0.0000 A.data= 2.1552 loss= 0.6672\n",
      "            step(): A.grad= 1.1552 A.data= 1.8121\n",
      "Ep280: zero_grad(): A.grad= 0.0000 A.data= 1.8121 loss= 0.3297\n",
      "            step(): A.grad= 0.8121 A.data=-0.0739\n",
      "Ep281: zero_grad(): A.grad= 0.0000 A.data=-0.0739 loss= 0.5767\n",
      "            step(): A.grad=-1.0739 A.data= 0.0806\n",
      "Ep282: zero_grad(): A.grad= 0.0000 A.data= 0.0806 loss= 0.4227\n",
      "            step(): A.grad=-0.9194 A.data= 1.9820\n",
      "Ep283: zero_grad(): A.grad= 0.0000 A.data= 1.9820 loss= 0.4822\n",
      "            step(): A.grad= 0.9820 A.data= 2.0176\n",
      "Ep284: zero_grad(): A.grad= 0.0000 A.data= 2.0176 loss= 0.5178\n",
      "            step(): A.grad= 1.0176 A.data= 0.1198\n",
      "Ep285: zero_grad(): A.grad= 0.0000 A.data= 0.1198 loss= 0.3874\n",
      "            step(): A.grad=-0.8802 A.data=-0.1057\n",
      "Ep286: zero_grad(): A.grad= 0.0000 A.data=-0.1057 loss= 0.6113\n",
      "            step(): A.grad=-1.1057 A.data= 1.7697\n",
      "Ep287: zero_grad(): A.grad= 0.0000 A.data= 1.7697 loss= 0.2962\n",
      "            step(): A.grad= 0.7697 A.data= 2.1826\n",
      "Ep288: zero_grad(): A.grad= 0.0000 A.data= 2.1826 loss= 0.6993\n",
      "            step(): A.grad= 1.1826 A.data= 0.3486\n",
      "Ep289: zero_grad(): A.grad= 0.0000 A.data= 0.3486 loss= 0.2122\n",
      "            step(): A.grad=-0.6514 A.data=-0.2478\n",
      "Ep290: zero_grad(): A.grad= 0.0000 A.data=-0.2478 loss= 0.7785\n",
      "            step(): A.grad=-1.2478 A.data= 1.5266\n",
      "Ep291: zero_grad(): A.grad= 0.0000 A.data= 1.5266 loss= 0.1387\n",
      "            step(): A.grad= 0.5266 A.data= 2.3004\n",
      "Ep292: zero_grad(): A.grad= 0.0000 A.data= 2.3004 loss= 0.8456\n",
      "            step(): A.grad= 1.3004 A.data= 0.6034\n",
      "Ep293: zero_grad(): A.grad= 0.0000 A.data= 0.6034 loss= 0.0786\n",
      "            step(): A.grad=-0.3966 A.data=-0.3401\n",
      "Ep294: zero_grad(): A.grad= 0.0000 A.data=-0.3401 loss= 0.8979\n",
      "            step(): A.grad=-1.3401 A.data= 1.2626\n",
      "Ep295: zero_grad(): A.grad= 0.0000 A.data= 1.2626 loss= 0.0345\n",
      "            step(): A.grad= 0.2626 A.data= 2.3664\n",
      "Ep296: zero_grad(): A.grad= 0.0000 A.data= 2.3664 loss= 0.9335\n",
      "            step(): A.grad= 1.3664 A.data= 0.8741\n",
      "Ep297: zero_grad(): A.grad= 0.0000 A.data= 0.8741 loss= 0.0079\n",
      "            step(): A.grad=-0.1259 A.data=-0.3789\n",
      "Ep298: zero_grad(): A.grad= 0.0000 A.data=-0.3789 loss= 0.9508\n",
      "            step(): A.grad=-1.3789 A.data= 0.9880\n",
      "Ep299: zero_grad(): A.grad= 0.0000 A.data= 0.9880 loss= 0.0001\n",
      "            step(): A.grad=-0.0120 A.data= 2.3778\n",
      "Ep300: zero_grad(): A.grad= 0.0000 A.data= 2.3778 loss= 0.9491\n",
      "            step(): A.grad= 1.3778 A.data= 1.1497\n",
      "Ep301: zero_grad(): A.grad= 0.0000 A.data= 1.1497 loss= 0.0112\n",
      "            step(): A.grad= 0.1497 A.data=-0.3628\n",
      "Ep302: zero_grad(): A.grad= 0.0000 A.data=-0.3628 loss= 0.9286\n",
      "            step(): A.grad=-1.3628 A.data= 0.7140\n",
      "Ep303: zero_grad(): A.grad= 0.0000 A.data= 0.7140 loss= 0.0409\n",
      "            step(): A.grad=-0.2860 A.data= 2.3342\n",
      "Ep304: zero_grad(): A.grad= 0.0000 A.data= 2.3342 loss= 0.8900\n",
      "            step(): A.grad= 1.3342 A.data= 1.4194\n",
      "Ep305: zero_grad(): A.grad= 0.0000 A.data= 1.4194 loss= 0.0880\n",
      "            step(): A.grad= 0.4194 A.data=-0.2922\n",
      "Ep306: zero_grad(): A.grad= 0.0000 A.data=-0.2922 loss= 0.8349\n",
      "            step(): A.grad=-1.2922 A.data= 0.4513\n",
      "Ep307: zero_grad(): A.grad= 0.0000 A.data= 0.4513 loss= 0.1505\n",
      "            step(): A.grad=-0.5487 A.data= 2.2374\n",
      "Ep308: zero_grad(): A.grad= 0.0000 A.data= 2.2374 loss= 0.7655\n",
      "            step(): A.grad= 1.2374 A.data= 1.6724\n",
      "Ep309: zero_grad(): A.grad= 0.0000 A.data= 1.6724 loss= 0.2261\n",
      "            step(): A.grad= 0.6724 A.data=-0.1701\n",
      "Ep310: zero_grad(): A.grad= 0.0000 A.data=-0.1701 loss= 0.6846\n",
      "            step(): A.grad=-1.1701 A.data= 0.2106\n",
      "Ep311: zero_grad(): A.grad= 0.0000 A.data= 0.2106 loss= 0.3116\n",
      "            step(): A.grad=-0.7894 A.data= 2.0912\n",
      "Ep312: zero_grad(): A.grad= 0.0000 A.data= 2.0912 loss= 0.5953\n",
      "            step(): A.grad= 1.0912 A.data= 1.8985\n",
      "Ep313: zero_grad(): A.grad= 0.0000 A.data= 1.8985 loss= 0.4037\n",
      "            step(): A.grad= 0.8985 A.data=-0.0013\n",
      "Ep314: zero_grad(): A.grad= 0.0000 A.data=-0.0013 loss= 0.5013\n",
      "            step(): A.grad=-1.0013 A.data= 0.0013\n",
      "Ep315: zero_grad(): A.grad= 0.0000 A.data= 0.0013 loss= 0.4987\n",
      "            step(): A.grad=-0.9987 A.data= 1.9015\n",
      "Ep316: zero_grad(): A.grad= 0.0000 A.data= 1.9015 loss= 0.4063\n",
      "            step(): A.grad= 0.9015 A.data= 2.0888\n",
      "Ep317: zero_grad(): A.grad= 0.0000 A.data= 2.0888 loss= 0.5927\n",
      "            step(): A.grad= 1.0888 A.data= 0.2074\n",
      "Ep318: zero_grad(): A.grad= 0.0000 A.data= 0.2074 loss= 0.3141\n",
      "            step(): A.grad=-0.7926 A.data=-0.1681\n",
      "Ep319: zero_grad(): A.grad= 0.0000 A.data=-0.1681 loss= 0.6822\n",
      "            step(): A.grad=-1.1681 A.data= 1.6758\n",
      "Ep320: zero_grad(): A.grad= 0.0000 A.data= 1.6758 loss= 0.2283\n",
      "            step(): A.grad= 0.6758 A.data= 2.2356\n",
      "Ep321: zero_grad(): A.grad= 0.0000 A.data= 2.2356 loss= 0.7634\n",
      "            step(): A.grad= 1.2356 A.data= 0.4478\n",
      "Ep322: zero_grad(): A.grad= 0.0000 A.data= 0.4478 loss= 0.1525\n",
      "            step(): A.grad=-0.5522 A.data=-0.2909\n",
      "Ep323: zero_grad(): A.grad= 0.0000 A.data=-0.2909 loss= 0.8332\n",
      "            step(): A.grad=-1.2909 A.data= 1.4231\n",
      "Ep324: zero_grad(): A.grad= 0.0000 A.data= 1.4231 loss= 0.0895\n",
      "            step(): A.grad= 0.4231 A.data= 2.3332\n",
      "Ep325: zero_grad(): A.grad= 0.0000 A.data= 2.3332 loss= 0.8887\n",
      "            step(): A.grad= 1.3332 A.data= 0.7102\n",
      "Ep326: zero_grad(): A.grad= 0.0000 A.data= 0.7102 loss= 0.0420\n",
      "            step(): A.grad=-0.2898 A.data=-0.3622\n",
      "Ep327: zero_grad(): A.grad= 0.0000 A.data=-0.3622 loss= 0.9277\n",
      "            step(): A.grad=-1.3622 A.data= 1.1536\n",
      "Ep328: zero_grad(): A.grad= 0.0000 A.data= 1.1536 loss= 0.0118\n",
      "            step(): A.grad= 0.1536 A.data= 2.3775\n",
      "Ep329: zero_grad(): A.grad= 0.0000 A.data= 2.3775 loss= 0.9488\n",
      "            step(): A.grad= 1.3775 A.data= 0.9841\n",
      "Ep330: zero_grad(): A.grad= 0.0000 A.data= 0.9841 loss= 0.0001\n",
      "            step(): A.grad=-0.0159 A.data=-0.3791\n",
      "Ep331: zero_grad(): A.grad= 0.0000 A.data=-0.3791 loss= 0.9510\n",
      "            step(): A.grad=-1.3791 A.data= 0.8779\n",
      "Ep332: zero_grad(): A.grad= 0.0000 A.data= 0.8779 loss= 0.0074\n",
      "            step(): A.grad=-0.1221 A.data= 2.3669\n",
      "Ep333: zero_grad(): A.grad= 0.0000 A.data= 2.3669 loss= 0.9342\n",
      "            step(): A.grad= 1.3669 A.data= 1.2587\n",
      "Ep334: zero_grad(): A.grad= 0.0000 A.data= 1.2587 loss= 0.0335\n",
      "            step(): A.grad= 0.2587 A.data=-0.3410\n",
      "Ep335: zero_grad(): A.grad= 0.0000 A.data=-0.3410 loss= 0.8992\n",
      "            step(): A.grad=-1.3410 A.data= 0.6071\n",
      "Ep336: zero_grad(): A.grad= 0.0000 A.data= 0.6071 loss= 0.0772\n",
      "            step(): A.grad=-0.3929 A.data= 2.3017\n",
      "Ep337: zero_grad(): A.grad= 0.0000 A.data= 2.3017 loss= 0.8473\n",
      "            step(): A.grad= 1.3017 A.data= 1.5230\n",
      "Ep338: zero_grad(): A.grad= 0.0000 A.data= 1.5230 loss= 0.1368\n",
      "            step(): A.grad= 0.5230 A.data=-0.2494\n",
      "Ep339: zero_grad(): A.grad= 0.0000 A.data=-0.2494 loss= 0.7805\n",
      "            step(): A.grad=-1.2494 A.data= 0.3520\n",
      "Ep340: zero_grad(): A.grad= 0.0000 A.data= 0.3520 loss= 0.2099\n",
      "            step(): A.grad=-0.6480 A.data= 2.1846\n",
      "Ep341: zero_grad(): A.grad= 0.0000 A.data= 2.1846 loss= 0.7017\n",
      "            step(): A.grad= 1.1846 A.data= 1.7664\n",
      "Ep342: zero_grad(): A.grad= 0.0000 A.data= 1.7664 loss= 0.2937\n",
      "            step(): A.grad= 0.7664 A.data=-0.1080\n",
      "Ep343: zero_grad(): A.grad= 0.0000 A.data=-0.1080 loss= 0.6138\n",
      "            step(): A.grad=-1.1080 A.data= 0.1228\n",
      "Ep344: zero_grad(): A.grad= 0.0000 A.data= 0.1228 loss= 0.3848\n",
      "            step(): A.grad=-0.8772 A.data= 2.0203\n",
      "Ep345: zero_grad(): A.grad= 0.0000 A.data= 2.0203 loss= 0.5205\n",
      "            step(): A.grad= 1.0203 A.data= 1.9793\n",
      "Ep346: zero_grad(): A.grad= 0.0000 A.data= 1.9793 loss= 0.4795\n",
      "            step(): A.grad= 0.9793 A.data= 0.0777\n",
      "Ep347: zero_grad(): A.grad= 0.0000 A.data= 0.0777 loss= 0.4254\n",
      "            step(): A.grad=-0.9223 A.data=-0.0715\n",
      "Ep348: zero_grad(): A.grad= 0.0000 A.data=-0.0715 loss= 0.5741\n",
      "            step(): A.grad=-1.0715 A.data= 1.8152\n",
      "Ep349: zero_grad(): A.grad= 0.0000 A.data= 1.8152 loss= 0.3323\n",
      "            step(): A.grad= 0.8152 A.data= 2.1530\n",
      "Ep350: zero_grad(): A.grad= 0.0000 A.data= 2.1530 loss= 0.6647\n",
      "            step(): A.grad= 1.1530 A.data= 0.3001\n",
      "Ep351: zero_grad(): A.grad= 0.0000 A.data= 0.3001 loss= 0.2449\n",
      "            step(): A.grad=-0.6999 A.data=-0.2230\n",
      "Ep352: zero_grad(): A.grad= 0.0000 A.data=-0.2230 loss= 0.7479\n",
      "            step(): A.grad=-1.2230 A.data= 1.5776\n",
      "Ep353: zero_grad(): A.grad= 0.0000 A.data= 1.5776 loss= 0.1668\n",
      "            step(): A.grad= 0.5776 A.data= 2.2808\n",
      "Ep354: zero_grad(): A.grad= 0.0000 A.data= 2.2808 loss= 0.8202\n",
      "            step(): A.grad= 1.2808 A.data= 0.5505\n",
      "Ep355: zero_grad(): A.grad= 0.0000 A.data= 0.5505 loss= 0.1010\n",
      "            step(): A.grad=-0.4495 A.data=-0.3257\n",
      "Ep356: zero_grad(): A.grad= 0.0000 A.data=-0.3257 loss= 0.8788\n",
      "            step(): A.grad=-1.3257 A.data= 1.3169\n",
      "Ep357: zero_grad(): A.grad= 0.0000 A.data= 1.3169 loss= 0.0502\n",
      "            step(): A.grad= 0.3169 A.data= 2.3574\n",
      "Ep358: zero_grad(): A.grad= 0.0000 A.data= 2.3574 loss= 0.9213\n",
      "            step(): A.grad= 1.3574 A.data= 0.8188\n",
      "Ep359: zero_grad(): A.grad= 0.0000 A.data= 0.8188 loss= 0.0164\n",
      "            step(): A.grad=-0.1812 A.data=-0.3755\n",
      "Ep360: zero_grad(): A.grad= 0.0000 A.data=-0.3755 loss= 0.9460\n",
      "            step(): A.grad=-1.3755 A.data= 1.0437\n",
      "Ep361: zero_grad(): A.grad= 0.0000 A.data= 1.0437 loss= 0.0010\n",
      "            step(): A.grad= 0.0437 A.data= 2.3799\n",
      "Ep362: zero_grad(): A.grad= 0.0000 A.data= 2.3799 loss= 0.9521\n",
      "            step(): A.grad= 1.3799 A.data= 1.0943\n",
      "Ep363: zero_grad(): A.grad= 0.0000 A.data= 1.0943 loss= 0.0044\n",
      "            step(): A.grad= 0.0943 A.data=-0.3705\n",
      "Ep364: zero_grad(): A.grad= 0.0000 A.data=-0.3705 loss= 0.9391\n",
      "            step(): A.grad=-1.3705 A.data= 0.7686\n",
      "Ep365: zero_grad(): A.grad= 0.0000 A.data= 0.7686 loss= 0.0268\n",
      "            step(): A.grad=-0.2314 A.data= 2.3473\n",
      "Ep366: zero_grad(): A.grad= 0.0000 A.data= 2.3473 loss= 0.9076\n",
      "            step(): A.grad= 1.3473 A.data= 1.3661\n",
      "Ep367: zero_grad(): A.grad= 0.0000 A.data= 1.3661 loss= 0.0670\n",
      "            step(): A.grad= 0.3661 A.data=-0.3107\n",
      "Ep368: zero_grad(): A.grad= 0.0000 A.data=-0.3107 loss= 0.8590\n",
      "            step(): A.grad=-1.3107 A.data= 0.5028\n",
      "Ep369: zero_grad(): A.grad= 0.0000 A.data= 0.5028 loss= 0.1236\n",
      "            step(): A.grad=-0.4972 A.data= 2.2610\n",
      "Ep370: zero_grad(): A.grad= 0.0000 A.data= 2.2610 loss= 0.7951\n",
      "            step(): A.grad= 1.2610 A.data= 1.6233\n",
      "Ep371: zero_grad(): A.grad= 0.0000 A.data= 1.6233 loss= 0.1942\n",
      "            step(): A.grad= 0.6233 A.data=-0.1987\n",
      "Ep372: zero_grad(): A.grad= 0.0000 A.data=-0.1987 loss= 0.7184\n",
      "            step(): A.grad=-1.1987 A.data= 0.2568\n",
      "Ep373: zero_grad(): A.grad= 0.0000 A.data= 0.2568 loss= 0.2761\n",
      "            step(): A.grad=-0.7432 A.data= 2.1243\n",
      "Ep374: zero_grad(): A.grad= 0.0000 A.data= 2.1243 loss= 0.6321\n",
      "            step(): A.grad= 1.1243 A.data= 1.8556\n",
      "Ep375: zero_grad(): A.grad= 0.0000 A.data= 1.8556 loss= 0.3660\n",
      "            step(): A.grad= 0.8556 A.data=-0.0388\n",
      "Ep376: zero_grad(): A.grad= 0.0000 A.data=-0.0388 loss= 0.5395\n",
      "            step(): A.grad=-1.0388 A.data= 0.0405\n",
      "Ep377: zero_grad(): A.grad= 0.0000 A.data= 0.0405 loss= 0.4603\n",
      "            step(): A.grad=-0.9595 A.data= 1.9428\n",
      "Ep378: zero_grad(): A.grad= 0.0000 A.data= 1.9428 loss= 0.4445\n",
      "            step(): A.grad= 0.9428 A.data= 2.0538\n",
      "Ep379: zero_grad(): A.grad= 0.0000 A.data= 2.0538 loss= 0.5552\n",
      "            step(): A.grad= 1.0538 A.data= 0.1625\n",
      "Ep380: zero_grad(): A.grad= 0.0000 A.data= 0.1625 loss= 0.3507\n",
      "            step(): A.grad=-0.8375 A.data=-0.1375\n",
      "Ep381: zero_grad(): A.grad= 0.0000 A.data=-0.1375 loss= 0.6470\n",
      "            step(): A.grad=-1.1375 A.data= 1.7237\n",
      "Ep382: zero_grad(): A.grad= 0.0000 A.data= 1.7237 loss= 0.2619\n",
      "            step(): A.grad= 0.7237 A.data= 2.2099\n",
      "Ep383: zero_grad(): A.grad= 0.0000 A.data= 2.2099 loss= 0.7319\n",
      "            step(): A.grad= 1.2099 A.data= 0.3973\n",
      "Ep384: zero_grad(): A.grad= 0.0000 A.data= 0.3973 loss= 0.1816\n",
      "            step(): A.grad=-0.6027 A.data=-0.2701\n",
      "Ep385: zero_grad(): A.grad= 0.0000 A.data=-0.2701 loss= 0.8066\n",
      "            step(): A.grad=-1.2701 A.data= 1.4757\n",
      "Ep386: zero_grad(): A.grad= 0.0000 A.data= 1.4757 loss= 0.1132\n",
      "            step(): A.grad= 0.4757 A.data= 2.3177\n",
      "Ep387: zero_grad(): A.grad= 0.0000 A.data= 2.3177 loss= 0.8682\n",
      "            step(): A.grad= 1.3177 A.data= 0.6561\n",
      "Ep388: zero_grad(): A.grad= 0.0000 A.data= 0.6561 loss= 0.0591\n",
      "            step(): A.grad=-0.3439 A.data=-0.3521\n",
      "Ep389: zero_grad(): A.grad= 0.0000 A.data=-0.3521 loss= 0.9141\n",
      "            step(): A.grad=-1.3521 A.data= 1.2087\n",
      "Ep390: zero_grad(): A.grad= 0.0000 A.data= 1.2087 loss= 0.0218\n",
      "            step(): A.grad= 0.2087 A.data= 2.3730\n",
      "Ep391: zero_grad(): A.grad= 0.0000 A.data= 2.3730 loss= 0.9425\n",
      "            step(): A.grad= 1.3730 A.data= 0.9286\n",
      "Ep392: zero_grad(): A.grad= 0.0000 A.data= 0.9286 loss= 0.0026\n",
      "            step(): A.grad=-0.0714 A.data=-0.3801\n",
      "Ep393: zero_grad(): A.grad= 0.0000 A.data=-0.3801 loss= 0.9524\n",
      "            step(): A.grad=-1.3801 A.data= 0.9334\n",
      "Ep394: zero_grad(): A.grad= 0.0000 A.data= 0.9334 loss= 0.0022\n",
      "            step(): A.grad=-0.0666 A.data= 2.3735\n",
      "Ep395: zero_grad(): A.grad= 0.0000 A.data= 2.3735 loss= 0.9432\n",
      "            step(): A.grad= 1.3735 A.data= 1.2039\n",
      "Ep396: zero_grad(): A.grad= 0.0000 A.data= 1.2039 loss= 0.0208\n",
      "            step(): A.grad= 0.2039 A.data=-0.3531\n",
      "Ep397: zero_grad(): A.grad= 0.0000 A.data=-0.3531 loss= 0.9154\n",
      "            step(): A.grad=-1.3531 A.data= 0.6608\n",
      "Ep398: zero_grad(): A.grad= 0.0000 A.data= 0.6608 loss= 0.0575\n",
      "            step(): A.grad=-0.3392 A.data= 2.3192\n",
      "Ep399: zero_grad(): A.grad= 0.0000 A.data= 2.3192 loss= 0.8701\n",
      "            step(): A.grad= 1.3192 A.data= 1.4711\n",
      "Ep400: zero_grad(): A.grad= 0.0000 A.data= 1.4711 loss= 0.1110\n",
      "            step(): A.grad= 0.4711 A.data=-0.2720\n",
      "Ep401: zero_grad(): A.grad= 0.0000 A.data=-0.2720 loss= 0.8090\n",
      "            step(): A.grad=-1.2720 A.data= 0.4016\n",
      "Ep402: zero_grad(): A.grad= 0.0000 A.data= 0.4016 loss= 0.1790\n",
      "            step(): A.grad=-0.5984 A.data= 2.2122\n",
      "Ep403: zero_grad(): A.grad= 0.0000 A.data= 2.2122 loss= 0.7347\n",
      "            step(): A.grad= 1.2122 A.data= 1.7196\n",
      "Ep404: zero_grad(): A.grad= 0.0000 A.data= 1.7196 loss= 0.2589\n",
      "            step(): A.grad= 0.7196 A.data=-0.1402\n",
      "Ep405: zero_grad(): A.grad= 0.0000 A.data=-0.1402 loss= 0.6501\n",
      "            step(): A.grad=-1.1402 A.data= 0.1664\n",
      "Ep406: zero_grad(): A.grad= 0.0000 A.data= 0.1664 loss= 0.3474\n",
      "            step(): A.grad=-0.8336 A.data= 2.0569\n",
      "Ep407: zero_grad(): A.grad= 0.0000 A.data= 2.0569 loss= 0.5585\n",
      "            step(): A.grad= 1.0569 A.data= 1.9393\n",
      "Ep408: zero_grad(): A.grad= 0.0000 A.data= 1.9393 loss= 0.4411\n",
      "            step(): A.grad= 0.9393 A.data= 0.0370\n",
      "Ep409: zero_grad(): A.grad= 0.0000 A.data= 0.0370 loss= 0.4636\n",
      "            step(): A.grad=-0.9630 A.data=-0.0356\n",
      "Ep410: zero_grad(): A.grad= 0.0000 A.data=-0.0356 loss= 0.5362\n",
      "            step(): A.grad=-1.0356 A.data= 1.8594\n",
      "Ep411: zero_grad(): A.grad= 0.0000 A.data= 1.8594 loss= 0.3693\n",
      "            step(): A.grad= 0.8594 A.data= 2.1215\n",
      "Ep412: zero_grad(): A.grad= 0.0000 A.data= 2.1215 loss= 0.6289\n",
      "            step(): A.grad= 1.1215 A.data= 0.2528\n",
      "Ep413: zero_grad(): A.grad= 0.0000 A.data= 0.2528 loss= 0.2792\n",
      "            step(): A.grad=-0.7472 A.data=-0.1962\n",
      "Ep414: zero_grad(): A.grad= 0.0000 A.data=-0.1962 loss= 0.7155\n",
      "            step(): A.grad=-1.1962 A.data= 1.6276\n",
      "Ep415: zero_grad(): A.grad= 0.0000 A.data= 1.6276 loss= 0.1970\n",
      "            step(): A.grad= 0.6276 A.data= 2.2590\n",
      "Ep416: zero_grad(): A.grad= 0.0000 A.data= 2.2590 loss= 0.7926\n",
      "            step(): A.grad= 1.2590 A.data= 0.4983\n",
      "Ep417: zero_grad(): A.grad= 0.0000 A.data= 0.4983 loss= 0.1259\n",
      "            step(): A.grad=-0.5017 A.data=-0.3092\n",
      "Ep418: zero_grad(): A.grad= 0.0000 A.data=-0.3092 loss= 0.8570\n",
      "            step(): A.grad=-1.3092 A.data= 1.3708\n",
      "Ep419: zero_grad(): A.grad= 0.0000 A.data= 1.3708 loss= 0.0687\n",
      "            step(): A.grad= 0.3708 A.data= 2.3463\n",
      "Ep420: zero_grad(): A.grad= 0.0000 A.data= 2.3463 loss= 0.9062\n",
      "            step(): A.grad= 1.3463 A.data= 0.7638\n",
      "Ep421: zero_grad(): A.grad= 0.0000 A.data= 0.7638 loss= 0.0279\n",
      "            step(): A.grad=-0.2362 A.data=-0.3699\n",
      "Ep422: zero_grad(): A.grad= 0.0000 A.data=-0.3699 loss= 0.9383\n",
      "            step(): A.grad=-1.3699 A.data= 1.0992\n",
      "Ep423: zero_grad(): A.grad= 0.0000 A.data= 1.0992 loss= 0.0049\n",
      "            step(): A.grad= 0.0992 A.data= 2.3798\n",
      "Ep424: zero_grad(): A.grad= 0.0000 A.data= 2.3798 loss= 0.9519\n",
      "            step(): A.grad= 1.3798 A.data= 1.0388\n",
      "Ep425: zero_grad(): A.grad= 0.0000 A.data= 1.0388 loss= 0.0008\n",
      "            step(): A.grad= 0.0388 A.data=-0.3759\n",
      "Ep426: zero_grad(): A.grad= 0.0000 A.data=-0.3759 loss= 0.9466\n",
      "            step(): A.grad=-1.3759 A.data= 0.8236\n",
      "Ep427: zero_grad(): A.grad= 0.0000 A.data= 0.8236 loss= 0.0156\n",
      "            step(): A.grad=-0.1764 A.data= 2.3583\n",
      "Ep428: zero_grad(): A.grad= 0.0000 A.data= 2.3583 loss= 0.9225\n",
      "            step(): A.grad= 1.3583 A.data= 1.3122\n",
      "Ep429: zero_grad(): A.grad= 0.0000 A.data= 1.3122 loss= 0.0487\n",
      "            step(): A.grad= 0.3122 A.data=-0.3271\n",
      "Ep430: zero_grad(): A.grad= 0.0000 A.data=-0.3271 loss= 0.8805\n",
      "            step(): A.grad=-1.3271 A.data= 0.5551\n",
      "Ep431: zero_grad(): A.grad= 0.0000 A.data= 0.5551 loss= 0.0990\n",
      "            step(): A.grad=-0.4449 A.data= 2.2826\n",
      "Ep432: zero_grad(): A.grad= 0.0000 A.data= 2.2826 loss= 0.8225\n",
      "            step(): A.grad= 1.2826 A.data= 1.5732\n",
      "Ep433: zero_grad(): A.grad= 0.0000 A.data= 1.5732 loss= 0.1643\n",
      "            step(): A.grad= 0.5732 A.data=-0.2253\n",
      "Ep434: zero_grad(): A.grad= 0.0000 A.data=-0.2253 loss= 0.7506\n",
      "            step(): A.grad=-1.2253 A.data= 0.3043\n",
      "Ep435: zero_grad(): A.grad= 0.0000 A.data= 0.3043 loss= 0.2420\n",
      "            step(): A.grad=-0.6957 A.data= 2.1557\n",
      "Ep436: zero_grad(): A.grad= 0.0000 A.data= 2.1557 loss= 0.6678\n",
      "            step(): A.grad= 1.1557 A.data= 1.8113\n",
      "Ep437: zero_grad(): A.grad= 0.0000 A.data= 1.8113 loss= 0.3291\n",
      "            step(): A.grad= 0.8113 A.data=-0.0746\n",
      "Ep438: zero_grad(): A.grad= 0.0000 A.data=-0.0746 loss= 0.5773\n",
      "            step(): A.grad=-1.0746 A.data= 0.0813\n",
      "Ep439: zero_grad(): A.grad= 0.0000 A.data= 0.0813 loss= 0.4220\n",
      "            step(): A.grad=-0.9187 A.data= 1.9827\n",
      "Ep440: zero_grad(): A.grad= 0.0000 A.data= 1.9827 loss= 0.4828\n",
      "            step(): A.grad= 0.9827 A.data= 2.0170\n",
      "Ep441: zero_grad(): A.grad= 0.0000 A.data= 2.0170 loss= 0.5171\n",
      "            step(): A.grad= 1.0170 A.data= 0.1190\n",
      "Ep442: zero_grad(): A.grad= 0.0000 A.data= 0.1190 loss= 0.3881\n",
      "            step(): A.grad=-0.8810 A.data=-0.1051\n",
      "Ep443: zero_grad(): A.grad= 0.0000 A.data=-0.1051 loss= 0.6106\n",
      "            step(): A.grad=-1.1051 A.data= 1.7705\n",
      "Ep444: zero_grad(): A.grad= 0.0000 A.data= 1.7705 loss= 0.2968\n",
      "            step(): A.grad= 0.7705 A.data= 2.1821\n",
      "Ep445: zero_grad(): A.grad= 0.0000 A.data= 2.1821 loss= 0.6987\n",
      "            step(): A.grad= 1.1821 A.data= 0.3477\n",
      "Ep446: zero_grad(): A.grad= 0.0000 A.data= 0.3477 loss= 0.2127\n",
      "            step(): A.grad=-0.6523 A.data=-0.2474\n",
      "Ep447: zero_grad(): A.grad= 0.0000 A.data=-0.2474 loss= 0.7780\n",
      "            step(): A.grad=-1.2474 A.data= 1.5275\n",
      "Ep448: zero_grad(): A.grad= 0.0000 A.data= 1.5275 loss= 0.1391\n",
      "            step(): A.grad= 0.5275 A.data= 2.3001\n",
      "Ep449: zero_grad(): A.grad= 0.0000 A.data= 2.3001 loss= 0.8451\n",
      "            step(): A.grad= 1.3001 A.data= 0.6025\n",
      "Ep450: zero_grad(): A.grad= 0.0000 A.data= 0.6025 loss= 0.0790\n",
      "            step(): A.grad=-0.3975 A.data=-0.3399\n",
      "Ep451: zero_grad(): A.grad= 0.0000 A.data=-0.3399 loss= 0.8976\n",
      "            step(): A.grad=-1.3399 A.data= 1.2635\n",
      "Ep452: zero_grad(): A.grad= 0.0000 A.data= 1.2635 loss= 0.0347\n",
      "            step(): A.grad= 0.2635 A.data= 2.3662\n",
      "Ep453: zero_grad(): A.grad= 0.0000 A.data= 2.3662 loss= 0.9333\n",
      "            step(): A.grad= 1.3662 A.data= 0.8731\n",
      "Ep454: zero_grad(): A.grad= 0.0000 A.data= 0.8731 loss= 0.0081\n",
      "            step(): A.grad=-0.1269 A.data=-0.3789\n",
      "Ep455: zero_grad(): A.grad= 0.0000 A.data=-0.3789 loss= 0.9507\n",
      "            step(): A.grad=-1.3789 A.data= 0.9890\n",
      "Ep456: zero_grad(): A.grad= 0.0000 A.data= 0.9890 loss= 0.0001\n",
      "            step(): A.grad=-0.0110 A.data= 2.3778\n",
      "Ep457: zero_grad(): A.grad= 0.0000 A.data= 2.3778 loss= 0.9492\n",
      "            step(): A.grad= 1.3778 A.data= 1.1488\n",
      "Ep458: zero_grad(): A.grad= 0.0000 A.data= 1.1488 loss= 0.0111\n",
      "            step(): A.grad= 0.1488 A.data=-0.3629\n",
      "Ep459: zero_grad(): A.grad= 0.0000 A.data=-0.3629 loss= 0.9288\n",
      "            step(): A.grad=-1.3629 A.data= 0.7149\n",
      "Ep460: zero_grad(): A.grad= 0.0000 A.data= 0.7149 loss= 0.0406\n",
      "            step(): A.grad=-0.2851 A.data= 2.3344\n",
      "Ep461: zero_grad(): A.grad= 0.0000 A.data= 2.3344 loss= 0.8903\n",
      "            step(): A.grad= 1.3344 A.data= 1.4185\n",
      "Ep462: zero_grad(): A.grad= 0.0000 A.data= 1.4185 loss= 0.0876\n",
      "            step(): A.grad= 0.4185 A.data=-0.2926\n",
      "Ep463: zero_grad(): A.grad= 0.0000 A.data=-0.2926 loss= 0.8354\n",
      "            step(): A.grad=-1.2926 A.data= 0.4522\n",
      "Ep464: zero_grad(): A.grad= 0.0000 A.data= 0.4522 loss= 0.1500\n",
      "            step(): A.grad=-0.5478 A.data= 2.2378\n",
      "Ep465: zero_grad(): A.grad= 0.0000 A.data= 2.2378 loss= 0.7661\n",
      "            step(): A.grad= 1.2378 A.data= 1.6715\n",
      "Ep466: zero_grad(): A.grad= 0.0000 A.data= 1.6715 loss= 0.2255\n",
      "            step(): A.grad= 0.6715 A.data=-0.1706\n",
      "Ep467: zero_grad(): A.grad= 0.0000 A.data=-0.1706 loss= 0.6852\n",
      "            step(): A.grad=-1.1706 A.data= 0.2114\n",
      "Ep468: zero_grad(): A.grad= 0.0000 A.data= 0.2114 loss= 0.3109\n",
      "            step(): A.grad=-0.7886 A.data= 2.0918\n",
      "Ep469: zero_grad(): A.grad= 0.0000 A.data= 2.0918 loss= 0.5960\n",
      "            step(): A.grad= 1.0918 A.data= 1.8978\n",
      "Ep470: zero_grad(): A.grad= 0.0000 A.data= 1.8978 loss= 0.4030\n",
      "            step(): A.grad= 0.8978 A.data=-0.0020\n",
      "Ep471: zero_grad(): A.grad= 0.0000 A.data=-0.0020 loss= 0.5020\n",
      "            step(): A.grad=-1.0020 A.data= 0.0020\n",
      "Ep472: zero_grad(): A.grad= 0.0000 A.data= 0.0020 loss= 0.4980\n",
      "            step(): A.grad=-0.9980 A.data= 1.9022\n",
      "Ep473: zero_grad(): A.grad= 0.0000 A.data= 1.9022 loss= 0.4070\n",
      "            step(): A.grad= 0.9022 A.data= 2.0882\n",
      "Ep474: zero_grad(): A.grad= 0.0000 A.data= 2.0882 loss= 0.5921\n",
      "            step(): A.grad= 1.0882 A.data= 0.2066\n",
      "Ep475: zero_grad(): A.grad= 0.0000 A.data= 0.2066 loss= 0.3147\n",
      "            step(): A.grad=-0.7934 A.data=-0.1675\n",
      "Ep476: zero_grad(): A.grad= 0.0000 A.data=-0.1675 loss= 0.6816\n",
      "            step(): A.grad=-1.1675 A.data= 1.6766\n",
      "Ep477: zero_grad(): A.grad= 0.0000 A.data= 1.6766 loss= 0.2289\n",
      "            step(): A.grad= 0.6766 A.data= 2.2352\n",
      "Ep478: zero_grad(): A.grad= 0.0000 A.data= 2.2352 loss= 0.7629\n",
      "            step(): A.grad= 1.2352 A.data= 0.4469\n",
      "Ep479: zero_grad(): A.grad= 0.0000 A.data= 0.4469 loss= 0.1530\n",
      "            step(): A.grad=-0.5531 A.data=-0.2905\n",
      "Ep480: zero_grad(): A.grad= 0.0000 A.data=-0.2905 loss= 0.8327\n",
      "            step(): A.grad=-1.2905 A.data= 1.4241\n",
      "Ep481: zero_grad(): A.grad= 0.0000 A.data= 1.4241 loss= 0.0899\n",
      "            step(): A.grad= 0.4241 A.data= 2.3329\n",
      "Ep482: zero_grad(): A.grad= 0.0000 A.data= 2.3329 loss= 0.8883\n",
      "            step(): A.grad= 1.3329 A.data= 0.7092\n",
      "Ep483: zero_grad(): A.grad= 0.0000 A.data= 0.7092 loss= 0.0423\n",
      "            step(): A.grad=-0.2908 A.data=-0.3620\n",
      "Ep484: zero_grad(): A.grad= 0.0000 A.data=-0.3620 loss= 0.9275\n",
      "            step(): A.grad=-1.3620 A.data= 1.1546\n",
      "Ep485: zero_grad(): A.grad= 0.0000 A.data= 1.1546 loss= 0.0119\n",
      "            step(): A.grad= 0.1546 A.data= 2.3775\n",
      "Ep486: zero_grad(): A.grad= 0.0000 A.data= 2.3775 loss= 0.9487\n",
      "            step(): A.grad= 1.3775 A.data= 0.9832\n",
      "Ep487: zero_grad(): A.grad= 0.0000 A.data= 0.9832 loss= 0.0001\n",
      "            step(): A.grad=-0.0168 A.data=-0.3791\n",
      "Ep488: zero_grad(): A.grad= 0.0000 A.data=-0.3791 loss= 0.9510\n",
      "            step(): A.grad=-1.3791 A.data= 0.8789\n",
      "Ep489: zero_grad(): A.grad= 0.0000 A.data= 0.8789 loss= 0.0073\n",
      "            step(): A.grad=-0.1211 A.data= 2.3670\n",
      "Ep490: zero_grad(): A.grad= 0.0000 A.data= 2.3670 loss= 0.9344\n",
      "            step(): A.grad= 1.3670 A.data= 1.2578\n",
      "Ep491: zero_grad(): A.grad= 0.0000 A.data= 1.2578 loss= 0.0332\n",
      "            step(): A.grad= 0.2578 A.data=-0.3413\n",
      "Ep492: zero_grad(): A.grad= 0.0000 A.data=-0.3413 loss= 0.8995\n",
      "            step(): A.grad=-1.3413 A.data= 0.6081\n",
      "Ep493: zero_grad(): A.grad= 0.0000 A.data= 0.6081 loss= 0.0768\n",
      "            step(): A.grad=-0.3919 A.data= 2.3021\n",
      "Ep494: zero_grad(): A.grad= 0.0000 A.data= 2.3021 loss= 0.8477\n",
      "            step(): A.grad= 1.3021 A.data= 1.5221\n",
      "Ep495: zero_grad(): A.grad= 0.0000 A.data= 1.5221 loss= 0.1363\n",
      "            step(): A.grad= 0.5221 A.data=-0.2498\n",
      "Ep496: zero_grad(): A.grad= 0.0000 A.data=-0.2498 loss= 0.7811\n",
      "            step(): A.grad=-1.2498 A.data= 0.3529\n",
      "Ep497: zero_grad(): A.grad= 0.0000 A.data= 0.3529 loss= 0.2094\n",
      "            step(): A.grad=-0.6471 A.data= 2.1851\n",
      "Ep498: zero_grad(): A.grad= 0.0000 A.data= 2.1851 loss= 0.7023\n",
      "            step(): A.grad= 1.1851 A.data= 1.7656\n",
      "Ep499: zero_grad(): A.grad= 0.0000 A.data= 1.7656 loss= 0.2931\n",
      "            step(): A.grad= 0.7656 A.data=-0.1086\n",
      "Ep500: zero_grad(): A.grad= 0.0000 A.data=-0.1086 loss= 0.6145\n",
      "            step(): A.grad=-1.1086 A.data= 0.1235\n",
      "Ep501: zero_grad(): A.grad= 0.0000 A.data= 0.1235 loss= 0.3841\n",
      "            step(): A.grad=-0.8765 A.data= 2.0209\n",
      "Ep502: zero_grad(): A.grad= 0.0000 A.data= 2.0209 loss= 0.5211\n",
      "            step(): A.grad= 1.0209 A.data= 1.9786\n",
      "Ep503: zero_grad(): A.grad= 0.0000 A.data= 1.9786 loss= 0.4788\n",
      "            step(): A.grad= 0.9786 A.data= 0.0769\n",
      "Ep504: zero_grad(): A.grad= 0.0000 A.data= 0.0769 loss= 0.4260\n",
      "            step(): A.grad=-0.9231 A.data=-0.0709\n",
      "Ep505: zero_grad(): A.grad= 0.0000 A.data=-0.0709 loss= 0.5734\n",
      "            step(): A.grad=-1.0709 A.data= 1.8160\n",
      "Ep506: zero_grad(): A.grad= 0.0000 A.data= 1.8160 loss= 0.3329\n",
      "            step(): A.grad= 0.8160 A.data= 2.1525\n",
      "Ep507: zero_grad(): A.grad= 0.0000 A.data= 2.1525 loss= 0.6641\n",
      "            step(): A.grad= 1.1525 A.data= 0.2993\n",
      "Ep508: zero_grad(): A.grad= 0.0000 A.data= 0.2993 loss= 0.2455\n",
      "            step(): A.grad=-0.7007 A.data=-0.2226\n",
      "Ep509: zero_grad(): A.grad= 0.0000 A.data=-0.2226 loss= 0.7473\n",
      "            step(): A.grad=-1.2226 A.data= 1.5785\n",
      "Ep510: zero_grad(): A.grad= 0.0000 A.data= 1.5785 loss= 0.1673\n",
      "            step(): A.grad= 0.5785 A.data= 2.2804\n",
      "Ep511: zero_grad(): A.grad= 0.0000 A.data= 2.2804 loss= 0.8197\n",
      "            step(): A.grad= 1.2804 A.data= 0.5496\n",
      "Ep512: zero_grad(): A.grad= 0.0000 A.data= 0.5496 loss= 0.1014\n",
      "            step(): A.grad=-0.4504 A.data=-0.3254\n",
      "Ep513: zero_grad(): A.grad= 0.0000 A.data=-0.3254 loss= 0.8784\n",
      "            step(): A.grad=-1.3254 A.data= 1.3179\n",
      "Ep514: zero_grad(): A.grad= 0.0000 A.data= 1.3179 loss= 0.0505\n",
      "            step(): A.grad= 0.3179 A.data= 2.3572\n",
      "Ep515: zero_grad(): A.grad= 0.0000 A.data= 2.3572 loss= 0.9210\n",
      "            step(): A.grad= 1.3572 A.data= 0.8178\n",
      "Ep516: zero_grad(): A.grad= 0.0000 A.data= 0.8178 loss= 0.0166\n",
      "            step(): A.grad=-0.1822 A.data=-0.3755\n",
      "Ep517: zero_grad(): A.grad= 0.0000 A.data=-0.3755 loss= 0.9459\n",
      "            step(): A.grad=-1.3755 A.data= 1.0446\n",
      "Ep518: zero_grad(): A.grad= 0.0000 A.data= 1.0446 loss= 0.0010\n",
      "            step(): A.grad= 0.0446 A.data= 2.3799\n",
      "Ep519: zero_grad(): A.grad= 0.0000 A.data= 2.3799 loss= 0.9521\n",
      "            step(): A.grad= 1.3799 A.data= 1.0934\n",
      "Ep520: zero_grad(): A.grad= 0.0000 A.data= 1.0934 loss= 0.0044\n",
      "            step(): A.grad= 0.0934 A.data=-0.3706\n",
      "Ep521: zero_grad(): A.grad= 0.0000 A.data=-0.3706 loss= 0.9392\n",
      "            step(): A.grad=-1.3706 A.data= 0.7696\n",
      "Ep522: zero_grad(): A.grad= 0.0000 A.data= 0.7696 loss= 0.0265\n",
      "            step(): A.grad=-0.2304 A.data= 2.3475\n",
      "Ep523: zero_grad(): A.grad= 0.0000 A.data= 2.3475 loss= 0.9079\n",
      "            step(): A.grad= 1.3475 A.data= 1.3652\n",
      "Ep524: zero_grad(): A.grad= 0.0000 A.data= 1.3652 loss= 0.0667\n",
      "            step(): A.grad= 0.3652 A.data=-0.3110\n",
      "Ep525: zero_grad(): A.grad= 0.0000 A.data=-0.3110 loss= 0.8594\n",
      "            step(): A.grad=-1.3110 A.data= 0.5037\n",
      "Ep526: zero_grad(): A.grad= 0.0000 A.data= 0.5037 loss= 0.1231\n",
      "            step(): A.grad=-0.4963 A.data= 2.2614\n",
      "Ep527: zero_grad(): A.grad= 0.0000 A.data= 2.2614 loss= 0.7956\n",
      "            step(): A.grad= 1.2614 A.data= 1.6224\n",
      "Ep528: zero_grad(): A.grad= 0.0000 A.data= 1.6224 loss= 0.1937\n",
      "            step(): A.grad= 0.6224 A.data=-0.1991\n",
      "Ep529: zero_grad(): A.grad= 0.0000 A.data=-0.1991 loss= 0.7190\n",
      "            step(): A.grad=-1.1991 A.data= 0.2577\n",
      "Ep530: zero_grad(): A.grad= 0.0000 A.data= 0.2577 loss= 0.2755\n",
      "            step(): A.grad=-0.7423 A.data= 2.1249\n",
      "Ep531: zero_grad(): A.grad= 0.0000 A.data= 2.1249 loss= 0.6327\n",
      "            step(): A.grad= 1.1249 A.data= 1.8548\n",
      "Ep532: zero_grad(): A.grad= 0.0000 A.data= 1.8548 loss= 0.3654\n",
      "            step(): A.grad= 0.8548 A.data=-0.0394\n",
      "Ep533: zero_grad(): A.grad= 0.0000 A.data=-0.0394 loss= 0.5402\n",
      "            step(): A.grad=-1.0394 A.data= 0.0412\n",
      "Ep534: zero_grad(): A.grad= 0.0000 A.data= 0.0412 loss= 0.4596\n",
      "            step(): A.grad=-0.9588 A.data= 1.9436\n",
      "Ep535: zero_grad(): A.grad= 0.0000 A.data= 1.9436 loss= 0.4451\n",
      "            step(): A.grad= 0.9436 A.data= 2.0531\n",
      "Ep536: zero_grad(): A.grad= 0.0000 A.data= 2.0531 loss= 0.5545\n",
      "            step(): A.grad= 1.0531 A.data= 0.1618\n",
      "Ep537: zero_grad(): A.grad= 0.0000 A.data= 0.1618 loss= 0.3513\n",
      "            step(): A.grad=-0.8382 A.data=-0.1369\n",
      "Ep538: zero_grad(): A.grad= 0.0000 A.data=-0.1369 loss= 0.6463\n",
      "            step(): A.grad=-1.1369 A.data= 1.7245\n",
      "Ep539: zero_grad(): A.grad= 0.0000 A.data= 1.7245 loss= 0.2625\n",
      "            step(): A.grad= 0.7245 A.data= 2.2094\n",
      "Ep540: zero_grad(): A.grad= 0.0000 A.data= 2.2094 loss= 0.7313\n",
      "            step(): A.grad= 1.2094 A.data= 0.3964\n",
      "Ep541: zero_grad(): A.grad= 0.0000 A.data= 0.3964 loss= 0.1822\n",
      "            step(): A.grad=-0.6036 A.data=-0.2698\n",
      "Ep542: zero_grad(): A.grad= 0.0000 A.data=-0.2698 loss= 0.8061\n",
      "            step(): A.grad=-1.2698 A.data= 1.4766\n",
      "Ep543: zero_grad(): A.grad= 0.0000 A.data= 1.4766 loss= 0.1136\n",
      "            step(): A.grad= 0.4766 A.data= 2.3174\n",
      "Ep544: zero_grad(): A.grad= 0.0000 A.data= 2.3174 loss= 0.8678\n",
      "            step(): A.grad= 1.3174 A.data= 0.6551\n",
      "Ep545: zero_grad(): A.grad= 0.0000 A.data= 0.6551 loss= 0.0595\n",
      "            step(): A.grad=-0.3449 A.data=-0.3519\n",
      "Ep546: zero_grad(): A.grad= 0.0000 A.data=-0.3519 loss= 0.9138\n",
      "            step(): A.grad=-1.3519 A.data= 1.2097\n",
      "Ep547: zero_grad(): A.grad= 0.0000 A.data= 1.2097 loss= 0.0220\n",
      "            step(): A.grad= 0.2097 A.data= 2.3729\n",
      "Ep548: zero_grad(): A.grad= 0.0000 A.data= 2.3729 loss= 0.9424\n",
      "            step(): A.grad= 1.3729 A.data= 0.9276\n",
      "Ep549: zero_grad(): A.grad= 0.0000 A.data= 0.9276 loss= 0.0026\n",
      "            step(): A.grad=-0.0724 A.data=-0.3801\n",
      "Ep550: zero_grad(): A.grad= 0.0000 A.data=-0.3801 loss= 0.9524\n",
      "            step(): A.grad=-1.3801 A.data= 0.9344\n",
      "Ep551: zero_grad(): A.grad= 0.0000 A.data= 0.9344 loss= 0.0022\n",
      "            step(): A.grad=-0.0656 A.data= 2.3736\n",
      "Ep552: zero_grad(): A.grad= 0.0000 A.data= 2.3736 loss= 0.9433\n",
      "            step(): A.grad= 1.3736 A.data= 1.2030\n",
      "Ep553: zero_grad(): A.grad= 0.0000 A.data= 1.2030 loss= 0.0206\n",
      "            step(): A.grad= 0.2030 A.data=-0.3533\n",
      "Ep554: zero_grad(): A.grad= 0.0000 A.data=-0.3533 loss= 0.9157\n",
      "            step(): A.grad=-1.3533 A.data= 0.6617\n",
      "Ep555: zero_grad(): A.grad= 0.0000 A.data= 0.6617 loss= 0.0572\n",
      "            step(): A.grad=-0.3383 A.data= 2.3194\n",
      "Ep556: zero_grad(): A.grad= 0.0000 A.data= 2.3194 loss= 0.8705\n",
      "            step(): A.grad= 1.3194 A.data= 1.4702\n",
      "Ep557: zero_grad(): A.grad= 0.0000 A.data= 1.4702 loss= 0.1106\n",
      "            step(): A.grad= 0.4702 A.data=-0.2724\n",
      "Ep558: zero_grad(): A.grad= 0.0000 A.data=-0.2724 loss= 0.8095\n",
      "            step(): A.grad=-1.2724 A.data= 0.4025\n",
      "Ep559: zero_grad(): A.grad= 0.0000 A.data= 0.4025 loss= 0.1785\n",
      "            step(): A.grad=-0.5975 A.data= 2.2127\n",
      "Ep560: zero_grad(): A.grad= 0.0000 A.data= 2.2127 loss= 0.7353\n",
      "            step(): A.grad= 1.2127 A.data= 1.7187\n",
      "Ep561: zero_grad(): A.grad= 0.0000 A.data= 1.7187 loss= 0.2583\n",
      "            step(): A.grad= 0.7187 A.data=-0.1408\n",
      "Ep562: zero_grad(): A.grad= 0.0000 A.data=-0.1408 loss= 0.6507\n",
      "            step(): A.grad=-1.1408 A.data= 0.1672\n",
      "Ep563: zero_grad(): A.grad= 0.0000 A.data= 0.1672 loss= 0.3468\n",
      "            step(): A.grad=-0.8328 A.data= 2.0575\n",
      "Ep564: zero_grad(): A.grad= 0.0000 A.data= 2.0575 loss= 0.5592\n",
      "            step(): A.grad= 1.0575 A.data= 1.9386\n",
      "Ep565: zero_grad(): A.grad= 0.0000 A.data= 1.9386 loss= 0.4405\n",
      "            step(): A.grad= 0.9386 A.data= 0.0363\n",
      "Ep566: zero_grad(): A.grad= 0.0000 A.data= 0.0363 loss= 0.4643\n",
      "            step(): A.grad=-0.9637 A.data=-0.0349\n",
      "Ep567: zero_grad(): A.grad= 0.0000 A.data=-0.0349 loss= 0.5355\n",
      "            step(): A.grad=-1.0349 A.data= 1.8602\n",
      "Ep568: zero_grad(): A.grad= 0.0000 A.data= 1.8602 loss= 0.3699\n",
      "            step(): A.grad= 0.8602 A.data= 2.1210\n",
      "Ep569: zero_grad(): A.grad= 0.0000 A.data= 2.1210 loss= 0.6283\n",
      "            step(): A.grad= 1.1210 A.data= 0.2519\n",
      "Ep570: zero_grad(): A.grad= 0.0000 A.data= 0.2519 loss= 0.2798\n",
      "            step(): A.grad=-0.7481 A.data=-0.1958\n",
      "Ep571: zero_grad(): A.grad= 0.0000 A.data=-0.1958 loss= 0.7149\n",
      "            step(): A.grad=-1.1958 A.data= 1.6285\n",
      "Ep572: zero_grad(): A.grad= 0.0000 A.data= 1.6285 loss= 0.1975\n",
      "            step(): A.grad= 0.6285 A.data= 2.2586\n",
      "Ep573: zero_grad(): A.grad= 0.0000 A.data= 2.2586 loss= 0.7920\n",
      "            step(): A.grad= 1.2586 A.data= 0.4974\n",
      "Ep574: zero_grad(): A.grad= 0.0000 A.data= 0.4974 loss= 0.1263\n",
      "            step(): A.grad=-0.5026 A.data=-0.3089\n",
      "Ep575: zero_grad(): A.grad= 0.0000 A.data=-0.3089 loss= 0.8566\n",
      "            step(): A.grad=-1.3089 A.data= 1.3717\n",
      "Ep576: zero_grad(): A.grad= 0.0000 A.data= 1.3717 loss= 0.0691\n",
      "            step(): A.grad= 0.3717 A.data= 2.3460\n",
      "Ep577: zero_grad(): A.grad= 0.0000 A.data= 2.3460 loss= 0.9059\n",
      "            step(): A.grad= 1.3460 A.data= 0.7629\n",
      "Ep578: zero_grad(): A.grad= 0.0000 A.data= 0.7629 loss= 0.0281\n",
      "            step(): A.grad=-0.2371 A.data=-0.3698\n",
      "Ep579: zero_grad(): A.grad= 0.0000 A.data=-0.3698 loss= 0.9381\n",
      "            step(): A.grad=-1.3698 A.data= 1.1002\n",
      "Ep580: zero_grad(): A.grad= 0.0000 A.data= 1.1002 loss= 0.0050\n",
      "            step(): A.grad= 0.1002 A.data= 2.3798\n",
      "Ep581: zero_grad(): A.grad= 0.0000 A.data= 2.3798 loss= 0.9519\n",
      "            step(): A.grad= 1.3798 A.data= 1.0378\n",
      "Ep582: zero_grad(): A.grad= 0.0000 A.data= 1.0378 loss= 0.0007\n",
      "            step(): A.grad= 0.0378 A.data=-0.3760\n",
      "Ep583: zero_grad(): A.grad= 0.0000 A.data=-0.3760 loss= 0.9467\n",
      "            step(): A.grad=-1.3760 A.data= 0.8246\n",
      "Ep584: zero_grad(): A.grad= 0.0000 A.data= 0.8246 loss= 0.0154\n",
      "            step(): A.grad=-0.1754 A.data= 2.3585\n",
      "Ep585: zero_grad(): A.grad= 0.0000 A.data= 2.3585 loss= 0.9227\n",
      "            step(): A.grad= 1.3585 A.data= 1.3113\n",
      "Ep586: zero_grad(): A.grad= 0.0000 A.data= 1.3113 loss= 0.0484\n",
      "            step(): A.grad= 0.3113 A.data=-0.3273\n",
      "Ep587: zero_grad(): A.grad= 0.0000 A.data=-0.3273 loss= 0.8809\n",
      "            step(): A.grad=-1.3273 A.data= 0.5560\n",
      "Ep588: zero_grad(): A.grad= 0.0000 A.data= 0.5560 loss= 0.0986\n",
      "            step(): A.grad=-0.4440 A.data= 2.2829\n",
      "Ep589: zero_grad(): A.grad= 0.0000 A.data= 2.2829 loss= 0.8230\n",
      "            step(): A.grad= 1.2829 A.data= 1.5723\n",
      "Ep590: zero_grad(): A.grad= 0.0000 A.data= 1.5723 loss= 0.1638\n",
      "            step(): A.grad= 0.5723 A.data=-0.2257\n",
      "Ep591: zero_grad(): A.grad= 0.0000 A.data=-0.2257 loss= 0.7512\n",
      "            step(): A.grad=-1.2257 A.data= 0.3051\n",
      "Ep592: zero_grad(): A.grad= 0.0000 A.data= 0.3051 loss= 0.2414\n",
      "            step(): A.grad=-0.6949 A.data= 2.1562\n",
      "Ep593: zero_grad(): A.grad= 0.0000 A.data= 2.1562 loss= 0.6684\n",
      "            step(): A.grad= 1.1562 A.data= 1.8105\n",
      "Ep594: zero_grad(): A.grad= 0.0000 A.data= 1.8105 loss= 0.3284\n",
      "            step(): A.grad= 0.8105 A.data=-0.0752\n",
      "Ep595: zero_grad(): A.grad= 0.0000 A.data=-0.0752 loss= 0.5780\n",
      "            step(): A.grad=-1.0752 A.data= 0.0820\n",
      "Ep596: zero_grad(): A.grad= 0.0000 A.data= 0.0820 loss= 0.4214\n",
      "            step(): A.grad=-0.9180 A.data= 1.9834\n",
      "Ep597: zero_grad(): A.grad= 0.0000 A.data= 1.9834 loss= 0.4835\n",
      "            step(): A.grad= 0.9834 A.data= 2.0163\n",
      "Ep598: zero_grad(): A.grad= 0.0000 A.data= 2.0163 loss= 0.5165\n",
      "            step(): A.grad= 1.0163 A.data= 0.1183\n",
      "Ep599: zero_grad(): A.grad= 0.0000 A.data= 0.1183 loss= 0.3887\n",
      "            step(): A.grad=-0.8817 A.data=-0.1045\n",
      "Ep600: zero_grad(): A.grad= 0.0000 A.data=-0.1045 loss= 0.6100\n",
      "            step(): A.grad=-1.1045 A.data= 1.7713\n",
      "Ep601: zero_grad(): A.grad= 0.0000 A.data= 1.7713 loss= 0.2974\n",
      "            step(): A.grad= 0.7713 A.data= 2.1816\n",
      "Ep602: zero_grad(): A.grad= 0.0000 A.data= 2.1816 loss= 0.6981\n",
      "            step(): A.grad= 1.1816 A.data= 0.3469\n",
      "Ep603: zero_grad(): A.grad= 0.0000 A.data= 0.3469 loss= 0.2133\n",
      "            step(): A.grad=-0.6531 A.data=-0.2469\n",
      "Ep604: zero_grad(): A.grad= 0.0000 A.data=-0.2469 loss= 0.7774\n",
      "            step(): A.grad=-1.2469 A.data= 1.5284\n",
      "Ep605: zero_grad(): A.grad= 0.0000 A.data= 1.5284 loss= 0.1396\n",
      "            step(): A.grad= 0.5284 A.data= 2.2998\n",
      "Ep606: zero_grad(): A.grad= 0.0000 A.data= 2.2998 loss= 0.8447\n",
      "            step(): A.grad= 1.2998 A.data= 0.6016\n",
      "Ep607: zero_grad(): A.grad= 0.0000 A.data= 0.6016 loss= 0.0794\n",
      "            step(): A.grad=-0.3984 A.data=-0.3396\n",
      "Ep608: zero_grad(): A.grad= 0.0000 A.data=-0.3396 loss= 0.8973\n",
      "            step(): A.grad=-1.3396 A.data= 1.2645\n",
      "Ep609: zero_grad(): A.grad= 0.0000 A.data= 1.2645 loss= 0.0350\n",
      "            step(): A.grad= 0.2645 A.data= 2.3661\n",
      "Ep610: zero_grad(): A.grad= 0.0000 A.data= 2.3661 loss= 0.9331\n",
      "            step(): A.grad= 1.3661 A.data= 0.8721\n",
      "Ep611: zero_grad(): A.grad= 0.0000 A.data= 0.8721 loss= 0.0082\n",
      "            step(): A.grad=-0.1279 A.data=-0.3789\n",
      "Ep612: zero_grad(): A.grad= 0.0000 A.data=-0.3789 loss= 0.9506\n",
      "            step(): A.grad=-1.3789 A.data= 0.9900\n",
      "Ep613: zero_grad(): A.grad= 0.0000 A.data= 0.9900 loss= 0.0001\n",
      "            step(): A.grad=-0.0100 A.data= 2.3779\n",
      "Ep614: zero_grad(): A.grad= 0.0000 A.data= 2.3779 loss= 0.9493\n",
      "            step(): A.grad= 1.3779 A.data= 1.1478\n",
      "Ep615: zero_grad(): A.grad= 0.0000 A.data= 1.1478 loss= 0.0109\n",
      "            step(): A.grad= 0.1478 A.data=-0.3631\n",
      "Ep616: zero_grad(): A.grad= 0.0000 A.data=-0.3631 loss= 0.9290\n",
      "            step(): A.grad=-1.3631 A.data= 0.7159\n",
      "Ep617: zero_grad(): A.grad= 0.0000 A.data= 0.7159 loss= 0.0404\n",
      "            step(): A.grad=-0.2841 A.data= 2.3347\n",
      "Ep618: zero_grad(): A.grad= 0.0000 A.data= 2.3347 loss= 0.8907\n",
      "            step(): A.grad= 1.3347 A.data= 1.4176\n",
      "Ep619: zero_grad(): A.grad= 0.0000 A.data= 1.4176 loss= 0.0872\n",
      "            step(): A.grad= 0.4176 A.data=-0.2929\n",
      "Ep620: zero_grad(): A.grad= 0.0000 A.data=-0.2929 loss= 0.8358\n",
      "            step(): A.grad=-1.2929 A.data= 0.4531\n",
      "Ep621: zero_grad(): A.grad= 0.0000 A.data= 0.4531 loss= 0.1495\n",
      "            step(): A.grad=-0.5469 A.data= 2.2382\n",
      "Ep622: zero_grad(): A.grad= 0.0000 A.data= 2.2382 loss= 0.7666\n",
      "            step(): A.grad= 1.2382 A.data= 1.6707\n",
      "Ep623: zero_grad(): A.grad= 0.0000 A.data= 1.6707 loss= 0.2249\n",
      "            step(): A.grad= 0.6707 A.data=-0.1712\n",
      "Ep624: zero_grad(): A.grad= 0.0000 A.data=-0.1712 loss= 0.6858\n",
      "            step(): A.grad=-1.1712 A.data= 0.2122\n",
      "Ep625: zero_grad(): A.grad= 0.0000 A.data= 0.2122 loss= 0.3103\n",
      "            step(): A.grad=-0.7878 A.data= 2.0924\n",
      "Ep626: zero_grad(): A.grad= 0.0000 A.data= 2.0924 loss= 0.5967\n",
      "            step(): A.grad= 1.0924 A.data= 1.8970\n",
      "Ep627: zero_grad(): A.grad= 0.0000 A.data= 1.8970 loss= 0.4023\n",
      "            step(): A.grad= 0.8970 A.data=-0.0027\n",
      "Ep628: zero_grad(): A.grad= 0.0000 A.data=-0.0027 loss= 0.5027\n",
      "            step(): A.grad=-1.0027 A.data= 0.0027\n",
      "Ep629: zero_grad(): A.grad= 0.0000 A.data= 0.0027 loss= 0.4973\n",
      "            step(): A.grad=-0.9973 A.data= 1.9029\n",
      "Ep630: zero_grad(): A.grad= 0.0000 A.data= 1.9029 loss= 0.4077\n",
      "            step(): A.grad= 0.9029 A.data= 2.0876\n",
      "Ep631: zero_grad(): A.grad= 0.0000 A.data= 2.0876 loss= 0.5914\n",
      "            step(): A.grad= 1.0876 A.data= 0.2058\n",
      "Ep632: zero_grad(): A.grad= 0.0000 A.data= 0.2058 loss= 0.3154\n",
      "            step(): A.grad=-0.7942 A.data=-0.1670\n",
      "Ep633: zero_grad(): A.grad= 0.0000 A.data=-0.1670 loss= 0.6810\n",
      "            step(): A.grad=-1.1670 A.data= 1.6775\n",
      "Ep634: zero_grad(): A.grad= 0.0000 A.data= 1.6775 loss= 0.2295\n",
      "            step(): A.grad= 0.6775 A.data= 2.2348\n",
      "Ep635: zero_grad(): A.grad= 0.0000 A.data= 2.2348 loss= 0.7623\n",
      "            step(): A.grad= 1.2348 A.data= 0.4460\n",
      "Ep636: zero_grad(): A.grad= 0.0000 A.data= 0.4460 loss= 0.1535\n",
      "            step(): A.grad=-0.5540 A.data=-0.2902\n",
      "Ep637: zero_grad(): A.grad= 0.0000 A.data=-0.2902 loss= 0.8323\n",
      "            step(): A.grad=-1.2902 A.data= 1.4250\n",
      "Ep638: zero_grad(): A.grad= 0.0000 A.data= 1.4250 loss= 0.0903\n",
      "            step(): A.grad= 0.4250 A.data= 2.3327\n",
      "Ep639: zero_grad(): A.grad= 0.0000 A.data= 2.3327 loss= 0.8880\n",
      "            step(): A.grad= 1.3327 A.data= 0.7083\n",
      "Ep640: zero_grad(): A.grad= 0.0000 A.data= 0.7083 loss= 0.0426\n",
      "            step(): A.grad=-0.2917 A.data=-0.3618\n",
      "Ep641: zero_grad(): A.grad= 0.0000 A.data=-0.3618 loss= 0.9273\n",
      "            step(): A.grad=-1.3618 A.data= 1.1555\n",
      "Ep642: zero_grad(): A.grad= 0.0000 A.data= 1.1555 loss= 0.0121\n",
      "            step(): A.grad= 0.1555 A.data= 2.3774\n",
      "Ep643: zero_grad(): A.grad= 0.0000 A.data= 2.3774 loss= 0.9486\n",
      "            step(): A.grad= 1.3774 A.data= 0.9822\n",
      "Ep644: zero_grad(): A.grad= 0.0000 A.data= 0.9822 loss= 0.0002\n",
      "            step(): A.grad=-0.0178 A.data=-0.3792\n",
      "Ep645: zero_grad(): A.grad= 0.0000 A.data=-0.3792 loss= 0.9511\n",
      "            step(): A.grad=-1.3792 A.data= 0.8799\n",
      "Ep646: zero_grad(): A.grad= 0.0000 A.data= 0.8799 loss= 0.0072\n",
      "            step(): A.grad=-0.1201 A.data= 2.3672\n",
      "Ep647: zero_grad(): A.grad= 0.0000 A.data= 2.3672 loss= 0.9346\n",
      "            step(): A.grad= 1.3672 A.data= 1.2568\n",
      "Ep648: zero_grad(): A.grad= 0.0000 A.data= 1.2568 loss= 0.0330\n",
      "            step(): A.grad= 0.2568 A.data=-0.3415\n",
      "Ep649: zero_grad(): A.grad= 0.0000 A.data=-0.3415 loss= 0.8998\n",
      "            step(): A.grad=-1.3415 A.data= 0.6090\n",
      "Ep650: zero_grad(): A.grad= 0.0000 A.data= 0.6090 loss= 0.0764\n",
      "            step(): A.grad=-0.3910 A.data= 2.3024\n",
      "Ep651: zero_grad(): A.grad= 0.0000 A.data= 2.3024 loss= 0.8481\n",
      "            step(): A.grad= 1.3024 A.data= 1.5212\n",
      "Ep652: zero_grad(): A.grad= 0.0000 A.data= 1.5212 loss= 0.1358\n",
      "            step(): A.grad= 0.5212 A.data=-0.2503\n",
      "Ep653: zero_grad(): A.grad= 0.0000 A.data=-0.2503 loss= 0.7816\n",
      "            step(): A.grad=-1.2503 A.data= 0.3538\n",
      "Ep654: zero_grad(): A.grad= 0.0000 A.data= 0.3538 loss= 0.2088\n",
      "            step(): A.grad=-0.6462 A.data= 2.1856\n",
      "Ep655: zero_grad(): A.grad= 0.0000 A.data= 2.1856 loss= 0.7029\n",
      "            step(): A.grad= 1.1856 A.data= 1.7648\n",
      "Ep656: zero_grad(): A.grad= 0.0000 A.data= 1.7648 loss= 0.2925\n",
      "            step(): A.grad= 0.7648 A.data=-0.1092\n",
      "Ep657: zero_grad(): A.grad= 0.0000 A.data=-0.1092 loss= 0.6151\n",
      "            step(): A.grad=-1.1092 A.data= 0.1243\n",
      "Ep658: zero_grad(): A.grad= 0.0000 A.data= 0.1243 loss= 0.3835\n",
      "            step(): A.grad=-0.8757 A.data= 2.0216\n",
      "Ep659: zero_grad(): A.grad= 0.0000 A.data= 2.0216 loss= 0.5218\n",
      "            step(): A.grad= 1.0216 A.data= 1.9779\n",
      "Ep660: zero_grad(): A.grad= 0.0000 A.data= 1.9779 loss= 0.4781\n",
      "            step(): A.grad= 0.9779 A.data= 0.0762\n",
      "Ep661: zero_grad(): A.grad= 0.0000 A.data= 0.0762 loss= 0.4267\n",
      "            step(): A.grad=-0.9238 A.data=-0.0703\n",
      "Ep662: zero_grad(): A.grad= 0.0000 A.data=-0.0703 loss= 0.5727\n",
      "            step(): A.grad=-1.0703 A.data= 1.8168\n",
      "Ep663: zero_grad(): A.grad= 0.0000 A.data= 1.8168 loss= 0.3336\n",
      "            step(): A.grad= 0.8168 A.data= 2.1519\n",
      "Ep664: zero_grad(): A.grad= 0.0000 A.data= 2.1519 loss= 0.6635\n",
      "            step(): A.grad= 1.1519 A.data= 0.2984\n",
      "Ep665: zero_grad(): A.grad= 0.0000 A.data= 0.2984 loss= 0.2461\n",
      "            step(): A.grad=-0.7016 A.data=-0.2221\n",
      "Ep666: zero_grad(): A.grad= 0.0000 A.data=-0.2221 loss= 0.7468\n",
      "            step(): A.grad=-1.2221 A.data= 1.5794\n",
      "Ep667: zero_grad(): A.grad= 0.0000 A.data= 1.5794 loss= 0.1678\n",
      "            step(): A.grad= 0.5794 A.data= 2.2800\n",
      "Ep668: zero_grad(): A.grad= 0.0000 A.data= 2.2800 loss= 0.8192\n",
      "            step(): A.grad= 1.2800 A.data= 0.5486\n",
      "Ep669: zero_grad(): A.grad= 0.0000 A.data= 0.5486 loss= 0.1019\n",
      "            step(): A.grad=-0.4514 A.data=-0.3252\n",
      "Ep670: zero_grad(): A.grad= 0.0000 A.data=-0.3252 loss= 0.8780\n",
      "            step(): A.grad=-1.3252 A.data= 1.3188\n",
      "Ep671: zero_grad(): A.grad= 0.0000 A.data= 1.3188 loss= 0.0508\n",
      "            step(): A.grad= 0.3188 A.data= 2.3571\n",
      "Ep672: zero_grad(): A.grad= 0.0000 A.data= 2.3571 loss= 0.9208\n",
      "            step(): A.grad= 1.3571 A.data= 0.8169\n",
      "Ep673: zero_grad(): A.grad= 0.0000 A.data= 0.8169 loss= 0.0168\n",
      "            step(): A.grad=-0.1831 A.data=-0.3754\n",
      "Ep674: zero_grad(): A.grad= 0.0000 A.data=-0.3754 loss= 0.9458\n",
      "            step(): A.grad=-1.3754 A.data= 1.0456\n",
      "Ep675: zero_grad(): A.grad= 0.0000 A.data= 1.0456 loss= 0.0010\n",
      "            step(): A.grad= 0.0456 A.data= 2.3799\n",
      "Ep676: zero_grad(): A.grad= 0.0000 A.data= 2.3799 loss= 0.9521\n",
      "            step(): A.grad= 1.3799 A.data= 1.0924\n",
      "Ep677: zero_grad(): A.grad= 0.0000 A.data= 1.0924 loss= 0.0043\n",
      "            step(): A.grad= 0.0924 A.data=-0.3707\n",
      "Ep678: zero_grad(): A.grad= 0.0000 A.data=-0.3707 loss= 0.9394\n",
      "            step(): A.grad=-1.3707 A.data= 0.7705\n",
      "Ep679: zero_grad(): A.grad= 0.0000 A.data= 0.7705 loss= 0.0263\n",
      "            step(): A.grad=-0.2295 A.data= 2.3477\n",
      "Ep680: zero_grad(): A.grad= 0.0000 A.data= 2.3477 loss= 0.9082\n",
      "            step(): A.grad= 1.3477 A.data= 1.3642\n",
      "Ep681: zero_grad(): A.grad= 0.0000 A.data= 1.3642 loss= 0.0663\n",
      "            step(): A.grad= 0.3642 A.data=-0.3113\n",
      "Ep682: zero_grad(): A.grad= 0.0000 A.data=-0.3113 loss= 0.8598\n",
      "            step(): A.grad=-1.3113 A.data= 0.5046\n",
      "Ep683: zero_grad(): A.grad= 0.0000 A.data= 0.5046 loss= 0.1227\n",
      "            step(): A.grad=-0.4954 A.data= 2.2618\n",
      "Ep684: zero_grad(): A.grad= 0.0000 A.data= 2.2618 loss= 0.7960\n",
      "            step(): A.grad= 1.2618 A.data= 1.6215\n",
      "Ep685: zero_grad(): A.grad= 0.0000 A.data= 1.6215 loss= 0.1932\n",
      "            step(): A.grad= 0.6215 A.data=-0.1996\n",
      "Ep686: zero_grad(): A.grad= 0.0000 A.data=-0.1996 loss= 0.7196\n",
      "            step(): A.grad=-1.1996 A.data= 0.2585\n",
      "Ep687: zero_grad(): A.grad= 0.0000 A.data= 0.2585 loss= 0.2749\n",
      "            step(): A.grad=-0.7415 A.data= 2.1255\n",
      "Ep688: zero_grad(): A.grad= 0.0000 A.data= 2.1255 loss= 0.6334\n",
      "            step(): A.grad= 1.1255 A.data= 1.8541\n",
      "Ep689: zero_grad(): A.grad= 0.0000 A.data= 1.8541 loss= 0.3647\n",
      "            step(): A.grad= 0.8541 A.data=-0.0401\n",
      "Ep690: zero_grad(): A.grad= 0.0000 A.data=-0.0401 loss= 0.5409\n",
      "            step(): A.grad=-1.0401 A.data= 0.0419\n",
      "Ep691: zero_grad(): A.grad= 0.0000 A.data= 0.0419 loss= 0.4589\n",
      "            step(): A.grad=-0.9581 A.data= 1.9443\n",
      "Ep692: zero_grad(): A.grad= 0.0000 A.data= 1.9443 loss= 0.4458\n",
      "            step(): A.grad= 0.9443 A.data= 2.0525\n",
      "Ep693: zero_grad(): A.grad= 0.0000 A.data= 2.0525 loss= 0.5539\n",
      "            step(): A.grad= 1.0525 A.data= 0.1610\n",
      "Ep694: zero_grad(): A.grad= 0.0000 A.data= 0.1610 loss= 0.3520\n",
      "            step(): A.grad=-0.8390 A.data=-0.1364\n",
      "Ep695: zero_grad(): A.grad= 0.0000 A.data=-0.1364 loss= 0.6457\n",
      "            step(): A.grad=-1.1364 A.data= 1.7254\n",
      "Ep696: zero_grad(): A.grad= 0.0000 A.data= 1.7254 loss= 0.2631\n",
      "            step(): A.grad= 0.7254 A.data= 2.2089\n",
      "Ep697: zero_grad(): A.grad= 0.0000 A.data= 2.2089 loss= 0.7308\n",
      "            step(): A.grad= 1.2089 A.data= 0.3955\n",
      "Ep698: zero_grad(): A.grad= 0.0000 A.data= 0.3955 loss= 0.1827\n",
      "            step(): A.grad=-0.6045 A.data=-0.2694\n",
      "Ep699: zero_grad(): A.grad= 0.0000 A.data=-0.2694 loss= 0.8057\n",
      "            step(): A.grad=-1.2694 A.data= 1.4775\n",
      "Ep700: zero_grad(): A.grad= 0.0000 A.data= 1.4775 loss= 0.1140\n",
      "            step(): A.grad= 0.4775 A.data= 2.3171\n",
      "Ep701: zero_grad(): A.grad= 0.0000 A.data= 2.3171 loss= 0.8674\n",
      "            step(): A.grad= 1.3171 A.data= 0.6542\n",
      "Ep702: zero_grad(): A.grad= 0.0000 A.data= 0.6542 loss= 0.0598\n",
      "            step(): A.grad=-0.3458 A.data=-0.3517\n",
      "Ep703: zero_grad(): A.grad= 0.0000 A.data=-0.3517 loss= 0.9136\n",
      "            step(): A.grad=-1.3517 A.data= 1.2107\n",
      "Ep704: zero_grad(): A.grad= 0.0000 A.data= 1.2107 loss= 0.0222\n",
      "            step(): A.grad= 0.2107 A.data= 2.3728\n",
      "Ep705: zero_grad(): A.grad= 0.0000 A.data= 2.3728 loss= 0.9423\n",
      "            step(): A.grad= 1.3728 A.data= 0.9266\n",
      "Ep706: zero_grad(): A.grad= 0.0000 A.data= 0.9266 loss= 0.0027\n",
      "            step(): A.grad=-0.0734 A.data=-0.3801\n",
      "Ep707: zero_grad(): A.grad= 0.0000 A.data=-0.3801 loss= 0.9524\n",
      "            step(): A.grad=-1.3801 A.data= 0.9354\n",
      "Ep708: zero_grad(): A.grad= 0.0000 A.data= 0.9354 loss= 0.0021\n",
      "            step(): A.grad=-0.0646 A.data= 2.3737\n",
      "Ep709: zero_grad(): A.grad= 0.0000 A.data= 2.3737 loss= 0.9435\n",
      "            step(): A.grad= 1.3737 A.data= 1.2020\n",
      "Ep710: zero_grad(): A.grad= 0.0000 A.data= 1.2020 loss= 0.0204\n",
      "            step(): A.grad= 0.2020 A.data=-0.3535\n",
      "Ep711: zero_grad(): A.grad= 0.0000 A.data=-0.3535 loss= 0.9159\n",
      "            step(): A.grad=-1.3535 A.data= 0.6627\n",
      "Ep712: zero_grad(): A.grad= 0.0000 A.data= 0.6627 loss= 0.0569\n",
      "            step(): A.grad=-0.3373 A.data= 2.3197\n",
      "Ep713: zero_grad(): A.grad= 0.0000 A.data= 2.3197 loss= 0.8708\n",
      "            step(): A.grad= 1.3197 A.data= 1.4693\n",
      "Ep714: zero_grad(): A.grad= 0.0000 A.data= 1.4693 loss= 0.1101\n",
      "            step(): A.grad= 0.4693 A.data=-0.2728\n",
      "Ep715: zero_grad(): A.grad= 0.0000 A.data=-0.2728 loss= 0.8100\n",
      "            step(): A.grad=-1.2728 A.data= 0.4034\n",
      "Ep716: zero_grad(): A.grad= 0.0000 A.data= 0.4034 loss= 0.1780\n",
      "            step(): A.grad=-0.5966 A.data= 2.2131\n",
      "Ep717: zero_grad(): A.grad= 0.0000 A.data= 2.2131 loss= 0.7358\n",
      "            step(): A.grad= 1.2131 A.data= 1.7179\n",
      "Ep718: zero_grad(): A.grad= 0.0000 A.data= 1.7179 loss= 0.2577\n",
      "            step(): A.grad= 0.7179 A.data=-0.1413\n",
      "Ep719: zero_grad(): A.grad= 0.0000 A.data=-0.1413 loss= 0.6513\n",
      "            step(): A.grad=-1.1413 A.data= 0.1680\n",
      "Ep720: zero_grad(): A.grad= 0.0000 A.data= 0.1680 loss= 0.3461\n",
      "            step(): A.grad=-0.8320 A.data= 2.0581\n",
      "Ep721: zero_grad(): A.grad= 0.0000 A.data= 2.0581 loss= 0.5598\n",
      "            step(): A.grad= 1.0581 A.data= 1.9379\n",
      "Ep722: zero_grad(): A.grad= 0.0000 A.data= 1.9379 loss= 0.4398\n",
      "            step(): A.grad= 0.9379 A.data= 0.0356\n",
      "Ep723: zero_grad(): A.grad= 0.0000 A.data= 0.0356 loss= 0.4650\n",
      "            step(): A.grad=-0.9644 A.data=-0.0343\n",
      "Ep724: zero_grad(): A.grad= 0.0000 A.data=-0.0343 loss= 0.5349\n",
      "            step(): A.grad=-1.0343 A.data= 1.8609\n",
      "Ep725: zero_grad(): A.grad= 0.0000 A.data= 1.8609 loss= 0.3706\n",
      "            step(): A.grad= 0.8609 A.data= 2.1204\n",
      "Ep726: zero_grad(): A.grad= 0.0000 A.data= 2.1204 loss= 0.6276\n",
      "            step(): A.grad= 1.1204 A.data= 0.2511\n",
      "Ep727: zero_grad(): A.grad= 0.0000 A.data= 0.2511 loss= 0.2804\n",
      "            step(): A.grad=-0.7489 A.data=-0.1953\n",
      "Ep728: zero_grad(): A.grad= 0.0000 A.data=-0.1953 loss= 0.7143\n",
      "            step(): A.grad=-1.1953 A.data= 1.6294\n",
      "Ep729: zero_grad(): A.grad= 0.0000 A.data= 1.6294 loss= 0.1980\n",
      "            step(): A.grad= 0.6294 A.data= 2.2582\n",
      "Ep730: zero_grad(): A.grad= 0.0000 A.data= 2.2582 loss= 0.7915\n",
      "            step(): A.grad= 1.2582 A.data= 0.4965\n",
      "Ep731: zero_grad(): A.grad= 0.0000 A.data= 0.4965 loss= 0.1268\n",
      "            step(): A.grad=-0.5035 A.data=-0.3086\n",
      "Ep732: zero_grad(): A.grad= 0.0000 A.data=-0.3086 loss= 0.8562\n",
      "            step(): A.grad=-1.3086 A.data= 1.3727\n",
      "Ep733: zero_grad(): A.grad= 0.0000 A.data= 1.3727 loss= 0.0694\n",
      "            step(): A.grad= 0.3727 A.data= 2.3458\n",
      "Ep734: zero_grad(): A.grad= 0.0000 A.data= 2.3458 loss= 0.9056\n",
      "            step(): A.grad= 1.3458 A.data= 0.7619\n",
      "Ep735: zero_grad(): A.grad= 0.0000 A.data= 0.7619 loss= 0.0283\n",
      "            step(): A.grad=-0.2381 A.data=-0.3696\n",
      "Ep736: zero_grad(): A.grad= 0.0000 A.data=-0.3696 loss= 0.9380\n",
      "            step(): A.grad=-1.3696 A.data= 1.1011\n",
      "Ep737: zero_grad(): A.grad= 0.0000 A.data= 1.1011 loss= 0.0051\n",
      "            step(): A.grad= 0.1011 A.data= 2.3798\n",
      "Ep738: zero_grad(): A.grad= 0.0000 A.data= 2.3798 loss= 0.9519\n",
      "            step(): A.grad= 1.3798 A.data= 1.0368\n",
      "Ep739: zero_grad(): A.grad= 0.0000 A.data= 1.0368 loss= 0.0007\n",
      "            step(): A.grad= 0.0368 A.data=-0.3761\n",
      "Ep740: zero_grad(): A.grad= 0.0000 A.data=-0.3761 loss= 0.9468\n",
      "            step(): A.grad=-1.3761 A.data= 0.8256\n",
      "Ep741: zero_grad(): A.grad= 0.0000 A.data= 0.8256 loss= 0.0152\n",
      "            step(): A.grad=-0.1744 A.data= 2.3586\n",
      "Ep742: zero_grad(): A.grad= 0.0000 A.data= 2.3586 loss= 0.9229\n",
      "            step(): A.grad= 1.3586 A.data= 1.3103\n",
      "Ep743: zero_grad(): A.grad= 0.0000 A.data= 1.3103 loss= 0.0481\n",
      "            step(): A.grad= 0.3103 A.data=-0.3276\n",
      "Ep744: zero_grad(): A.grad= 0.0000 A.data=-0.3276 loss= 0.8813\n",
      "            step(): A.grad=-1.3276 A.data= 0.5569\n",
      "Ep745: zero_grad(): A.grad= 0.0000 A.data= 0.5569 loss= 0.0982\n",
      "            step(): A.grad=-0.4431 A.data= 2.2833\n",
      "Ep746: zero_grad(): A.grad= 0.0000 A.data= 2.2833 loss= 0.8234\n",
      "            step(): A.grad= 1.2833 A.data= 1.5714\n",
      "Ep747: zero_grad(): A.grad= 0.0000 A.data= 1.5714 loss= 0.1632\n",
      "            step(): A.grad= 0.5714 A.data=-0.2261\n",
      "Ep748: zero_grad(): A.grad= 0.0000 A.data=-0.2261 loss= 0.7517\n",
      "            step(): A.grad=-1.2261 A.data= 0.3060\n",
      "Ep749: zero_grad(): A.grad= 0.0000 A.data= 0.3060 loss= 0.2408\n",
      "            step(): A.grad=-0.6940 A.data= 2.1567\n",
      "Ep750: zero_grad(): A.grad= 0.0000 A.data= 2.1567 loss= 0.6690\n",
      "            step(): A.grad= 1.1567 A.data= 1.8097\n",
      "Ep751: zero_grad(): A.grad= 0.0000 A.data= 1.8097 loss= 0.3278\n",
      "            step(): A.grad= 0.8097 A.data=-0.0758\n",
      "Ep752: zero_grad(): A.grad= 0.0000 A.data=-0.0758 loss= 0.5786\n",
      "            step(): A.grad=-1.0758 A.data= 0.0827\n",
      "Ep753: zero_grad(): A.grad= 0.0000 A.data= 0.0827 loss= 0.4207\n",
      "            step(): A.grad=-0.9173 A.data= 1.9840\n",
      "Ep754: zero_grad(): A.grad= 0.0000 A.data= 1.9840 loss= 0.4842\n",
      "            step(): A.grad= 0.9840 A.data= 2.0157\n",
      "Ep755: zero_grad(): A.grad= 0.0000 A.data= 2.0157 loss= 0.5158\n",
      "            step(): A.grad= 1.0157 A.data= 0.1175\n",
      "Ep756: zero_grad(): A.grad= 0.0000 A.data= 0.1175 loss= 0.3894\n",
      "            step(): A.grad=-0.8825 A.data=-0.1039\n",
      "Ep757: zero_grad(): A.grad= 0.0000 A.data=-0.1039 loss= 0.6093\n",
      "            step(): A.grad=-1.1039 A.data= 1.7721\n",
      "Ep758: zero_grad(): A.grad= 0.0000 A.data= 1.7721 loss= 0.2981\n",
      "            step(): A.grad= 0.7721 A.data= 2.1811\n",
      "Ep759: zero_grad(): A.grad= 0.0000 A.data= 2.1811 loss= 0.6975\n",
      "            step(): A.grad= 1.1811 A.data= 0.3460\n",
      "Ep760: zero_grad(): A.grad= 0.0000 A.data= 0.3460 loss= 0.2138\n",
      "            step(): A.grad=-0.6540 A.data=-0.2465\n",
      "Ep761: zero_grad(): A.grad= 0.0000 A.data=-0.2465 loss= 0.7769\n",
      "            step(): A.grad=-1.2465 A.data= 1.5293\n",
      "Ep762: zero_grad(): A.grad= 0.0000 A.data= 1.5293 loss= 0.1401\n",
      "            step(): A.grad= 0.5293 A.data= 2.2995\n",
      "Ep763: zero_grad(): A.grad= 0.0000 A.data= 2.2995 loss= 0.8443\n",
      "            step(): A.grad= 1.2995 A.data= 0.6006\n",
      "Ep764: zero_grad(): A.grad= 0.0000 A.data= 0.6006 loss= 0.0798\n",
      "            step(): A.grad=-0.3994 A.data=-0.3394\n",
      "Ep765: zero_grad(): A.grad= 0.0000 A.data=-0.3394 loss= 0.8970\n",
      "            step(): A.grad=-1.3394 A.data= 1.2654\n",
      "Ep766: zero_grad(): A.grad= 0.0000 A.data= 1.2654 loss= 0.0352\n",
      "            step(): A.grad= 0.2654 A.data= 2.3659\n",
      "Ep767: zero_grad(): A.grad= 0.0000 A.data= 2.3659 loss= 0.9329\n",
      "            step(): A.grad= 1.3659 A.data= 0.8712\n",
      "Ep768: zero_grad(): A.grad= 0.0000 A.data= 0.8712 loss= 0.0083\n",
      "            step(): A.grad=-0.1288 A.data=-0.3788\n",
      "Ep769: zero_grad(): A.grad= 0.0000 A.data=-0.3788 loss= 0.9506\n",
      "            step(): A.grad=-1.3788 A.data= 0.9910\n",
      "Ep770: zero_grad(): A.grad= 0.0000 A.data= 0.9910 loss= 0.0000\n",
      "            step(): A.grad=-0.0090 A.data= 2.3779\n",
      "Ep771: zero_grad(): A.grad= 0.0000 A.data= 2.3779 loss= 0.9493\n",
      "            step(): A.grad= 1.3779 A.data= 1.1468\n",
      "Ep772: zero_grad(): A.grad= 0.0000 A.data= 1.1468 loss= 0.0108\n",
      "            step(): A.grad= 0.1468 A.data=-0.3632\n",
      "Ep773: zero_grad(): A.grad= 0.0000 A.data=-0.3632 loss= 0.9292\n",
      "            step(): A.grad=-1.3632 A.data= 0.7168\n",
      "Ep774: zero_grad(): A.grad= 0.0000 A.data= 0.7168 loss= 0.0401\n",
      "            step(): A.grad=-0.2832 A.data= 2.3349\n",
      "Ep775: zero_grad(): A.grad= 0.0000 A.data= 2.3349 loss= 0.8910\n",
      "            step(): A.grad= 1.3349 A.data= 1.4166\n",
      "Ep776: zero_grad(): A.grad= 0.0000 A.data= 1.4166 loss= 0.0868\n",
      "            step(): A.grad= 0.4166 A.data=-0.2933\n",
      "Ep777: zero_grad(): A.grad= 0.0000 A.data=-0.2933 loss= 0.8363\n",
      "            step(): A.grad=-1.2933 A.data= 0.4540\n",
      "Ep778: zero_grad(): A.grad= 0.0000 A.data= 0.4540 loss= 0.1490\n",
      "            step(): A.grad=-0.5460 A.data= 2.2387\n",
      "Ep779: zero_grad(): A.grad= 0.0000 A.data= 2.2387 loss= 0.7671\n",
      "            step(): A.grad= 1.2387 A.data= 1.6698\n",
      "Ep780: zero_grad(): A.grad= 0.0000 A.data= 1.6698 loss= 0.2243\n",
      "            step(): A.grad= 0.6698 A.data=-0.1717\n",
      "Ep781: zero_grad(): A.grad= 0.0000 A.data=-0.1717 loss= 0.6864\n",
      "            step(): A.grad=-1.1717 A.data= 0.2130\n",
      "Ep782: zero_grad(): A.grad= 0.0000 A.data= 0.2130 loss= 0.3097\n",
      "            step(): A.grad=-0.7870 A.data= 2.0930\n",
      "Ep783: zero_grad(): A.grad= 0.0000 A.data= 2.0930 loss= 0.5973\n",
      "            step(): A.grad= 1.0930 A.data= 1.8963\n",
      "Ep784: zero_grad(): A.grad= 0.0000 A.data= 1.8963 loss= 0.4017\n",
      "            step(): A.grad= 0.8963 A.data=-0.0033\n",
      "Ep785: zero_grad(): A.grad= 0.0000 A.data=-0.0033 loss= 0.5034\n",
      "            step(): A.grad=-1.0033 A.data= 0.0034\n",
      "Ep786: zero_grad(): A.grad= 0.0000 A.data= 0.0034 loss= 0.4966\n",
      "            step(): A.grad=-0.9966 A.data= 1.9037\n",
      "Ep787: zero_grad(): A.grad= 0.0000 A.data= 1.9037 loss= 0.4083\n",
      "            step(): A.grad= 0.9037 A.data= 2.0870\n",
      "Ep788: zero_grad(): A.grad= 0.0000 A.data= 2.0870 loss= 0.5908\n",
      "            step(): A.grad= 1.0870 A.data= 0.2050\n",
      "Ep789: zero_grad(): A.grad= 0.0000 A.data= 0.2050 loss= 0.3160\n",
      "            step(): A.grad=-0.7950 A.data=-0.1665\n",
      "Ep790: zero_grad(): A.grad= 0.0000 A.data=-0.1665 loss= 0.6804\n",
      "            step(): A.grad=-1.1665 A.data= 1.6783\n",
      "Ep791: zero_grad(): A.grad= 0.0000 A.data= 1.6783 loss= 0.2301\n",
      "            step(): A.grad= 0.6783 A.data= 2.2343\n",
      "Ep792: zero_grad(): A.grad= 0.0000 A.data= 2.2343 loss= 0.7618\n",
      "            step(): A.grad= 1.2343 A.data= 0.4451\n",
      "Ep793: zero_grad(): A.grad= 0.0000 A.data= 0.4451 loss= 0.1540\n",
      "            step(): A.grad=-0.5549 A.data=-0.2898\n",
      "Ep794: zero_grad(): A.grad= 0.0000 A.data=-0.2898 loss= 0.8318\n",
      "            step(): A.grad=-1.2898 A.data= 1.4259\n",
      "Ep795: zero_grad(): A.grad= 0.0000 A.data= 1.4259 loss= 0.0907\n",
      "            step(): A.grad= 0.4259 A.data= 2.3324\n",
      "Ep796: zero_grad(): A.grad= 0.0000 A.data= 2.3324 loss= 0.8877\n",
      "            step(): A.grad= 1.3324 A.data= 0.7073\n",
      "Ep797: zero_grad(): A.grad= 0.0000 A.data= 0.7073 loss= 0.0428\n",
      "            step(): A.grad=-0.2927 A.data=-0.3617\n",
      "Ep798: zero_grad(): A.grad= 0.0000 A.data=-0.3617 loss= 0.9271\n",
      "            step(): A.grad=-1.3617 A.data= 1.1565\n",
      "Ep799: zero_grad(): A.grad= 0.0000 A.data= 1.1565 loss= 0.0122\n",
      "            step(): A.grad= 0.1565 A.data= 2.3773\n",
      "Ep800: zero_grad(): A.grad= 0.0000 A.data= 2.3773 loss= 0.9485\n",
      "            step(): A.grad= 1.3773 A.data= 0.9812\n",
      "Ep801: zero_grad(): A.grad= 0.0000 A.data= 0.9812 loss= 0.0002\n",
      "            step(): A.grad=-0.0188 A.data=-0.3792\n",
      "Ep802: zero_grad(): A.grad= 0.0000 A.data=-0.3792 loss= 0.9511\n",
      "            step(): A.grad=-1.3792 A.data= 0.8809\n",
      "Ep803: zero_grad(): A.grad= 0.0000 A.data= 0.8809 loss= 0.0071\n",
      "            step(): A.grad=-0.1191 A.data= 2.3673\n",
      "Ep804: zero_grad(): A.grad= 0.0000 A.data= 2.3673 loss= 0.9348\n",
      "            step(): A.grad= 1.3673 A.data= 1.2559\n",
      "Ep805: zero_grad(): A.grad= 0.0000 A.data= 1.2559 loss= 0.0327\n",
      "            step(): A.grad= 0.2559 A.data=-0.3417\n",
      "Ep806: zero_grad(): A.grad= 0.0000 A.data=-0.3417 loss= 0.9001\n",
      "            step(): A.grad=-1.3417 A.data= 0.6099\n",
      "Ep807: zero_grad(): A.grad= 0.0000 A.data= 0.6099 loss= 0.0761\n",
      "            step(): A.grad=-0.3901 A.data= 2.3027\n",
      "Ep808: zero_grad(): A.grad= 0.0000 A.data= 2.3027 loss= 0.8485\n",
      "            step(): A.grad= 1.3027 A.data= 1.5203\n",
      "Ep809: zero_grad(): A.grad= 0.0000 A.data= 1.5203 loss= 0.1354\n",
      "            step(): A.grad= 0.5203 A.data=-0.2507\n",
      "Ep810: zero_grad(): A.grad= 0.0000 A.data=-0.2507 loss= 0.7821\n",
      "            step(): A.grad=-1.2507 A.data= 0.3546\n",
      "Ep811: zero_grad(): A.grad= 0.0000 A.data= 0.3546 loss= 0.2083\n",
      "            step(): A.grad=-0.6454 A.data= 2.1861\n",
      "Ep812: zero_grad(): A.grad= 0.0000 A.data= 2.1861 loss= 0.7035\n",
      "            step(): A.grad= 1.1861 A.data= 1.7640\n",
      "Ep813: zero_grad(): A.grad= 0.0000 A.data= 1.7640 loss= 0.2918\n",
      "            step(): A.grad= 0.7640 A.data=-0.1097\n",
      "Ep814: zero_grad(): A.grad= 0.0000 A.data=-0.1097 loss= 0.6158\n",
      "            step(): A.grad=-1.1097 A.data= 0.1250\n",
      "Ep815: zero_grad(): A.grad= 0.0000 A.data= 0.1250 loss= 0.3828\n",
      "            step(): A.grad=-0.8750 A.data= 2.0222\n",
      "Ep816: zero_grad(): A.grad= 0.0000 A.data= 2.0222 loss= 0.5225\n",
      "            step(): A.grad= 1.0222 A.data= 1.9772\n",
      "Ep817: zero_grad(): A.grad= 0.0000 A.data= 1.9772 loss= 0.4775\n",
      "            step(): A.grad= 0.9772 A.data= 0.0755\n",
      "Ep818: zero_grad(): A.grad= 0.0000 A.data= 0.0755 loss= 0.4274\n",
      "            step(): A.grad=-0.9245 A.data=-0.0697\n",
      "Ep819: zero_grad(): A.grad= 0.0000 A.data=-0.0697 loss= 0.5721\n",
      "            step(): A.grad=-1.0697 A.data= 1.8176\n",
      "Ep820: zero_grad(): A.grad= 0.0000 A.data= 1.8176 loss= 0.3342\n",
      "            step(): A.grad= 0.8176 A.data= 2.1514\n",
      "Ep821: zero_grad(): A.grad= 0.0000 A.data= 2.1514 loss= 0.6629\n",
      "            step(): A.grad= 1.1514 A.data= 0.2976\n",
      "Ep822: zero_grad(): A.grad= 0.0000 A.data= 0.2976 loss= 0.2467\n",
      "            step(): A.grad=-0.7024 A.data=-0.2216\n",
      "Ep823: zero_grad(): A.grad= 0.0000 A.data=-0.2216 loss= 0.7462\n",
      "            step(): A.grad=-1.2216 A.data= 1.5802\n",
      "Ep824: zero_grad(): A.grad= 0.0000 A.data= 1.5802 loss= 0.1683\n",
      "            step(): A.grad= 0.5802 A.data= 2.2797\n",
      "Ep825: zero_grad(): A.grad= 0.0000 A.data= 2.2797 loss= 0.8188\n",
      "            step(): A.grad= 1.2797 A.data= 0.5477\n",
      "Ep826: zero_grad(): A.grad= 0.0000 A.data= 0.5477 loss= 0.1023\n",
      "            step(): A.grad=-0.4523 A.data=-0.3249\n",
      "Ep827: zero_grad(): A.grad= 0.0000 A.data=-0.3249 loss= 0.8777\n",
      "            step(): A.grad=-1.3249 A.data= 1.3198\n",
      "Ep828: zero_grad(): A.grad= 0.0000 A.data= 1.3198 loss= 0.0511\n",
      "            step(): A.grad= 0.3198 A.data= 2.3569\n",
      "Ep829: zero_grad(): A.grad= 0.0000 A.data= 2.3569 loss= 0.9206\n",
      "            step(): A.grad= 1.3569 A.data= 0.8159\n",
      "Ep830: zero_grad(): A.grad= 0.0000 A.data= 0.8159 loss= 0.0169\n",
      "            step(): A.grad=-0.1841 A.data=-0.3753\n",
      "Ep831: zero_grad(): A.grad= 0.0000 A.data=-0.3753 loss= 0.9457\n",
      "            step(): A.grad=-1.3753 A.data= 1.0466\n",
      "Ep832: zero_grad(): A.grad= 0.0000 A.data= 1.0466 loss= 0.0011\n",
      "            step(): A.grad= 0.0466 A.data= 2.3799\n",
      "Ep833: zero_grad(): A.grad= 0.0000 A.data= 2.3799 loss= 0.9521\n",
      "            step(): A.grad= 1.3799 A.data= 1.0914\n",
      "Ep834: zero_grad(): A.grad= 0.0000 A.data= 1.0914 loss= 0.0042\n",
      "            step(): A.grad= 0.0914 A.data=-0.3708\n",
      "Ep835: zero_grad(): A.grad= 0.0000 A.data=-0.3708 loss= 0.9396\n",
      "            step(): A.grad=-1.3708 A.data= 0.7715\n",
      "Ep836: zero_grad(): A.grad= 0.0000 A.data= 0.7715 loss= 0.0261\n",
      "            step(): A.grad=-0.2285 A.data= 2.3480\n",
      "Ep837: zero_grad(): A.grad= 0.0000 A.data= 2.3480 loss= 0.9085\n",
      "            step(): A.grad= 1.3480 A.data= 1.3633\n",
      "Ep838: zero_grad(): A.grad= 0.0000 A.data= 1.3633 loss= 0.0660\n",
      "            step(): A.grad= 0.3633 A.data=-0.3116\n",
      "Ep839: zero_grad(): A.grad= 0.0000 A.data=-0.3116 loss= 0.8602\n",
      "            step(): A.grad=-1.3116 A.data= 0.5055\n",
      "Ep840: zero_grad(): A.grad= 0.0000 A.data= 0.5055 loss= 0.1222\n",
      "            step(): A.grad=-0.4945 A.data= 2.2622\n",
      "Ep841: zero_grad(): A.grad= 0.0000 A.data= 2.2622 loss= 0.7965\n",
      "            step(): A.grad= 1.2622 A.data= 1.6207\n",
      "Ep842: zero_grad(): A.grad= 0.0000 A.data= 1.6207 loss= 0.1926\n",
      "            step(): A.grad= 0.6207 A.data=-0.2001\n",
      "Ep843: zero_grad(): A.grad= 0.0000 A.data=-0.2001 loss= 0.7201\n",
      "            step(): A.grad=-1.2001 A.data= 0.2593\n",
      "Ep844: zero_grad(): A.grad= 0.0000 A.data= 0.2593 loss= 0.2743\n",
      "            step(): A.grad=-0.7407 A.data= 2.1260\n",
      "Ep845: zero_grad(): A.grad= 0.0000 A.data= 2.1260 loss= 0.6340\n",
      "            step(): A.grad= 1.1260 A.data= 1.8533\n",
      "Ep846: zero_grad(): A.grad= 0.0000 A.data= 1.8533 loss= 0.3641\n",
      "            step(): A.grad= 0.8533 A.data=-0.0407\n",
      "Ep847: zero_grad(): A.grad= 0.0000 A.data=-0.0407 loss= 0.5415\n",
      "            step(): A.grad=-1.0407 A.data= 0.0426\n",
      "Ep848: zero_grad(): A.grad= 0.0000 A.data= 0.0426 loss= 0.4583\n",
      "            step(): A.grad=-0.9574 A.data= 1.9450\n",
      "Ep849: zero_grad(): A.grad= 0.0000 A.data= 1.9450 loss= 0.4465\n",
      "            step(): A.grad= 0.9450 A.data= 2.0519\n",
      "Ep850: zero_grad(): A.grad= 0.0000 A.data= 2.0519 loss= 0.5532\n",
      "            step(): A.grad= 1.0519 A.data= 0.1602\n",
      "Ep851: zero_grad(): A.grad= 0.0000 A.data= 0.1602 loss= 0.3526\n",
      "            step(): A.grad=-0.8398 A.data=-0.1358\n",
      "Ep852: zero_grad(): A.grad= 0.0000 A.data=-0.1358 loss= 0.6451\n",
      "            step(): A.grad=-1.1358 A.data= 1.7262\n",
      "Ep853: zero_grad(): A.grad= 0.0000 A.data= 1.7262 loss= 0.2637\n",
      "            step(): A.grad= 0.7262 A.data= 2.2085\n",
      "Ep854: zero_grad(): A.grad= 0.0000 A.data= 2.2085 loss= 0.7302\n",
      "            step(): A.grad= 1.2085 A.data= 0.3946\n",
      "Ep855: zero_grad(): A.grad= 0.0000 A.data= 0.3946 loss= 0.1832\n",
      "            step(): A.grad=-0.6054 A.data=-0.2690\n",
      "Ep856: zero_grad(): A.grad= 0.0000 A.data=-0.2690 loss= 0.8052\n",
      "            step(): A.grad=-1.2690 A.data= 1.4785\n",
      "Ep857: zero_grad(): A.grad= 0.0000 A.data= 1.4785 loss= 0.1145\n",
      "            step(): A.grad= 0.4785 A.data= 2.3168\n",
      "Ep858: zero_grad(): A.grad= 0.0000 A.data= 2.3168 loss= 0.8670\n",
      "            step(): A.grad= 1.3168 A.data= 0.6532\n",
      "Ep859: zero_grad(): A.grad= 0.0000 A.data= 0.6532 loss= 0.0601\n",
      "            step(): A.grad=-0.3468 A.data=-0.3515\n",
      "Ep860: zero_grad(): A.grad= 0.0000 A.data=-0.3515 loss= 0.9133\n",
      "            step(): A.grad=-1.3515 A.data= 1.2116\n",
      "Ep861: zero_grad(): A.grad= 0.0000 A.data= 1.2116 loss= 0.0224\n",
      "            step(): A.grad= 0.2116 A.data= 2.3727\n",
      "Ep862: zero_grad(): A.grad= 0.0000 A.data= 2.3727 loss= 0.9421\n",
      "            step(): A.grad= 1.3727 A.data= 0.9256\n",
      "Ep863: zero_grad(): A.grad= 0.0000 A.data= 0.9256 loss= 0.0028\n",
      "            step(): A.grad=-0.0744 A.data=-0.3801\n",
      "Ep864: zero_grad(): A.grad= 0.0000 A.data=-0.3801 loss= 0.9524\n",
      "            step(): A.grad=-1.3801 A.data= 0.9363\n",
      "Ep865: zero_grad(): A.grad= 0.0000 A.data= 0.9363 loss= 0.0020\n",
      "            step(): A.grad=-0.0637 A.data= 2.3738\n",
      "Ep866: zero_grad(): A.grad= 0.0000 A.data= 2.3738 loss= 0.9436\n",
      "            step(): A.grad= 1.3738 A.data= 1.2010\n",
      "Ep867: zero_grad(): A.grad= 0.0000 A.data= 1.2010 loss= 0.0202\n",
      "            step(): A.grad= 0.2010 A.data=-0.3536\n",
      "Ep868: zero_grad(): A.grad= 0.0000 A.data=-0.3536 loss= 0.9162\n",
      "            step(): A.grad=-1.3536 A.data= 0.6636\n",
      "Ep869: zero_grad(): A.grad= 0.0000 A.data= 0.6636 loss= 0.0566\n",
      "            step(): A.grad=-0.3364 A.data= 2.3200\n",
      "Ep870: zero_grad(): A.grad= 0.0000 A.data= 2.3200 loss= 0.8712\n",
      "            step(): A.grad= 1.3200 A.data= 1.4684\n",
      "Ep871: zero_grad(): A.grad= 0.0000 A.data= 1.4684 loss= 0.1097\n",
      "            step(): A.grad= 0.4684 A.data=-0.2732\n",
      "Ep872: zero_grad(): A.grad= 0.0000 A.data=-0.2732 loss= 0.8105\n",
      "            step(): A.grad=-1.2732 A.data= 0.4043\n",
      "Ep873: zero_grad(): A.grad= 0.0000 A.data= 0.4043 loss= 0.1774\n",
      "            step(): A.grad=-0.5957 A.data= 2.2136\n",
      "Ep874: zero_grad(): A.grad= 0.0000 A.data= 2.2136 loss= 0.7364\n",
      "            step(): A.grad= 1.2136 A.data= 1.7171\n",
      "Ep875: zero_grad(): A.grad= 0.0000 A.data= 1.7171 loss= 0.2571\n",
      "            step(): A.grad= 0.7171 A.data=-0.1419\n",
      "Ep876: zero_grad(): A.grad= 0.0000 A.data=-0.1419 loss= 0.6520\n",
      "            step(): A.grad=-1.1419 A.data= 0.1687\n",
      "Ep877: zero_grad(): A.grad= 0.0000 A.data= 0.1687 loss= 0.3455\n",
      "            step(): A.grad=-0.8313 A.data= 2.0588\n",
      "Ep878: zero_grad(): A.grad= 0.0000 A.data= 2.0588 loss= 0.5605\n",
      "            step(): A.grad= 1.0588 A.data= 1.9371\n",
      "Ep879: zero_grad(): A.grad= 0.0000 A.data= 1.9371 loss= 0.4391\n",
      "            step(): A.grad= 0.9371 A.data= 0.0350\n",
      "Ep880: zero_grad(): A.grad= 0.0000 A.data= 0.0350 loss= 0.4657\n",
      "            step(): A.grad=-0.9650 A.data=-0.0336\n",
      "Ep881: zero_grad(): A.grad= 0.0000 A.data=-0.0336 loss= 0.5342\n",
      "            step(): A.grad=-1.0336 A.data= 1.8617\n",
      "Ep882: zero_grad(): A.grad= 0.0000 A.data= 1.8617 loss= 0.3712\n",
      "            step(): A.grad= 0.8617 A.data= 2.1198\n",
      "Ep883: zero_grad(): A.grad= 0.0000 A.data= 2.1198 loss= 0.6270\n",
      "            step(): A.grad= 1.1198 A.data= 0.2503\n",
      "Ep884: zero_grad(): A.grad= 0.0000 A.data= 0.2503 loss= 0.2810\n",
      "            step(): A.grad=-0.7497 A.data=-0.1948\n",
      "Ep885: zero_grad(): A.grad= 0.0000 A.data=-0.1948 loss= 0.7138\n",
      "            step(): A.grad=-1.1948 A.data= 1.6302\n",
      "Ep886: zero_grad(): A.grad= 0.0000 A.data= 1.6302 loss= 0.1986\n",
      "            step(): A.grad= 0.6302 A.data= 2.2578\n",
      "Ep887: zero_grad(): A.grad= 0.0000 A.data= 2.2578 loss= 0.7910\n",
      "            step(): A.grad= 1.2578 A.data= 0.4956\n",
      "Ep888: zero_grad(): A.grad= 0.0000 A.data= 0.4956 loss= 0.1272\n",
      "            step(): A.grad=-0.5044 A.data=-0.3083\n",
      "Ep889: zero_grad(): A.grad= 0.0000 A.data=-0.3083 loss= 0.8558\n",
      "            step(): A.grad=-1.3083 A.data= 1.3736\n",
      "Ep890: zero_grad(): A.grad= 0.0000 A.data= 1.3736 loss= 0.0698\n",
      "            step(): A.grad= 0.3736 A.data= 2.3456\n",
      "Ep891: zero_grad(): A.grad= 0.0000 A.data= 2.3456 loss= 0.9053\n",
      "            step(): A.grad= 1.3456 A.data= 0.7609\n",
      "Ep892: zero_grad(): A.grad= 0.0000 A.data= 0.7609 loss= 0.0286\n",
      "            step(): A.grad=-0.2391 A.data=-0.3695\n",
      "Ep893: zero_grad(): A.grad= 0.0000 A.data=-0.3695 loss= 0.9378\n",
      "            step(): A.grad=-1.3695 A.data= 1.1021\n",
      "Ep894: zero_grad(): A.grad= 0.0000 A.data= 1.1021 loss= 0.0052\n",
      "            step(): A.grad= 0.1021 A.data= 2.3797\n",
      "Ep895: zero_grad(): A.grad= 0.0000 A.data= 2.3797 loss= 0.9518\n",
      "            step(): A.grad= 1.3797 A.data= 1.0359\n",
      "Ep896: zero_grad(): A.grad= 0.0000 A.data= 1.0359 loss= 0.0006\n",
      "            step(): A.grad= 0.0359 A.data=-0.3761\n",
      "Ep897: zero_grad(): A.grad= 0.0000 A.data=-0.3761 loss= 0.9469\n",
      "            step(): A.grad=-1.3761 A.data= 0.8265\n",
      "Ep898: zero_grad(): A.grad= 0.0000 A.data= 0.8265 loss= 0.0150\n",
      "            step(): A.grad=-0.1735 A.data= 2.3588\n",
      "Ep899: zero_grad(): A.grad= 0.0000 A.data= 2.3588 loss= 0.9232\n",
      "            step(): A.grad= 1.3588 A.data= 1.3094\n",
      "Ep900: zero_grad(): A.grad= 0.0000 A.data= 1.3094 loss= 0.0479\n",
      "            step(): A.grad= 0.3094 A.data=-0.3279\n",
      "Ep901: zero_grad(): A.grad= 0.0000 A.data=-0.3279 loss= 0.8816\n",
      "            step(): A.grad=-1.3279 A.data= 0.5579\n",
      "Ep902: zero_grad(): A.grad= 0.0000 A.data= 0.5579 loss= 0.0977\n",
      "            step(): A.grad=-0.4421 A.data= 2.2836\n",
      "Ep903: zero_grad(): A.grad= 0.0000 A.data= 2.2836 loss= 0.8239\n",
      "            step(): A.grad= 1.2836 A.data= 1.5705\n",
      "Ep904: zero_grad(): A.grad= 0.0000 A.data= 1.5705 loss= 0.1627\n",
      "            step(): A.grad= 0.5705 A.data=-0.2266\n",
      "Ep905: zero_grad(): A.grad= 0.0000 A.data=-0.2266 loss= 0.7523\n",
      "            step(): A.grad=-1.2266 A.data= 0.3068\n",
      "Ep906: zero_grad(): A.grad= 0.0000 A.data= 0.3068 loss= 0.2402\n",
      "            step(): A.grad=-0.6932 A.data= 2.1573\n",
      "Ep907: zero_grad(): A.grad= 0.0000 A.data= 2.1573 loss= 0.6696\n",
      "            step(): A.grad= 1.1573 A.data= 1.8089\n",
      "Ep908: zero_grad(): A.grad= 0.0000 A.data= 1.8089 loss= 0.3272\n",
      "            step(): A.grad= 0.8089 A.data=-0.0764\n",
      "Ep909: zero_grad(): A.grad= 0.0000 A.data=-0.0764 loss= 0.5793\n",
      "            step(): A.grad=-1.0764 A.data= 0.0835\n",
      "Ep910: zero_grad(): A.grad= 0.0000 A.data= 0.0835 loss= 0.4200\n",
      "            step(): A.grad=-0.9165 A.data= 1.9847\n",
      "Ep911: zero_grad(): A.grad= 0.0000 A.data= 1.9847 loss= 0.4848\n",
      "            step(): A.grad= 0.9847 A.data= 2.0150\n",
      "Ep912: zero_grad(): A.grad= 0.0000 A.data= 2.0150 loss= 0.5151\n",
      "            step(): A.grad= 1.0150 A.data= 0.1168\n",
      "Ep913: zero_grad(): A.grad= 0.0000 A.data= 0.1168 loss= 0.3900\n",
      "            step(): A.grad=-0.8832 A.data=-0.1033\n",
      "Ep914: zero_grad(): A.grad= 0.0000 A.data=-0.1033 loss= 0.6087\n",
      "            step(): A.grad=-1.1033 A.data= 1.7729\n",
      "Ep915: zero_grad(): A.grad= 0.0000 A.data= 1.7729 loss= 0.2987\n",
      "            step(): A.grad= 0.7729 A.data= 2.1806\n",
      "Ep916: zero_grad(): A.grad= 0.0000 A.data= 2.1806 loss= 0.6969\n",
      "            step(): A.grad= 1.1806 A.data= 0.3452\n",
      "Ep917: zero_grad(): A.grad= 0.0000 A.data= 0.3452 loss= 0.2144\n",
      "            step(): A.grad=-0.6548 A.data=-0.2461\n",
      "Ep918: zero_grad(): A.grad= 0.0000 A.data=-0.2461 loss= 0.7764\n",
      "            step(): A.grad=-1.2461 A.data= 1.5302\n",
      "Ep919: zero_grad(): A.grad= 0.0000 A.data= 1.5302 loss= 0.1406\n",
      "            step(): A.grad= 0.5302 A.data= 2.2991\n",
      "Ep920: zero_grad(): A.grad= 0.0000 A.data= 2.2991 loss= 0.8439\n",
      "            step(): A.grad= 1.2991 A.data= 0.5997\n",
      "Ep921: zero_grad(): A.grad= 0.0000 A.data= 0.5997 loss= 0.0801\n",
      "            step(): A.grad=-0.4003 A.data=-0.3392\n",
      "Ep922: zero_grad(): A.grad= 0.0000 A.data=-0.3392 loss= 0.8967\n",
      "            step(): A.grad=-1.3392 A.data= 1.2664\n",
      "Ep923: zero_grad(): A.grad= 0.0000 A.data= 1.2664 loss= 0.0355\n",
      "            step(): A.grad= 0.2664 A.data= 2.3658\n",
      "Ep924: zero_grad(): A.grad= 0.0000 A.data= 2.3658 loss= 0.9327\n",
      "            step(): A.grad= 1.3658 A.data= 0.8702\n",
      "Ep925: zero_grad(): A.grad= 0.0000 A.data= 0.8702 loss= 0.0084\n",
      "            step(): A.grad=-0.1298 A.data=-0.3788\n",
      "Ep926: zero_grad(): A.grad= 0.0000 A.data=-0.3788 loss= 0.9505\n",
      "            step(): A.grad=-1.3788 A.data= 0.9919\n",
      "Ep927: zero_grad(): A.grad= 0.0000 A.data= 0.9919 loss= 0.0000\n",
      "            step(): A.grad=-0.0081 A.data= 2.3780\n",
      "Ep928: zero_grad(): A.grad= 0.0000 A.data= 2.3780 loss= 0.9494\n",
      "            step(): A.grad= 1.3780 A.data= 1.1459\n",
      "Ep929: zero_grad(): A.grad= 0.0000 A.data= 1.1459 loss= 0.0106\n",
      "            step(): A.grad= 0.1459 A.data=-0.3634\n",
      "Ep930: zero_grad(): A.grad= 0.0000 A.data=-0.3634 loss= 0.9294\n",
      "            step(): A.grad=-1.3634 A.data= 0.7178\n",
      "Ep931: zero_grad(): A.grad= 0.0000 A.data= 0.7178 loss= 0.0398\n",
      "            step(): A.grad=-0.2822 A.data= 2.3352\n",
      "Ep932: zero_grad(): A.grad= 0.0000 A.data= 2.3352 loss= 0.8913\n",
      "            step(): A.grad= 1.3352 A.data= 1.4157\n",
      "Ep933: zero_grad(): A.grad= 0.0000 A.data= 1.4157 loss= 0.0864\n",
      "            step(): A.grad= 0.4157 A.data=-0.2936\n",
      "Ep934: zero_grad(): A.grad= 0.0000 A.data=-0.2936 loss= 0.8367\n",
      "            step(): A.grad=-1.2936 A.data= 0.4549\n",
      "Ep935: zero_grad(): A.grad= 0.0000 A.data= 0.4549 loss= 0.1486\n",
      "            step(): A.grad=-0.5451 A.data= 2.2391\n",
      "Ep936: zero_grad(): A.grad= 0.0000 A.data= 2.2391 loss= 0.7677\n",
      "            step(): A.grad= 1.2391 A.data= 1.6690\n",
      "Ep937: zero_grad(): A.grad= 0.0000 A.data= 1.6690 loss= 0.2238\n",
      "            step(): A.grad= 0.6690 A.data=-0.1722\n",
      "Ep938: zero_grad(): A.grad= 0.0000 A.data=-0.1722 loss= 0.6870\n",
      "            step(): A.grad=-1.1722 A.data= 0.2138\n",
      "Ep939: zero_grad(): A.grad= 0.0000 A.data= 0.2138 loss= 0.3091\n",
      "            step(): A.grad=-0.7862 A.data= 2.0936\n",
      "Ep940: zero_grad(): A.grad= 0.0000 A.data= 2.0936 loss= 0.5979\n",
      "            step(): A.grad= 1.0936 A.data= 1.8956\n",
      "Ep941: zero_grad(): A.grad= 0.0000 A.data= 1.8956 loss= 0.4010\n",
      "            step(): A.grad= 0.8956 A.data=-0.0040\n",
      "Ep942: zero_grad(): A.grad= 0.0000 A.data=-0.0040 loss= 0.5040\n",
      "            step(): A.grad=-1.0040 A.data= 0.0040\n",
      "Ep943: zero_grad(): A.grad= 0.0000 A.data= 0.0040 loss= 0.4960\n",
      "            step(): A.grad=-0.9960 A.data= 1.9044\n",
      "Ep944: zero_grad(): A.grad= 0.0000 A.data= 1.9044 loss= 0.4090\n",
      "            step(): A.grad= 0.9044 A.data= 2.0864\n",
      "Ep945: zero_grad(): A.grad= 0.0000 A.data= 2.0864 loss= 0.5901\n",
      "            step(): A.grad= 1.0864 A.data= 0.2042\n",
      "Ep946: zero_grad(): A.grad= 0.0000 A.data= 0.2042 loss= 0.3166\n",
      "            step(): A.grad=-0.7958 A.data=-0.1660\n",
      "Ep947: zero_grad(): A.grad= 0.0000 A.data=-0.1660 loss= 0.6798\n",
      "            step(): A.grad=-1.1660 A.data= 1.6792\n",
      "Ep948: zero_grad(): A.grad= 0.0000 A.data= 1.6792 loss= 0.2306\n",
      "            step(): A.grad= 0.6792 A.data= 2.2339\n",
      "Ep949: zero_grad(): A.grad= 0.0000 A.data= 2.2339 loss= 0.7613\n",
      "            step(): A.grad= 1.2339 A.data= 0.4442\n",
      "Ep950: zero_grad(): A.grad= 0.0000 A.data= 0.4442 loss= 0.1545\n",
      "            step(): A.grad=-0.5558 A.data=-0.2895\n",
      "Ep951: zero_grad(): A.grad= 0.0000 A.data=-0.2895 loss= 0.8314\n",
      "            step(): A.grad=-1.2895 A.data= 1.4268\n",
      "Ep952: zero_grad(): A.grad= 0.0000 A.data= 1.4268 loss= 0.0911\n",
      "            step(): A.grad= 0.4268 A.data= 2.3322\n",
      "Ep953: zero_grad(): A.grad= 0.0000 A.data= 2.3322 loss= 0.8873\n",
      "            step(): A.grad= 1.3322 A.data= 0.7064\n",
      "Ep954: zero_grad(): A.grad= 0.0000 A.data= 0.7064 loss= 0.0431\n",
      "            step(): A.grad=-0.2936 A.data=-0.3615\n",
      "Ep955: zero_grad(): A.grad= 0.0000 A.data=-0.3615 loss= 0.9269\n",
      "            step(): A.grad=-1.3615 A.data= 1.1575\n",
      "Ep956: zero_grad(): A.grad= 0.0000 A.data= 1.1575 loss= 0.0124\n",
      "            step(): A.grad= 0.1575 A.data= 2.3773\n",
      "Ep957: zero_grad(): A.grad= 0.0000 A.data= 2.3773 loss= 0.9484\n",
      "            step(): A.grad= 1.3773 A.data= 0.9803\n",
      "Ep958: zero_grad(): A.grad= 0.0000 A.data= 0.9803 loss= 0.0002\n",
      "            step(): A.grad=-0.0197 A.data=-0.3792\n",
      "Ep959: zero_grad(): A.grad= 0.0000 A.data=-0.3792 loss= 0.9512\n",
      "            step(): A.grad=-1.3792 A.data= 0.8818\n",
      "Ep960: zero_grad(): A.grad= 0.0000 A.data= 0.8818 loss= 0.0070\n",
      "            step(): A.grad=-0.1182 A.data= 2.3674\n",
      "Ep961: zero_grad(): A.grad= 0.0000 A.data= 2.3674 loss= 0.9349\n",
      "            step(): A.grad= 1.3674 A.data= 1.2549\n",
      "Ep962: zero_grad(): A.grad= 0.0000 A.data= 1.2549 loss= 0.0325\n",
      "            step(): A.grad= 0.2549 A.data=-0.3419\n",
      "Ep963: zero_grad(): A.grad= 0.0000 A.data=-0.3419 loss= 0.9004\n",
      "            step(): A.grad=-1.3419 A.data= 0.6109\n",
      "Ep964: zero_grad(): A.grad= 0.0000 A.data= 0.6109 loss= 0.0757\n",
      "            step(): A.grad=-0.3891 A.data= 2.3030\n",
      "Ep965: zero_grad(): A.grad= 0.0000 A.data= 2.3030 loss= 0.8489\n",
      "            step(): A.grad= 1.3030 A.data= 1.5194\n",
      "Ep966: zero_grad(): A.grad= 0.0000 A.data= 1.5194 loss= 0.1349\n",
      "            step(): A.grad= 0.5194 A.data=-0.2511\n",
      "Ep967: zero_grad(): A.grad= 0.0000 A.data=-0.2511 loss= 0.7826\n",
      "            step(): A.grad=-1.2511 A.data= 0.3555\n",
      "Ep968: zero_grad(): A.grad= 0.0000 A.data= 0.3555 loss= 0.2077\n",
      "            step(): A.grad=-0.6445 A.data= 2.1866\n",
      "Ep969: zero_grad(): A.grad= 0.0000 A.data= 2.1866 loss= 0.7040\n",
      "            step(): A.grad= 1.1866 A.data= 1.7632\n",
      "Ep970: zero_grad(): A.grad= 0.0000 A.data= 1.7632 loss= 0.2912\n",
      "            step(): A.grad= 0.7632 A.data=-0.1103\n",
      "Ep971: zero_grad(): A.grad= 0.0000 A.data=-0.1103 loss= 0.6164\n",
      "            step(): A.grad=-1.1103 A.data= 0.1258\n",
      "Ep972: zero_grad(): A.grad= 0.0000 A.data= 0.1258 loss= 0.3821\n",
      "            step(): A.grad=-0.8742 A.data= 2.0229\n",
      "Ep973: zero_grad(): A.grad= 0.0000 A.data= 2.0229 loss= 0.5232\n",
      "            step(): A.grad= 1.0229 A.data= 1.9765\n",
      "Ep974: zero_grad(): A.grad= 0.0000 A.data= 1.9765 loss= 0.4768\n",
      "            step(): A.grad= 0.9765 A.data= 0.0748\n",
      "Ep975: zero_grad(): A.grad= 0.0000 A.data= 0.0748 loss= 0.4280\n",
      "            step(): A.grad=-0.9252 A.data=-0.0690\n",
      "Ep976: zero_grad(): A.grad= 0.0000 A.data=-0.0690 loss= 0.5714\n",
      "            step(): A.grad=-1.0690 A.data= 1.8183\n",
      "Ep977: zero_grad(): A.grad= 0.0000 A.data= 1.8183 loss= 0.3348\n",
      "            step(): A.grad= 0.8183 A.data= 2.1509\n",
      "Ep978: zero_grad(): A.grad= 0.0000 A.data= 2.1509 loss= 0.6623\n",
      "            step(): A.grad= 1.1509 A.data= 0.2968\n",
      "Ep979: zero_grad(): A.grad= 0.0000 A.data= 0.2968 loss= 0.2473\n",
      "            step(): A.grad=-0.7032 A.data=-0.2212\n",
      "Ep980: zero_grad(): A.grad= 0.0000 A.data=-0.2212 loss= 0.7457\n",
      "            step(): A.grad=-1.2212 A.data= 1.5811\n",
      "Ep981: zero_grad(): A.grad= 0.0000 A.data= 1.5811 loss= 0.1689\n",
      "            step(): A.grad= 0.5811 A.data= 2.2793\n",
      "Ep982: zero_grad(): A.grad= 0.0000 A.data= 2.2793 loss= 0.8183\n",
      "            step(): A.grad= 1.2793 A.data= 0.5468\n",
      "Ep983: zero_grad(): A.grad= 0.0000 A.data= 0.5468 loss= 0.1027\n",
      "            step(): A.grad=-0.4532 A.data=-0.3246\n",
      "Ep984: zero_grad(): A.grad= 0.0000 A.data=-0.3246 loss= 0.8773\n",
      "            step(): A.grad=-1.3246 A.data= 1.3207\n",
      "Ep985: zero_grad(): A.grad= 0.0000 A.data= 1.3207 loss= 0.0514\n",
      "            step(): A.grad= 0.3207 A.data= 2.3567\n",
      "Ep986: zero_grad(): A.grad= 0.0000 A.data= 2.3567 loss= 0.9203\n",
      "            step(): A.grad= 1.3567 A.data= 0.8149\n",
      "Ep987: zero_grad(): A.grad= 0.0000 A.data= 0.8149 loss= 0.0171\n",
      "            step(): A.grad=-0.1851 A.data=-0.3752\n",
      "Ep988: zero_grad(): A.grad= 0.0000 A.data=-0.3752 loss= 0.9456\n",
      "            step(): A.grad=-1.3752 A.data= 1.0475\n",
      "Ep989: zero_grad(): A.grad= 0.0000 A.data= 1.0475 loss= 0.0011\n",
      "            step(): A.grad= 0.0475 A.data= 2.3800\n",
      "Ep990: zero_grad(): A.grad= 0.0000 A.data= 2.3800 loss= 0.9521\n",
      "            step(): A.grad= 1.3800 A.data= 1.0905\n",
      "Ep991: zero_grad(): A.grad= 0.0000 A.data= 1.0905 loss= 0.0041\n",
      "            step(): A.grad= 0.0905 A.data=-0.3709\n",
      "Ep992: zero_grad(): A.grad= 0.0000 A.data=-0.3709 loss= 0.9397\n",
      "            step(): A.grad=-1.3709 A.data= 0.7725\n",
      "Ep993: zero_grad(): A.grad= 0.0000 A.data= 0.7725 loss= 0.0259\n",
      "            step(): A.grad=-0.2275 A.data= 2.3482\n",
      "Ep994: zero_grad(): A.grad= 0.0000 A.data= 2.3482 loss= 0.9088\n",
      "            step(): A.grad= 1.3482 A.data= 1.3624\n",
      "Ep995: zero_grad(): A.grad= 0.0000 A.data= 1.3624 loss= 0.0657\n",
      "            step(): A.grad= 0.3624 A.data=-0.3119\n",
      "Ep996: zero_grad(): A.grad= 0.0000 A.data=-0.3119 loss= 0.8606\n",
      "            step(): A.grad=-1.3119 A.data= 0.5064\n",
      "Ep997: zero_grad(): A.grad= 0.0000 A.data= 0.5064 loss= 0.1218\n",
      "            step(): A.grad=-0.4936 A.data= 2.2626\n",
      "Ep998: zero_grad(): A.grad= 0.0000 A.data= 2.2626 loss= 0.7970\n",
      "            step(): A.grad= 1.2626 A.data= 1.6198\n",
      "Ep999: zero_grad(): A.grad= 0.0000 A.data= 1.6198 loss= 0.1921\n",
      "            step(): A.grad= 0.6198 A.data=-0.2006\n",
      "Ep1000: zero_grad(): A.grad= 0.0000 A.data=-0.2006 loss= 0.7207\n",
      "            step(): A.grad=-1.2006 A.data= 0.2601\n"
     ]
    }
   ],
   "source": [
    "m = MyModel().to('cpu')\n",
    "optimizer = torch.optim.SGD(m.parameters(), lr=lr, momentum=1.0)\n",
    "epoch = train(m, optimizer, epochs, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep  1: zero_grad(): A.grad=  None  A.data= 0.0000 loss= 0.5000\n",
      "            step(): A.grad=-1.0000 A.data= 1.9000\n",
      "Ep  2: zero_grad(): A.grad= 0.0000 A.data= 1.9000 loss= 0.4050\n",
      "            step(): A.grad= 0.9000 A.data= 2.2800\n",
      "Ep  3: zero_grad(): A.grad= 0.0000 A.data= 2.2800 loss= 0.8192\n",
      "            step(): A.grad= 1.2800 A.data= 0.2660\n",
      "Ep  4: zero_grad(): A.grad= 0.0000 A.data= 0.2660 loss= 0.2694\n",
      "            step(): A.grad=-0.7340 A.data=-0.5548\n",
      "Ep  5: zero_grad(): A.grad= 0.0000 A.data=-0.5548 loss= 1.2087\n",
      "            step(): A.grad=-1.5548 A.data= 1.4964\n",
      "Ep  6: zero_grad(): A.grad= 0.0000 A.data= 1.4964 loss= 0.1232\n",
      "            step(): A.grad= 0.4964 A.data= 2.8096\n",
      "Ep  7: zero_grad(): A.grad= 0.0000 A.data= 2.8096 loss= 1.6373\n",
      "            step(): A.grad= 1.8096 A.data= 0.8158\n",
      "Ep  8: zero_grad(): A.grad= 0.0000 A.data= 0.8158 loss= 0.0170\n",
      "            step(): A.grad=-0.1842 A.data=-1.0274\n",
      "Ep  9: zero_grad(): A.grad= 0.0000 A.data=-1.0274 loss= 2.0551\n",
      "            step(): A.grad=-2.0274 A.data= 0.7971\n",
      "Ep 10: zero_grad(): A.grad= 0.0000 A.data= 0.7971 loss= 0.0206\n",
      "            step(): A.grad=-0.2029 A.data= 3.1895\n",
      "Ep 11: zero_grad(): A.grad= 0.0000 A.data= 3.1895 loss= 2.3970\n",
      "            step(): A.grad= 2.1895 A.data= 1.6611\n",
      "Ep 12: zero_grad(): A.grad= 0.0000 A.data= 1.6611 loss= 0.2185\n",
      "            step(): A.grad= 0.6611 A.data=-1.2763\n",
      "Ep 13: zero_grad(): A.grad= 0.0000 A.data=-1.2763 loss= 2.5907\n",
      "            step(): A.grad=-2.2763 A.data=-0.1824\n",
      "Ep 14: zero_grad(): A.grad= 0.0000 A.data=-0.1824 loss= 0.6991\n",
      "            step(): A.grad=-1.1824 A.data= 3.2674\n",
      "Ep 15: zero_grad(): A.grad= 0.0000 A.data= 3.2674 loss= 2.5705\n",
      "            step(): A.grad= 2.2674 A.data= 2.7542\n",
      "Ep 16: zero_grad(): A.grad= 0.0000 A.data= 2.7542 loss= 1.5385\n",
      "            step(): A.grad= 1.7542 A.data=-1.1433\n",
      "Ep 17: zero_grad(): A.grad= 0.0000 A.data=-1.1433 loss= 2.2969\n",
      "            step(): A.grad=-2.1433 A.data=-1.3582\n",
      "Ep 18: zero_grad(): A.grad= 0.0000 A.data=-1.3582 loss= 2.7806\n",
      "            step(): A.grad=-2.3582 A.data= 2.8860\n",
      "Ep 19: zero_grad(): A.grad= 0.0000 A.data= 2.8860 loss= 1.7785\n",
      "            step(): A.grad= 1.8860 A.data= 3.9713\n",
      "Ep 20: zero_grad(): A.grad= 0.0000 A.data= 3.9713 loss= 4.4142\n",
      "            step(): A.grad= 2.9713 A.data=-0.4803\n",
      "Ep 21: zero_grad(): A.grad= 0.0000 A.data=-0.4803 loss= 1.0957\n",
      "            step(): A.grad=-1.4803 A.data=-2.5644\n",
      "Ep 22: zero_grad(): A.grad= 0.0000 A.data=-2.5644 loss= 6.3526\n",
      "            step(): A.grad=-3.5644 A.data= 1.9155\n",
      "Ep 23: zero_grad(): A.grad= 0.0000 A.data= 1.9155 loss= 0.4190\n",
      "            step(): A.grad= 0.9155 A.data= 5.1040\n",
      "Ep 24: zero_grad(): A.grad= 0.0000 A.data= 5.1040 loss= 8.4213\n",
      "            step(): A.grad= 4.1040 A.data= 0.8138\n",
      "Ep 25: zero_grad(): A.grad= 0.0000 A.data= 0.8138 loss= 0.0173\n",
      "            step(): A.grad=-0.1862 A.data=-3.5516\n",
      "Ep 26: zero_grad(): A.grad= 0.0000 A.data=-3.5516 loss=10.3587\n",
      "            step(): A.grad=-4.5516 A.data= 0.2945\n",
      "Ep 27: zero_grad(): A.grad= 0.0000 A.data= 0.2945 loss= 0.2488\n",
      "            step(): A.grad=-0.7055 A.data= 5.8657\n",
      "Ep 28: zero_grad(): A.grad= 0.0000 A.data= 5.8657 loss=11.8375\n",
      "            step(): A.grad= 4.8657 A.data= 2.7492\n",
      "Ep 29: zero_grad(): A.grad= 0.0000 A.data= 2.7492 loss= 1.5298\n",
      "            step(): A.grad= 1.7492 A.data=-4.0024\n",
      "Ep 30: zero_grad(): A.grad= 0.0000 A.data=-4.0024 loss=12.5122\n",
      "            step(): A.grad=-5.0024 A.data=-1.9246\n",
      "Ep 31: zero_grad(): A.grad= 0.0000 A.data=-1.9246 loss= 4.2765\n",
      "            step(): A.grad=-2.9246 A.data= 5.9178\n",
      "Ep 32: zero_grad(): A.grad= 0.0000 A.data= 5.9178 loss=12.0922\n",
      "            step(): A.grad= 4.9178 A.data= 5.2006\n",
      "Ep 33: zero_grad(): A.grad= 0.0000 A.data= 5.2006 loss= 8.8224\n",
      "            step(): A.grad= 4.2006 A.data=-3.5694\n",
      "Ep 34: zero_grad(): A.grad= 0.0000 A.data=-3.5694 loss=10.4398\n",
      "            step(): A.grad=-4.5694 A.data=-4.5345\n",
      "Ep 35: zero_grad(): A.grad= 0.0000 A.data=-4.5345 loss=15.3155\n",
      "            step(): A.grad=-5.5345 A.data= 4.9195\n",
      "Ep 36: zero_grad(): A.grad= 0.0000 A.data= 4.9195 loss= 7.6811\n",
      "            step(): A.grad= 3.9195 A.data= 7.8719\n",
      "Ep 37: zero_grad(): A.grad= 0.0000 A.data= 7.8719 loss=23.6113\n",
      "            step(): A.grad= 6.8719 A.data=-1.9370\n",
      "Ep 38: zero_grad(): A.grad= 0.0000 A.data=-1.9370 loss= 4.3131\n",
      "            step(): A.grad=-2.9370 A.data=-7.1465\n",
      "Ep 39: zero_grad(): A.grad= 0.0000 A.data=-7.1465 loss=33.1824\n",
      "            step(): A.grad=-8.1465 A.data= 2.6014\n",
      "Ep 40: zero_grad(): A.grad= 0.0000 A.data= 2.6014 loss= 1.2823\n",
      "            step(): A.grad= 1.6014 A.data=10.2814\n",
      "Ep 41: zero_grad(): A.grad= 0.0000 A.data=10.2814 loss=43.0721\n",
      "            step(): A.grad= 9.2814 A.data= 1.0947\n",
      "Ep 42: zero_grad(): A.grad= 0.0000 A.data= 1.0947 loss= 0.0045\n",
      "            step(): A.grad= 0.0947 A.data=-9.1906\n",
      "Ep 43: zero_grad(): A.grad= 0.0000 A.data=-9.1906 loss=51.9241\n",
      "            step(): A.grad=-10.1906 A.data=-1.1423\n",
      "Ep 44: zero_grad(): A.grad= 0.0000 A.data=-1.1423 loss= 2.2947\n",
      "            step(): A.grad=-2.1423 A.data=11.7812\n",
      "Ep 45: zero_grad(): A.grad= 0.0000 A.data=11.7812 loss=58.1171\n",
      "            step(): A.grad=10.7812 A.data= 5.5127\n",
      "Ep 46: zero_grad(): A.grad= 0.0000 A.data= 5.5127 loss=10.1824\n",
      "            step(): A.grad= 4.5127 A.data=-9.9568\n",
      "Ep 47: zero_grad(): A.grad= 0.0000 A.data=-9.9568 loss=60.0254\n",
      "            step(): A.grad=-10.9568 A.data=-6.1554\n",
      "Ep 48: zero_grad(): A.grad= 0.0000 A.data=-6.1554 loss=25.5997\n",
      "            step(): A.grad=-7.1554 A.data=11.6214\n",
      "Ep 49: zero_grad(): A.grad= 0.0000 A.data=11.6214 loss=56.4068\n",
      "            step(): A.grad=10.6214 A.data=10.9952\n",
      "Ep 50: zero_grad(): A.grad= 0.0000 A.data=10.9952 loss=49.9519\n",
      "            step(): A.grad= 9.9952 A.data=-8.6845\n",
      "Ep 51: zero_grad(): A.grad= 0.0000 A.data=-8.6845 loss=46.8945\n",
      "            step(): A.grad=-9.6845 A.data=-11.9316\n",
      "Ep 52: zero_grad(): A.grad= 0.0000 A.data=-11.9316 loss=83.6131\n",
      "            step(): A.grad=-12.9316 A.data= 9.0666\n",
      "Ep 53: zero_grad(): A.grad= 0.0000 A.data= 9.0666 loss=32.5350\n",
      "            step(): A.grad= 8.0666 A.data=16.8381\n",
      "Ep 54: zero_grad(): A.grad= 0.0000 A.data=16.8381 loss=125.4224\n",
      "            step(): A.grad=15.8381 A.data=-4.7056\n",
      "Ep 55: zero_grad(): A.grad= 0.0000 A.data=-4.7056 loss=16.2771\n",
      "            step(): A.grad=-5.7056 A.data=-17.5630\n",
      "Ep 56: zero_grad(): A.grad= 0.0000 A.data=-17.5630 loss=172.2928\n",
      "            step(): A.grad=-18.5630 A.data= 3.5636\n",
      "Ep 57: zero_grad(): A.grad= 0.0000 A.data= 3.5636 loss= 3.2860\n",
      "            step(): A.grad= 2.5636 A.data=21.9320\n",
      "Ep 58: zero_grad(): A.grad= 0.0000 A.data=21.9320 loss=219.0751\n",
      "            step(): A.grad=20.9320 A.data= 2.3665\n",
      "Ep 59: zero_grad(): A.grad= 0.0000 A.data= 2.3665 loss= 0.9336\n",
      "            step(): A.grad= 1.3665 A.data=-21.7520\n",
      "Ep 60: zero_grad(): A.grad= 0.0000 A.data=-21.7520 loss=258.8257\n",
      "            step(): A.grad=-22.7520 A.data=-5.0535\n",
      "Ep 61: zero_grad(): A.grad= 0.0000 A.data=-5.0535 loss=18.3224\n",
      "            step(): A.grad=-6.0535 A.data=24.8164\n",
      "Ep 62: zero_grad(): A.grad= 0.0000 A.data=24.8164 loss=283.6116\n",
      "            step(): A.grad=23.8164 A.data=12.4221\n",
      "Ep 63: zero_grad(): A.grad= 0.0000 A.data=12.4221 loss=65.2325\n",
      "            step(): A.grad=11.4221 A.data=-22.9137\n",
      "Ep 64: zero_grad(): A.grad= 0.0000 A.data=-22.9137 loss=285.9317\n",
      "            step(): A.grad=-23.9137 A.data=-16.3471\n",
      "Ep 65: zero_grad(): A.grad= 0.0000 A.data=-16.3471 loss=150.4606\n",
      "            step(): A.grad=-17.3471 A.data=23.8356\n",
      "Ep 66: zero_grad(): A.grad= 0.0000 A.data=23.8356 loss=260.7327\n",
      "            step(): A.grad=22.8356 A.data=24.6489\n",
      "Ep 67: zero_grad(): A.grad= 0.0000 A.data=24.6489 loss=279.6355\n",
      "            step(): A.grad=23.6489 A.data=-19.3894\n",
      "Ep 68: zero_grad(): A.grad= 0.0000 A.data=-19.3894 loss=207.8637\n",
      "            step(): A.grad=-20.3894 A.data=-29.0917\n",
      "Ep 69: zero_grad(): A.grad= 0.0000 A.data=-29.0917 loss=452.7547\n",
      "            step(): A.grad=-30.0917 A.data=17.4100\n",
      "Ep 70: zero_grad(): A.grad= 0.0000 A.data=17.4100 loss=134.6440\n",
      "            step(): A.grad=16.4100 A.data=37.3829\n",
      "Ep 71: zero_grad(): A.grad= 0.0000 A.data=37.3829 loss=661.8561\n",
      "            step(): A.grad=36.3829 A.data=-9.7744\n",
      "Ep 72: zero_grad(): A.grad= 0.0000 A.data=-9.7744 loss=58.0441\n",
      "            step(): A.grad=-10.7744 A.data=-41.1760\n",
      "Ep 73: zero_grad(): A.grad= 0.0000 A.data=-41.1760 loss=889.4086\n",
      "            step(): A.grad=-42.1760 A.data= 4.4167\n",
      "Ep 74: zero_grad(): A.grad= 0.0000 A.data= 4.4167 loss= 5.8368\n",
      "            step(): A.grad= 3.4167 A.data=48.0770\n",
      "Ep 75: zero_grad(): A.grad= 0.0000 A.data=48.0770 loss=1108.1201\n",
      "            step(): A.grad=47.0770 A.data= 6.6571\n",
      "Ep 76: zero_grad(): A.grad= 0.0000 A.data= 6.6571 loss=16.0012\n",
      "            step(): A.grad= 5.6571 A.data=-49.6532\n",
      "Ep 77: zero_grad(): A.grad= 0.0000 A.data=-49.6532 loss=1282.8756\n",
      "            step(): A.grad=-50.6532 A.data=-15.3534\n",
      "Ep 78: zero_grad(): A.grad= 0.0000 A.data=-15.3534 loss=133.7174\n",
      "            step(): A.grad=-16.3534 A.data=53.4479\n",
      "Ep 79: zero_grad(): A.grad= 0.0000 A.data=53.4479 loss=1375.3903\n",
      "            step(): A.grad=52.4479 A.data=29.4784\n",
      "Ep 80: zero_grad(): A.grad= 0.0000 A.data=29.4784 loss=405.5085\n",
      "            step(): A.grad=28.4784 A.data=-50.9970\n",
      "Ep 81: zero_grad(): A.grad= 0.0000 A.data=-50.9970 loss=1351.8439\n",
      "            step(): A.grad=-51.9970 A.data=-40.7256\n",
      "Ep 82: zero_grad(): A.grad= 0.0000 A.data=-40.7256 loss=870.5129\n",
      "            step(): A.grad=-41.7256 A.data=49.8516\n",
      "Ep 83: zero_grad(): A.grad= 0.0000 A.data=49.8516 loss=1193.2383\n",
      "            step(): A.grad=48.8516 A.data=56.6685\n",
      "Ep 84: zero_grad(): A.grad= 0.0000 A.data=56.6685 loss=1549.4896\n",
      "            step(): A.grad=55.6685 A.data=-41.6030\n",
      "Ep 85: zero_grad(): A.grad= 0.0000 A.data=-41.6030 loss=907.5095\n",
      "            step(): A.grad=-42.6030 A.data=-68.7559\n",
      "Ep 86: zero_grad(): A.grad= 0.0000 A.data=-68.7559 loss=2432.9453\n",
      "            step(): A.grad=-69.7559 A.data=33.9121\n",
      "Ep 87: zero_grad(): A.grad= 0.0000 A.data=33.9121 loss=541.6047\n",
      "            step(): A.grad=32.9121 A.data=84.3140\n",
      "Ep 88: zero_grad(): A.grad= 0.0000 A.data=84.3140 loss=3470.6084\n",
      "            step(): A.grad=83.3140 A.data=-18.5406\n",
      "Ep 89: zero_grad(): A.grad= 0.0000 A.data=-18.5406 loss=190.9169\n",
      "            step(): A.grad=-19.5406 A.data=-94.5535\n",
      "Ep 90: zero_grad(): A.grad= 0.0000 A.data=-94.5535 loss=4565.2339\n",
      "            step(): A.grad=-95.5535 A.data= 3.3839\n",
      "Ep 91: zero_grad(): A.grad= 0.0000 A.data= 3.3839 loss= 2.8416\n",
      "            step(): A.grad= 2.3839 A.data=106.5856\n",
      "Ep 92: zero_grad(): A.grad= 0.0000 A.data=106.5856 loss=5574.1611\n",
      "            step(): A.grad=105.5856 A.data=19.4948\n",
      "Ep 93: zero_grad(): A.grad= 0.0000 A.data=19.4948 loss=171.0290\n",
      "            step(): A.grad=18.4948 A.data=-111.4452\n",
      "Ep 94: zero_grad(): A.grad= 0.0000 A.data=-111.4452 loss=6321.9619\n",
      "            step(): A.grad=-112.4452 A.data=-41.8333\n",
      "Ep 95: zero_grad(): A.grad= 0.0000 A.data=-41.8333 loss=917.3474\n",
      "            step(): A.grad=-42.8333 A.data=116.1231\n",
      "Ep 96: zero_grad(): A.grad= 0.0000 A.data=116.1231 loss=6626.6606\n",
      "            step(): A.grad=115.1231 A.data=71.1413\n",
      "Ep 97: zero_grad(): A.grad= 0.0000 A.data=71.1413 loss=2459.9001\n",
      "            step(): A.grad=70.1413 A.data=-111.6071\n",
      "Ep 98: zero_grad(): A.grad= 0.0000 A.data=-111.6071 loss=6340.1807\n",
      "            step(): A.grad=-112.6071 A.data=-98.6769\n",
      "Ep 99: zero_grad(): A.grad= 0.0000 A.data=-98.6769 loss=4967.7378\n",
      "            step(): A.grad=-99.6769 A.data=104.9325\n",
      "Ep100: zero_grad(): A.grad= 0.0000 A.data=104.9325 loss=5400.9780\n",
      "            step(): A.grad=103.9325 A.data=131.4310\n",
      "Ep101: zero_grad(): A.grad= 0.0000 A.data=131.4310 loss=8506.1289\n",
      "            step(): A.grad=130.4310 A.data=-87.2395\n",
      "Ep102: zero_grad(): A.grad= 0.0000 A.data=-87.2395 loss=3893.1047\n",
      "            step(): A.grad=-88.2395 A.data=-160.1221\n",
      "Ep103: zero_grad(): A.grad= 0.0000 A.data=-160.1221 loss=12980.1582\n",
      "            step(): A.grad=-161.1221 A.data=65.8391\n",
      "Ep104: zero_grad(): A.grad= 0.0000 A.data=65.8391 loss=2102.0513\n",
      "            step(): A.grad=64.8391 A.data=191.2021\n",
      "Ep105: zero_grad(): A.grad= 0.0000 A.data=191.2021 loss=18088.4121\n",
      "            step(): A.grad=190.2021 A.data=-32.2825\n",
      "Ep106: zero_grad(): A.grad= 0.0000 A.data=-32.2825 loss=553.8640\n",
      "            step(): A.grad=-33.2825 A.data=-214.8788\n",
      "Ep107: zero_grad(): A.grad= 0.0000 A.data=-214.8788 loss=23301.8184\n",
      "            step(): A.grad=-215.8788 A.data=-5.5650\n",
      "Ep108: zero_grad(): A.grad= 0.0000 A.data=-5.5650 loss=21.5494\n",
      "            step(): A.grad=-6.5650 A.data=237.1536\n",
      "Ep109: zero_grad(): A.grad= 0.0000 A.data=237.1536 loss=27884.2715\n",
      "            step(): A.grad=236.1536 A.data=55.4522\n",
      "Ep110: zero_grad(): A.grad= 0.0000 A.data=55.4522 loss=1482.5223\n",
      "            step(): A.grad=54.4522 A.data=-247.8786\n",
      "Ep111: zero_grad(): A.grad= 0.0000 A.data=-247.8786 loss=30970.2676\n",
      "            step(): A.grad=-248.8786 A.data=-108.6732\n",
      "Ep112: zero_grad(): A.grad= 0.0000 A.data=-108.6732 loss=6014.1025\n",
      "            step(): A.grad=-109.6732 A.data=252.8318\n",
      "Ep113: zero_grad(): A.grad= 0.0000 A.data=252.8318 loss=31709.6211\n",
      "            step(): A.grad=251.8318 A.data=172.0069\n",
      "Ep114: zero_grad(): A.grad= 0.0000 A.data=172.0069 loss=14621.6738\n",
      "            step(): A.grad=171.0069 A.data=-241.8136\n",
      "Ep115: zero_grad(): A.grad= 0.0000 A.data=-241.8136 loss=29479.2148\n",
      "            step(): A.grad=-242.8136 A.data=-235.6703\n",
      "Ep116: zero_grad(): A.grad= 0.0000 A.data=-235.6703 loss=28006.4121\n",
      "            step(): A.grad=-236.6703 A.data=220.7609\n",
      "Ep117: zero_grad(): A.grad= 0.0000 A.data=220.7609 loss=24147.4180\n",
      "            step(): A.grad=219.7609 A.data=305.2895\n",
      "Ep118: zero_grad(): A.grad= 0.0000 A.data=305.2895 loss=46296.0547\n",
      "            step(): A.grad=304.2895 A.data=-179.8790\n",
      "Ep119: zero_grad(): A.grad= 0.0000 A.data=-179.8790 loss=16358.6113\n",
      "            step(): A.grad=-180.8790 A.data=-369.8943\n",
      "Ep120: zero_grad(): A.grad= 0.0000 A.data=-369.8943 loss=68781.2891\n",
      "            step(): A.grad=-370.8943 A.data=125.7880\n",
      "Ep121: zero_grad(): A.grad= 0.0000 A.data=125.7880 loss=7786.0254\n",
      "            step(): A.grad=124.7880 A.data=433.9414\n",
      "Ep122: zero_grad(): A.grad= 0.0000 A.data=433.9414 loss=93719.1172\n",
      "            step(): A.grad=432.9414 A.data=-49.6786\n",
      "Ep123: zero_grad(): A.grad= 0.0000 A.data=-49.6786 loss=1284.1581\n",
      "            step(): A.grad=-50.6786 A.data=-485.3712\n",
      "Ep124: zero_grad(): A.grad= 0.0000 A.data=-485.3712 loss=118278.4766\n",
      "            step(): A.grad=-486.3712 A.data=-40.5278\n",
      "Ep125: zero_grad(): A.grad= 0.0000 A.data=-40.5278 loss=862.2804\n",
      "            step(): A.grad=-41.5278 A.data=527.7028\n",
      "Ep126: zero_grad(): A.grad= 0.0000 A.data=527.7028 loss=138707.9375\n",
      "            step(): A.grad=526.7028 A.data=152.0212\n",
      "Ep127: zero_grad(): A.grad= 0.0000 A.data=152.0212 loss=11403.7031\n",
      "            step(): A.grad=151.0212 A.data=-548.1689\n",
      "Ep128: zero_grad(): A.grad= 0.0000 A.data=-548.1689 loss=150793.2656\n",
      "            step(): A.grad=-549.1689 A.data=-274.9571\n",
      "Ep129: zero_grad(): A.grad= 0.0000 A.data=-274.9571 loss=38076.1602\n",
      "            step(): A.grad=-275.9571 A.data=549.8944\n",
      "Ep130: zero_grad(): A.grad= 0.0000 A.data=549.8944 loss=150642.5312\n",
      "            step(): A.grad=548.8944 A.data=414.3317\n",
      "Ep131: zero_grad(): A.grad= 0.0000 A.data=414.3317 loss=85421.5547\n",
      "            step(): A.grad=413.3317 A.data=-520.1175\n",
      "Ep132: zero_grad(): A.grad= 0.0000 A.data=-520.1175 loss=135781.7188\n",
      "            step(): A.grad=-521.1175 A.data=-557.8884\n",
      "Ep133: zero_grad(): A.grad= 0.0000 A.data=-557.8884 loss=156178.1406\n",
      "            step(): A.grad=-558.8884 A.data=462.4515\n",
      "Ep134: zero_grad(): A.grad= 0.0000 A.data=462.4515 loss=106468.7344\n",
      "            step(): A.grad=461.4515 A.data=708.0675\n",
      "Ep135: zero_grad(): A.grad= 0.0000 A.data=708.0675 loss=249972.2344\n",
      "            step(): A.grad=707.0675 A.data=-365.1830\n",
      "Ep136: zero_grad(): A.grad= 0.0000 A.data=-365.1830 loss=67044.9922\n",
      "            step(): A.grad=-366.1830 A.data=-850.0109\n",
      "Ep137: zero_grad(): A.grad= 0.0000 A.data=-850.0109 loss=362109.8125\n",
      "            step(): A.grad=-851.0109 A.data=233.5991\n",
      "Ep138: zero_grad(): A.grad= 0.0000 A.data=233.5991 loss=27051.1621\n",
      "            step(): A.grad=232.5991 A.data=983.6318\n",
      "Ep139: zero_grad(): A.grad= 0.0000 A.data=983.6318 loss=482782.6562\n",
      "            step(): A.grad=982.6318 A.data=-58.3326\n",
      "Ep140: zero_grad(): A.grad= 0.0000 A.data=-58.3326 loss=1760.1812\n",
      "            step(): A.grad=-59.3326 A.data=-1091.7616\n",
      "Ep141: zero_grad(): A.grad= 0.0000 A.data=-1091.7616 loss=597063.9375\n",
      "            step(): A.grad=-1092.7616 A.data=-152.2864\n",
      "Ep142: zero_grad(): A.grad= 0.0000 A.data=-152.2864 loss=11748.3564\n",
      "            step(): A.grad=-153.2864 A.data=1172.3804\n",
      "Ep143: zero_grad(): A.grad= 0.0000 A.data=1172.3804 loss=686066.0000\n",
      "            step(): A.grad=1171.3804 A.data=403.8912\n",
      "Ep144: zero_grad(): A.grad= 0.0000 A.data=403.8912 loss=81160.6484\n",
      "            step(): A.grad=402.8912 A.data=-1206.9402\n",
      "Ep145: zero_grad(): A.grad= 0.0000 A.data=-1206.9402 loss=729559.7500\n",
      "            step(): A.grad=-1207.9402 A.data=-683.7685\n",
      "Ep146: zero_grad(): A.grad= 0.0000 A.data=-683.7685 loss=234453.9375\n",
      "            step(): A.grad=-684.7685 A.data=1192.7805\n",
      "Ep147: zero_grad(): A.grad= 0.0000 A.data=1192.7805 loss=710170.3750\n",
      "            step(): A.grad=1191.7805 A.data=992.6014\n",
      "Ep148: zero_grad(): A.grad= 0.0000 A.data=992.6014 loss=491636.7188\n",
      "            step(): A.grad=991.6014 A.data=-1111.6383\n",
      "Ep149: zero_grad(): A.grad= 0.0000 A.data=-1111.6383 loss=618982.0000\n",
      "            step(): A.grad=-1112.6383 A.data=-1312.2894\n",
      "Ep150: zero_grad(): A.grad= 0.0000 A.data=-1312.2894 loss=862364.5625\n",
      "            step(): A.grad=-1313.2894 A.data=962.2443\n",
      "Ep151: zero_grad(): A.grad= 0.0000 A.data=962.2443 loss=461995.2812\n",
      "            step(): A.grad=961.2443 A.data=1637.8672\n",
      "Ep152: zero_grad(): A.grad= 0.0000 A.data=1637.8672 loss=1339667.1250\n",
      "            step(): A.grad=1636.8672 A.data=-728.9951\n",
      "Ep153: zero_grad(): A.grad= 0.0000 A.data=-728.9951 loss=266446.4375\n",
      "            step(): A.grad=-729.9951 A.data=-1945.5531\n",
      "Ep154: zero_grad(): A.grad= 0.0000 A.data=-1945.5531 loss=1894534.5000\n",
      "            step(): A.grad=-1946.5531 A.data=414.6837\n",
      "Ep155: zero_grad(): A.grad= 0.0000 A.data=414.6837 loss=85567.1094\n",
      "            step(): A.grad=413.6837 A.data=2224.9453\n",
      "Ep156: zero_grad(): A.grad= 0.0000 A.data=2224.9453 loss=2472966.5000\n",
      "            step(): A.grad=2223.9453 A.data=-9.2629\n",
      "Ep157: zero_grad(): A.grad= 0.0000 A.data=-9.2629 loss=52.6640\n",
      "            step(): A.grad=-10.2629 A.data=-2447.3923\n",
      "Ep158: zero_grad(): A.grad= 0.0000 A.data=-2447.3923 loss=2997312.5000\n",
      "            step(): A.grad=-2448.3923 A.data=-477.3895\n",
      "Ep159: zero_grad(): A.grad= 0.0000 A.data=-477.3895 loss=114428.2656\n",
      "            step(): A.grad=-478.3895 A.data=2598.5537\n",
      "Ep160: zero_grad(): A.grad= 0.0000 A.data=2598.5537 loss=3373642.7500\n",
      "            step(): A.grad=2597.5537 A.data=1046.7396\n",
      "Ep161: zero_grad(): A.grad= 0.0000 A.data=1046.7396 loss=546785.6875\n",
      "            step(): A.grad=1045.7396 A.data=-2647.1611\n",
      "Ep162: zero_grad(): A.grad= 0.0000 A.data=-2647.1611 loss=3506378.7500\n",
      "            step(): A.grad=-2648.1611 A.data=-1678.9460\n",
      "Ep163: zero_grad(): A.grad= 0.0000 A.data=-1678.9460 loss=1411109.3750\n",
      "            step(): A.grad=-1679.9460 A.data=2577.9880\n",
      "Ep164: zero_grad(): A.grad= 0.0000 A.data=2577.9880 loss=3320433.7500\n",
      "            step(): A.grad=2576.9880 A.data=2364.3386\n",
      "Ep165: zero_grad(): A.grad= 0.0000 A.data=2364.3386 loss=2792684.7500\n",
      "            step(): A.grad=2363.3386 A.data=-2361.0188\n",
      "Ep166: zero_grad(): A.grad= 0.0000 A.data=-2361.0188 loss=2789566.5000\n",
      "            step(): A.grad=-2362.0188 A.data=-3071.0767\n",
      "Ep167: zero_grad(): A.grad= 0.0000 A.data=-3071.0767 loss=4718827.5000\n",
      "            step(): A.grad=-3072.0767 A.data=1984.8057\n",
      "Ep168: zero_grad(): A.grad= 0.0000 A.data=1984.8057 loss=1967742.5000\n",
      "            step(): A.grad=1983.8057 A.data=3777.0454\n",
      "Ep169: zero_grad(): A.grad= 0.0000 A.data=3777.0454 loss=7129259.5000\n",
      "            step(): A.grad=3776.0454 A.data=-1425.9766\n",
      "Ep170: zero_grad(): A.grad= 0.0000 A.data=-1425.9766 loss=1018131.0625\n",
      "            step(): A.grad=-1426.9766 A.data=-4438.0449\n",
      "Ep171: zero_grad(): A.grad= 0.0000 A.data=-4438.0449 loss=9852560.0000\n",
      "            step(): A.grad=-4439.0449 A.data=682.8647\n",
      "Ep172: zero_grad(): A.grad= 0.0000 A.data=682.8647 loss=232469.7656\n",
      "            step(): A.grad=681.8647 A.data=5020.3228\n",
      "Ep173: zero_grad(): A.grad= 0.0000 A.data=5020.3228 loss=12596800.0000\n",
      "            step(): A.grad=5019.3228 A.data=254.8135\n",
      "Ep174: zero_grad(): A.grad= 0.0000 A.data=254.8135 loss=32210.6406\n",
      "            step(): A.grad=253.8135 A.data=-5469.4927\n",
      "Ep175: zero_grad(): A.grad= 0.0000 A.data=-5469.4927 loss=14963145.0000\n",
      "            step(): A.grad=-5470.4927 A.data=-1372.2935\n",
      "Ep176: zero_grad(): A.grad= 0.0000 A.data=-1372.2935 loss=942967.4375\n",
      "            step(): A.grad=-1373.2935 A.data=5743.8833\n",
      "Ep177: zero_grad(): A.grad= 0.0000 A.data=5743.8833 loss=16490354.0000\n",
      "            step(): A.grad=5742.8833 A.data=2660.2000\n",
      "Ep178: zero_grad(): A.grad= 0.0000 A.data=2660.2000 loss=3535672.2500\n",
      "            step(): A.grad=2659.2000 A.data=-5784.3311\n",
      "Ep179: zero_grad(): A.grad= 0.0000 A.data=-5784.3311 loss=16735028.0000\n",
      "            step(): A.grad=-5785.3311 A.data=-4081.1870\n",
      "Ep180: zero_grad(): A.grad= 0.0000 A.data=-4081.1870 loss=8332125.5000\n",
      "            step(): A.grad=-4082.1870 A.data=5548.4263\n",
      "Ep181: zero_grad(): A.grad= 0.0000 A.data=5548.4263 loss=15386969.0000\n",
      "            step(): A.grad=5547.4263 A.data=5600.8916\n",
      "Ep182: zero_grad(): A.grad= 0.0000 A.data=5600.8916 loss=15679393.0000\n",
      "            step(): A.grad=5599.8916 A.data=-4981.1904\n",
      "Ep183: zero_grad(): A.grad= 0.0000 A.data=-4981.1904 loss=12411111.0000\n",
      "            step(): A.grad=-4982.1904 A.data=-7155.3193\n",
      "Ep184: zero_grad(): A.grad= 0.0000 A.data=-7155.3193 loss=25606454.0000\n",
      "            step(): A.grad=-7156.3193 A.data=4050.1455\n",
      "Ep185: zero_grad(): A.grad= 0.0000 A.data=4050.1455 loss=8197789.5000\n",
      "            step(): A.grad=4049.1455 A.data=8682.7812\n",
      "Ep186: zero_grad(): A.grad= 0.0000 A.data=8682.7812 loss=37686664.0000\n",
      "            step(): A.grad=8681.7812 A.data=-2716.7051\n",
      "Ep187: zero_grad(): A.grad= 0.0000 A.data=-2716.7051 loss=3692960.5000\n",
      "            step(): A.grad=-2717.7051 A.data=-10092.5000\n",
      "Ep188: zero_grad(): A.grad= 0.0000 A.data=-10092.5000 loss=50939372.0000\n",
      "            step(): A.grad=-10093.5000 A.data=971.7754\n",
      "Ep189: zero_grad(): A.grad= 0.0000 A.data=971.7754 loss=471202.4375\n",
      "            step(): A.grad=970.7754 A.data=11298.0049\n",
      "Ep190: zero_grad(): A.grad= 0.0000 A.data=11298.0049 loss=63811160.0000\n",
      "            step(): A.grad=11297.0049 A.data=1192.5488\n",
      "Ep191: zero_grad(): A.grad= 0.0000 A.data=1192.5488 loss=709894.3125\n",
      "            step(): A.grad=1191.5488 A.data=-12187.3955\n",
      "Ep192: zero_grad(): A.grad= 0.0000 A.data=-12187.3955 loss=74278496.0000\n",
      "            step(): A.grad=-12188.3955 A.data=-3747.3838\n",
      "Ep193: zero_grad(): A.grad= 0.0000 A.data=-3747.3838 loss=7025190.5000\n",
      "            step(): A.grad=-3748.3838 A.data=12658.5596\n",
      "Ep194: zero_grad(): A.grad= 0.0000 A.data=12658.5596 loss=80106904.0000\n",
      "            step(): A.grad=12657.5596 A.data=6655.7344\n",
      "Ep195: zero_grad(): A.grad= 0.0000 A.data=6655.7344 loss=22142744.0000\n",
      "            step(): A.grad=6654.7344 A.data=-12591.3691\n",
      "Ep196: zero_grad(): A.grad= 0.0000 A.data=-12591.3691 loss=79283880.0000\n",
      "            step(): A.grad=-12592.3691 A.data=-9837.6826\n",
      "Ep197: zero_grad(): A.grad= 0.0000 A.data=-9837.6826 loss=48399836.0000\n",
      "            step(): A.grad=-9838.6826 A.data=11884.8682\n",
      "Ep198: zero_grad(): A.grad= 0.0000 A.data=11884.8682 loss=70613160.0000\n",
      "            step(): A.grad=11883.8682 A.data=13200.3252\n",
      "Ep199: zero_grad(): A.grad= 0.0000 A.data=13200.3252 loss=87111096.0000\n",
      "            step(): A.grad=13199.3252 A.data=-10431.3896\n",
      "Ep200: zero_grad(): A.grad= 0.0000 A.data=-10431.3896 loss=54417376.0000\n",
      "            step(): A.grad=-10432.3896 A.data=-16604.7363\n",
      "Ep201: zero_grad(): A.grad= 0.0000 A.data=-16604.7363 loss=137875232.0000\n",
      "            step(): A.grad=-16605.7363 A.data=8155.4805\n",
      "Ep202: zero_grad(): A.grad= 0.0000 A.data=8155.4805 loss=33247776.0000\n",
      "            step(): A.grad=8154.4805 A.data=19898.2070\n",
      "Ep203: zero_grad(): A.grad= 0.0000 A.data=19898.2070 loss=197949424.0000\n",
      "            step(): A.grad=19897.2070 A.data=-4989.4844\n",
      "Ep204: zero_grad(): A.grad= 0.0000 A.data=-4989.4844 loss=12452467.0000\n",
      "            step(): A.grad=-4990.4844 A.data=-22884.0254\n",
      "Ep205: zero_grad(): A.grad= 0.0000 A.data=-22884.0254 loss=261862192.0000\n",
      "            step(): A.grad=-22885.0254 A.data=913.5254\n",
      "Ep206: zero_grad(): A.grad= 0.0000 A.data=913.5254 loss=416351.2812\n",
      "            step(): A.grad=912.5254 A.data=25357.0352\n",
      "Ep207: zero_grad(): A.grad= 0.0000 A.data=25357.0352 loss=321464256.0000\n",
      "            step(): A.grad=25356.0352 A.data=4068.4297\n",
      "Ep208: zero_grad(): A.grad= 0.0000 A.data=4068.4297 loss=8271992.0000\n",
      "            step(): A.grad=4067.4297 A.data=-27077.1523\n",
      "Ep209: zero_grad(): A.grad= 0.0000 A.data=-27077.1523 loss=366613152.0000\n",
      "            step(): A.grad=-27078.1523 A.data=-9888.8066\n",
      "Ep210: zero_grad(): A.grad= 0.0000 A.data=-9888.8066 loss=48904136.0000\n",
      "            step(): A.grad=-9889.8066 A.data=27809.0059\n",
      "Ep211: zero_grad(): A.grad= 0.0000 A.data=27809.0059 loss=386642592.0000\n",
      "            step(): A.grad=27808.0059 A.data=16441.3906\n",
      "Ep212: zero_grad(): A.grad= 0.0000 A.data=16441.3906 loss=135143216.0000\n",
      "            step(): A.grad=16440.3906 A.data=-27299.7266\n",
      "Ep213: zero_grad(): A.grad= 0.0000 A.data=-27299.7266 loss=372664832.0000\n",
      "            step(): A.grad=-27300.7266 A.data=-23543.5742\n",
      "Ep214: zero_grad(): A.grad= 0.0000 A.data=-23543.5742 loss=277173472.0000\n",
      "            step(): A.grad=-23544.5742 A.data=25322.8828\n",
      "Ep215: zero_grad(): A.grad= 0.0000 A.data=25322.8828 loss=320598880.0000\n",
      "            step(): A.grad=25321.8828 A.data=30964.4102\n",
      "Ep216: zero_grad(): A.grad= 0.0000 A.data=30964.4102 loss=479366400.0000\n",
      "            step(): A.grad=30963.4102 A.data=-21660.3906\n",
      "Ep217: zero_grad(): A.grad= 0.0000 A.data=-21660.3906 loss=234607920.0000\n",
      "            step(): A.grad=-21661.3906 A.data=-38391.0312\n",
      "Ep218: zero_grad(): A.grad= 0.0000 A.data=-38391.0312 loss=736974016.0000\n",
      "            step(): A.grad=-38392.0312 A.data=16150.1250\n",
      "Ep219: zero_grad(): A.grad= 0.0000 A.data=16150.1250 loss=130397120.0000\n",
      "            step(): A.grad=16149.1250 A.data=45462.0586\n",
      "Ep220: zero_grad(): A.grad= 0.0000 A.data=45462.0586 loss=1033353920.0000\n",
      "            step(): A.grad=45461.0586 A.data=-8670.8203\n",
      "Ep221: zero_grad(): A.grad= 0.0000 A.data=-8670.8203 loss=37600232.0000\n",
      "            step(): A.grad=-8671.8203 A.data=-51740.5312\n",
      "Ep222: zero_grad(): A.grad= 0.0000 A.data=-51740.5312 loss=1338593024.0000\n",
      "            step(): A.grad=-51741.5312 A.data=-808.3086\n",
      "Ep223: zero_grad(): A.grad= 0.0000 A.data=-808.3086 loss=327490.1875\n",
      "            step(): A.grad=-809.3086 A.data=56754.8242\n",
      "Ep224: zero_grad(): A.grad= 0.0000 A.data=56754.8242 loss=1610498304.0000\n",
      "            step(): A.grad=56753.8242 A.data=12242.0117\n",
      "Ep225: zero_grad(): A.grad= 0.0000 A.data=12242.0117 loss=74921184.0000\n",
      "            step(): A.grad=12241.0117 A.data=-59980.0039\n",
      "Ep226: zero_grad(): A.grad= 0.0000 A.data=-59980.0039 loss=1798860416.0000\n",
      "            step(): A.grad=-59981.0039 A.data=-25460.3164\n",
      "Ep227: zero_grad(): A.grad= 0.0000 A.data=-25460.3164 loss=324139328.0000\n",
      "            step(): A.grad=-25461.3164 A.data=60887.8398\n",
      "Ep228: zero_grad(): A.grad= 0.0000 A.data=60887.8398 loss=1853603584.0000\n",
      "            step(): A.grad=60886.8398 A.data=40185.8203\n",
      "Ep229: zero_grad(): A.grad= 0.0000 A.data=40185.8203 loss=807409920.0000\n",
      "            step(): A.grad=40184.8203 A.data=-58937.5625\n",
      "Ep230: zero_grad(): A.grad= 0.0000 A.data=-58937.5625 loss=1736877056.0000\n",
      "            step(): A.grad=-58938.5625 A.data=-55990.0234\n",
      "Ep231: zero_grad(): A.grad= 0.0000 A.data=-55990.0234 loss=1567497344.0000\n",
      "            step(): A.grad=-55991.0234 A.data=53635.2109\n",
      "Ep232: zero_grad(): A.grad= 0.0000 A.data=53635.2109 loss=1438314240.0000\n",
      "            step(): A.grad=53634.2109 A.data=72317.9688\n",
      "Ep233: zero_grad(): A.grad= 0.0000 A.data=72317.9688 loss=2614872064.0000\n",
      "            step(): A.grad=72316.9688 A.data=-44533.2344\n",
      "Ep234: zero_grad(): A.grad= 0.0000 A.data=-44533.2344 loss=991649024.0000\n",
      "            step(): A.grad=-44534.2344 A.data=-88454.5078\n",
      "Ep235: zero_grad(): A.grad= 0.0000 A.data=-88454.5078 loss=3912188416.0000\n",
      "            step(): A.grad=-88455.5078 A.data=31297.5469\n",
      "Ep236: zero_grad(): A.grad= 0.0000 A.data=31297.5469 loss=489736928.0000\n",
      "            step(): A.grad=31296.5469 A.data=103561.3750\n",
      "Ep237: zero_grad(): A.grad= 0.0000 A.data=103561.3750 loss=5362375680.0000\n",
      "            step(): A.grad=103560.3750 A.data=-13713.1250\n",
      "Ep238: zero_grad(): A.grad= 0.0000 A.data=-13713.1250 loss=94038616.0000\n",
      "            step(): A.grad=-13714.1250 A.data=-116658.2422\n",
      "Ep239: zero_grad(): A.grad= 0.0000 A.data=-116658.2422 loss=6804689408.0000\n",
      "            step(): A.grad=-116659.2422 A.data=-8245.3125\n",
      "Ep240: zero_grad(): A.grad= 0.0000 A.data=-8245.3125 loss=34000836.0000\n",
      "            step(): A.grad=-8246.3125 A.data=126676.9062\n",
      "Ep241: zero_grad(): A.grad= 0.0000 A.data=126676.9062 loss=8023392768.0000\n",
      "            step(): A.grad=126675.9062 A.data=34407.1328\n",
      "Ep242: zero_grad(): A.grad= 0.0000 A.data=34407.1328 loss=591891008.0000\n",
      "            step(): A.grad=34406.1328 A.data=-132461.2812\n",
      "Ep243: zero_grad(): A.grad= 0.0000 A.data=-132461.2812 loss=8773128192.0000\n",
      "            step(): A.grad=-132462.2812 A.data=-64338.1953\n",
      "Ep244: zero_grad(): A.grad= 0.0000 A.data=-64338.1953 loss=2069766016.0000\n",
      "            step(): A.grad=-64339.1953 A.data=132841.6562\n",
      "Ep245: zero_grad(): A.grad= 0.0000 A.data=132841.6562 loss=8823319552.0000\n",
      "            step(): A.grad=132840.6562 A.data=97342.2656\n",
      "Ep246: zero_grad(): A.grad= 0.0000 A.data=97342.2656 loss=4737660928.0000\n",
      "            step(): A.grad=97341.2656 A.data=-126655.4688\n",
      "Ep247: zero_grad(): A.grad= 0.0000 A.data=-126655.4688 loss=8020930560.0000\n",
      "            step(): A.grad=-126656.4688 A.data=-132405.7031\n",
      "Ep248: zero_grad(): A.grad= 0.0000 A.data=-132405.7031 loss=8765767680.0000\n",
      "            step(): A.grad=-132406.7031 A.data=112841.7812\n",
      "Ep249: zero_grad(): A.grad= 0.0000 A.data=112841.7812 loss=6366520832.0000\n",
      "            step(): A.grad=112840.7812 A.data=168216.5312\n",
      "Ep250: zero_grad(): A.grad= 0.0000 A.data=168216.5312 loss=14148232192.0000\n",
      "            step(): A.grad=168215.5312 A.data=-90480.7500\n",
      "Ep251: zero_grad(): A.grad= 0.0000 A.data=-90480.7500 loss=4093473536.0000\n",
      "            step(): A.grad=-90481.7500 A.data=-203132.4688\n",
      "Ep252: zero_grad(): A.grad= 0.0000 A.data=-203132.4688 loss=20631603200.0000\n",
      "            step(): A.grad=-203133.4688 A.data=58904.2500\n",
      "Ep253: zero_grad(): A.grad= 0.0000 A.data=58904.2500 loss=1734796416.0000\n",
      "            step(): A.grad=58903.2500 A.data=235228.4688\n",
      "Ep254: zero_grad(): A.grad= 0.0000 A.data=235228.4688 loss=27665981440.0000\n",
      "            step(): A.grad=235227.4688 A.data=-17747.0938\n",
      "Ep255: zero_grad(): A.grad= 0.0000 A.data=-17747.0938 loss=157497408.0000\n",
      "            step(): A.grad=-17748.0938 A.data=-262298.8125\n",
      "Ep256: zero_grad(): A.grad= 0.0000 A.data=-262298.8125 loss=34400595968.0000\n",
      "            step(): A.grad=-262299.8125 A.data=-32936.0625\n",
      "Ep257: zero_grad(): A.grad= 0.0000 A.data=-32936.0625 loss=542425024.0000\n",
      "            step(): A.grad=-32937.0625 A.data=281943.3750\n",
      "Ep258: zero_grad(): A.grad= 0.0000 A.data=281943.3750 loss=39745753088.0000\n",
      "            step(): A.grad=281942.3750 A.data=92620.2500\n",
      "Ep259: zero_grad(): A.grad= 0.0000 A.data=92620.2500 loss=4289162752.0000\n",
      "            step(): A.grad=92619.2500 A.data=-291611.7500\n",
      "Ep260: zero_grad(): A.grad= 0.0000 A.data=-291611.7500 loss=42518999040.0000\n",
      "            step(): A.grad=-291612.7500 A.data=-160202.7656\n",
      "Ep261: zero_grad(): A.grad= 0.0000 A.data=-160202.7656 loss=12832623616.0000\n",
      "            step(): A.grad=-160203.7656 A.data=288734.2500\n",
      "Ep262: zero_grad(): A.grad= 0.0000 A.data=288734.2500 loss=41683443712.0000\n",
      "            step(): A.grad=288733.2500 A.data=233971.8281\n",
      "Ep263: zero_grad(): A.grad= 0.0000 A.data=233971.8281 loss=27371173888.0000\n",
      "            step(): A.grad=233970.8281 A.data=-270811.3750\n",
      "Ep264: zero_grad(): A.grad= 0.0000 A.data=-270811.3750 loss=36669669376.0000\n",
      "            step(): A.grad=-270812.3750 A.data=-311529.3750\n",
      "Ep265: zero_grad(): A.grad= 0.0000 A.data=-311529.3750 loss=48525586432.0000\n",
      "            step(): A.grad=-311530.3750 A.data=235588.5000\n",
      "Ep266: zero_grad(): A.grad= 0.0000 A.data=235588.5000 loss=27750735872.0000\n",
      "            step(): A.grad=235587.5000 A.data=389801.9375\n",
      "Ep267: zero_grad(): A.grad= 0.0000 A.data=389801.9375 loss=75972386816.0000\n",
      "            step(): A.grad=389800.9375 A.data=-181185.0625\n",
      "Ep268: zero_grad(): A.grad= 0.0000 A.data=-181185.0625 loss=16414194688.0000\n",
      "            step(): A.grad=-181186.0625 A.data=-465017.2812\n",
      "Ep269: zero_grad(): A.grad= 0.0000 A.data=-465017.2812 loss=108120997888.0000\n",
      "            step(): A.grad=-465018.2812 A.data=106302.0312\n",
      "Ep270: zero_grad(): A.grad= 0.0000 A.data=106302.0312 loss=5649954816.0000\n",
      "            step(): A.grad=106301.0312 A.data=532781.3125\n",
      "Ep271: zero_grad(): A.grad= 0.0000 A.data=532781.3125 loss=141927432192.0000\n",
      "            step(): A.grad=532780.3125 A.data=-10374.0625\n",
      "Ep272: zero_grad(): A.grad= 0.0000 A.data=-10374.0625 loss=53820960.0000\n",
      "            step(): A.grad=-10375.0625 A.data=-588132.3750\n",
      "Ep273: zero_grad(): A.grad= 0.0000 A.data=-588132.3750 loss=172950437888.0000\n",
      "            step(): A.grad=-588133.3750 A.data=-106213.1250\n",
      "Ep274: zero_grad(): A.grad= 0.0000 A.data=-106213.1250 loss=5640720384.0000\n",
      "            step(): A.grad=-106214.1250 A.data=625704.9375\n",
      "Ep275: zero_grad(): A.grad= 0.0000 A.data=625704.9375 loss=195752706048.0000\n",
      "            step(): A.grad=625703.9375 A.data=241977.3125\n",
      "Ep276: zero_grad(): A.grad= 0.0000 A.data=241977.3125 loss=29276268544.0000\n",
      "            step(): A.grad=241976.3125 A.data=-639878.1250\n",
      "Ep277: zero_grad(): A.grad= 0.0000 A.data=-639878.1250 loss=204722651136.0000\n",
      "            step(): A.grad=-639879.1250 A.data=-394148.8125\n",
      "Ep278: zero_grad(): A.grad= 0.0000 A.data=-394148.8125 loss=77677035520.0000\n",
      "            step(): A.grad=-394149.8125 A.data=625038.0000\n",
      "Ep279: zero_grad(): A.grad= 0.0000 A.data=625038.0000 loss=195335618560.0000\n",
      "            step(): A.grad=625037.0000 A.data=558573.2500\n",
      "Ep280: zero_grad(): A.grad= 0.0000 A.data=558573.2500 loss=156001484800.0000\n",
      "            step(): A.grad=558572.2500 A.data=-575825.1250\n",
      "Ep281: zero_grad(): A.grad= 0.0000 A.data=-575825.1250 loss=165787860992.0000\n",
      "            step(): A.grad=-575826.1250 A.data=-729593.7500\n",
      "Ep282: zero_grad(): A.grad= 0.0000 A.data=-729593.7500 loss=266154246144.0000\n",
      "            step(): A.grad=-729594.7500 A.data=487490.7500\n",
      "Ep283: zero_grad(): A.grad= 0.0000 A.data=487490.7500 loss=118823124992.0000\n",
      "            step(): A.grad=487489.7500 A.data=900053.3125\n",
      "Ep284: zero_grad(): A.grad= 0.0000 A.data=900053.3125 loss=405047083008.0000\n",
      "            step(): A.grad=900052.3125 A.data=-356227.3125\n",
      "Ep285: zero_grad(): A.grad= 0.0000 A.data=-356227.3125 loss=63449305088.0000\n",
      "            step(): A.grad=-356228.3125 A.data=-1061302.2500\n",
      "Ep286: zero_grad(): A.grad= 0.0000 A.data=-1061302.2500 loss=563182305280.0000\n",
      "            step(): A.grad=-1061303.2500 A.data=179591.5000\n",
      "Ep287: zero_grad(): A.grad= 0.0000 A.data=179591.5000 loss=16126373888.0000\n",
      "            step(): A.grad=179590.5000 A.data=1203352.7500\n",
      "Ep288: zero_grad(): A.grad= 0.0000 A.data=1203352.7500 loss=724027703296.0000\n",
      "            step(): A.grad=1203351.7500 A.data=43121.7500\n",
      "Ep289: zero_grad(): A.grad= 0.0000 A.data=43121.7500 loss=929699520.0000\n",
      "            step(): A.grad=43120.7500 A.data=-1315061.7500\n",
      "Ep290: zero_grad(): A.grad= 0.0000 A.data=-1315061.7500 loss=864695025664.0000\n",
      "            step(): A.grad=-1315062.7500 A.data=-310444.5000\n",
      "Ep291: zero_grad(): A.grad= 0.0000 A.data=-310444.5000 loss=48188203008.0000\n",
      "            step(): A.grad=-310445.5000 A.data=1384481.0000\n",
      "Ep292: zero_grad(): A.grad= 0.0000 A.data=1384481.0000 loss=958392434688.0000\n",
      "            step(): A.grad=1384480.0000 A.data=618387.1250\n",
      "Ep293: zero_grad(): A.grad= 0.0000 A.data=618387.1250 loss=191200706560.0000\n",
      "            step(): A.grad=618386.1250 A.data=-1399249.8750\n",
      "Ep294: zero_grad(): A.grad= 0.0000 A.data=-1399249.8750 loss=978951536640.0000\n",
      "            step(): A.grad=-1399250.8750 A.data=-960074.1250\n",
      "Ep295: zero_grad(): A.grad= 0.0000 A.data=-960074.1250 loss=460872122368.0000\n",
      "            step(): A.grad=-960075.1250 A.data=1347161.8750\n",
      "Ep296: zero_grad(): A.grad= 0.0000 A.data=1347161.8750 loss=907421220864.0000\n",
      "            step(): A.grad=1347160.8750 A.data=1325515.8750\n",
      "Ep297: zero_grad(): A.grad= 0.0000 A.data=1325515.8750 loss=878494810112.0000\n",
      "            step(): A.grad=1325514.8750 A.data=-1216772.8750\n",
      "Ep298: zero_grad(): A.grad= 0.0000 A.data=-1216772.8750 loss=740269359104.0000\n",
      "            step(): A.grad=-1216773.8750 A.data=-1701420.3750\n",
      "Ep299: zero_grad(): A.grad= 0.0000 A.data=-1701420.3750 loss=1447417348096.0000\n",
      "            step(): A.grad=-1701421.3750 A.data=998167.8750\n",
      "Ep300: zero_grad(): A.grad= 0.0000 A.data=998167.8750 loss=498168561664.0000\n",
      "            step(): A.grad=998166.8750 A.data=2071198.1250\n",
      "Ep301: zero_grad(): A.grad= 0.0000 A.data=2071198.1250 loss=2144928792576.0000\n",
      "            step(): A.grad=2071197.1250 A.data=-683742.8750\n",
      "Ep302: zero_grad(): A.grad= 0.0000 A.data=-683742.8750 loss=233752838144.0000\n",
      "            step(): A.grad=-683743.8750 A.data=-2415064.5000\n",
      "Ep303: zero_grad(): A.grad= 0.0000 A.data=-2415064.5000 loss=2916270735360.0000\n",
      "            step(): A.grad=-2415065.5000 A.data=269105.7500\n",
      "Ep304: zero_grad(): A.grad= 0.0000 A.data=269105.7500 loss=36208685056.0000\n",
      "            step(): A.grad=269104.7500 A.data=2710394.0000\n",
      "Ep305: zero_grad(): A.grad= 0.0000 A.data=2710394.0000 loss=3673115066368.0000\n",
      "            step(): A.grad=2710393.0000 A.data=246064.5000\n",
      "Ep306: zero_grad(): A.grad= 0.0000 A.data=246064.5000 loss=30273622016.0000\n",
      "            step(): A.grad=246063.5000 A.data=-2932218.7500\n",
      "Ep307: zero_grad(): A.grad= 0.0000 A.data=-2932218.7500 loss=4298956341248.0000\n",
      "            step(): A.grad=-2932219.7500 A.data=-857112.8750\n",
      "Ep308: zero_grad(): A.grad= 0.0000 A.data=-857112.8750 loss=367322103808.0000\n",
      "            step(): A.grad=-857113.8750 A.data=3054020.0000\n",
      "Ep309: zero_grad(): A.grad= 0.0000 A.data=3054020.0000 loss=4663516069888.0000\n",
      "            step(): A.grad=3054019.0000 A.data=1553630.3750\n",
      "Ep310: zero_grad(): A.grad= 0.0000 A.data=1553630.3750 loss=1206882140160.0000\n",
      "            step(): A.grad=1553629.3750 A.data=-3048694.0000\n",
      "Ep311: zero_grad(): A.grad= 0.0000 A.data=-3048694.0000 loss=4647270481920.0000\n",
      "            step(): A.grad=-3048695.0000 A.data=-2318730.5000\n",
      "Ep312: zero_grad(): A.grad= 0.0000 A.data=-2318730.5000 loss=2688257884160.0000\n",
      "            step(): A.grad=-2318731.5000 A.data=2889819.0000\n",
      "Ep313: zero_grad(): A.grad= 0.0000 A.data=2889819.0000 loss=4175524003840.0000\n",
      "            step(): A.grad=2889818.0000 A.data=3128569.7500\n",
      "Ep314: zero_grad(): A.grad= 0.0000 A.data=3128569.7500 loss=4893971054592.0000\n",
      "            step(): A.grad=3128568.7500 A.data=-2553085.2500\n",
      "Ep315: zero_grad(): A.grad= 0.0000 A.data=-2553085.2500 loss=3259124678656.0000\n",
      "            step(): A.grad=-2553086.2500 A.data=-3952042.0000\n",
      "Ep316: zero_grad(): A.grad= 0.0000 A.data=-3952042.0000 loss=7809322188800.0000\n",
      "            step(): A.grad=-3952043.0000 A.data=2017987.5000\n",
      "Ep317: zero_grad(): A.grad= 0.0000 A.data=2017987.5000 loss=2036134707200.0000\n",
      "            step(): A.grad=2017986.5000 A.data=4750846.0000\n",
      "Ep318: zero_grad(): A.grad= 0.0000 A.data=4750846.0000 loss=11285264596992.0000\n",
      "            step(): A.grad=4750845.0000 A.data=-1269615.5000\n",
      "Ep319: zero_grad(): A.grad= 0.0000 A.data=-1269615.5000 loss=805963038720.0000\n",
      "            step(): A.grad=-1269616.5000 A.data=-5479852.0000\n",
      "Ep320: zero_grad(): A.grad= 0.0000 A.data=-5479852.0000 loss=15014394068992.0000\n",
      "            step(): A.grad=-5479853.0000 A.data=300608.0000\n",
      "Ep321: zero_grad(): A.grad= 0.0000 A.data=300608.0000 loss=45182283776.0000\n",
      "            step(): A.grad=300607.0000 A.data=6087961.0000\n",
      "Ep322: zero_grad(): A.grad= 0.0000 A.data=6087961.0000 loss=18531628351488.0000\n",
      "            step(): A.grad=6087960.0000 A.data=886925.5000\n",
      "Ep323: zero_grad(): A.grad= 0.0000 A.data=886925.5000 loss=393317548032.0000\n",
      "            step(): A.grad=886924.5000 A.data=-6519370.5000\n",
      "Ep324: zero_grad(): A.grad= 0.0000 A.data=-6519370.5000 loss=21251101818880.0000\n",
      "            step(): A.grad=-6519371.5000 A.data=-2279490.0000\n",
      "Ep325: zero_grad(): A.grad= 0.0000 A.data=-2279490.0000 loss=2598039715840.0000\n",
      "            step(): A.grad=-2279491.0000 A.data=6715412.0000\n",
      "Ep326: zero_grad(): A.grad= 0.0000 A.data=6715412.0000 loss=22548372783104.0000\n",
      "            step(): A.grad=6715411.0000 A.data=3850523.7500\n",
      "Ep327: zero_grad(): A.grad= 0.0000 A.data=3850523.7500 loss=7413262974976.0000\n",
      "            step(): A.grad=3850522.7500 A.data=-6616847.0000\n",
      "Ep328: zero_grad(): A.grad= 0.0000 A.data=-6616847.0000 loss=21891339255808.0000\n",
      "            step(): A.grad=-6616848.0000 A.data=-5558944.0000\n",
      "Ep329: zero_grad(): A.grad= 0.0000 A.data=-5558944.0000 loss=15450935132160.0000\n",
      "            step(): A.grad=-5558945.0000 A.data=6166744.0000\n",
      "Ep330: zero_grad(): A.grad= 0.0000 A.data=6166744.0000 loss=19014359187456.0000\n",
      "            step(): A.grad=6166743.0000 A.data=7348189.5000\n",
      "Ep331: zero_grad(): A.grad= 0.0000 A.data=7348189.5000 loss=26997937930240.0000\n",
      "            step(): A.grad=7348188.5000 A.data=-5313777.5000\n",
      "Ep332: zero_grad(): A.grad= 0.0000 A.data=-5313777.5000 loss=14118120587264.0000\n",
      "            step(): A.grad=-5313778.5000 A.data=-9145763.0000\n",
      "Ep333: zero_grad(): A.grad= 0.0000 A.data=-9145763.0000 loss=41822499897344.0000\n",
      "            step(): A.grad=-9145764.0000 A.data=4016004.0000\n",
      "Ep334: zero_grad(): A.grad= 0.0000 A.data=4016004.0000 loss=8064139788288.0000\n",
      "            step(): A.grad=4016003.0000 A.data=10863543.0000\n",
      "Ep335: zero_grad(): A.grad= 0.0000 A.data=10863543.0000 loss=59008270467072.0000\n",
      "            step(): A.grad=10863542.0000 A.data=-2244893.0000\n",
      "Ep336: zero_grad(): A.grad= 0.0000 A.data=-2244893.0000 loss=2519774527488.0000\n",
      "            step(): A.grad=-2244894.0000 A.data=-12398875.0000\n",
      "Ep337: zero_grad(): A.grad= 0.0000 A.data=-12398875.0000 loss=76866065006592.0000\n",
      "            step(): A.grad=-12398876.0000 A.data=-10391.0000\n",
      "Ep338: zero_grad(): A.grad= 0.0000 A.data=-10391.0000 loss=53996832.0000\n",
      "            step(): A.grad=-10392.0000 A.data=13636687.0000\n",
      "Ep339: zero_grad(): A.grad= 0.0000 A.data=13636687.0000 loss=92979599507456.0000\n",
      "            step(): A.grad=13636686.0000 A.data=2738770.0000\n",
      "Ep340: zero_grad(): A.grad= 0.0000 A.data=2738770.0000 loss=3750427885568.0000\n",
      "            step(): A.grad=2738769.0000 A.data=-14452600.0000\n",
      "Ep341: zero_grad(): A.grad= 0.0000 A.data=-14452600.0000 loss=104438840688640.0000\n",
      "            step(): A.grad=-14452601.0000 A.data=-5903166.0000\n",
      "Ep342: zero_grad(): A.grad= 0.0000 A.data=-5903166.0000 loss=17423690366976.0000\n",
      "            step(): A.grad=-5903167.0000 A.data=14717230.0000\n",
      "Ep343: zero_grad(): A.grad= 0.0000 A.data=14717230.0000 loss=108298414063616.0000\n",
      "            step(): A.grad=14717229.0000 A.data=9436930.0000\n",
      "Ep344: zero_grad(): A.grad= 0.0000 A.data=9436930.0000 loss=44527813394432.0000\n",
      "            step(): A.grad=9436929.0000 A.data=-14301564.0000\n",
      "Ep345: zero_grad(): A.grad= 0.0000 A.data=-14301564.0000 loss=102267382398976.0000\n",
      "            step(): A.grad=-14301565.0000 A.data=-13240935.0000\n",
      "Ep346: zero_grad(): A.grad= 0.0000 A.data=-13240935.0000 loss=87661196869632.0000\n",
      "            step(): A.grad=-13240936.0000 A.data=13083537.0000\n",
      "Ep347: zero_grad(): A.grad= 0.0000 A.data=13083537.0000 loss=85589453963264.0000\n",
      "            step(): A.grad=13083536.0000 A.data=17181738.0000\n",
      "Ep348: zero_grad(): A.grad= 0.0000 A.data=17181738.0000 loss=147606030254080.0000\n",
      "            step(): A.grad=17181736.0000 A.data=-10955538.0000\n",
      "Ep349: zero_grad(): A.grad= 0.0000 A.data=-10955538.0000 loss=60011917082624.0000\n",
      "            step(): A.grad=-10955539.0000 A.data=-21091020.0000\n",
      "Ep350: zero_grad(): A.grad= 0.0000 A.data=-21091020.0000 loss=222415569289216.0000\n",
      "            step(): A.grad=-21091020.0000 A.data=7832888.0000\n",
      "Ep351: zero_grad(): A.grad= 0.0000 A.data=7832888.0000 loss=30677059764224.0000\n",
      "            step(): A.grad=7832887.0000 A.data=24766702.0000\n",
      "Ep352: zero_grad(): A.grad= 0.0000 A.data=24766702.0000 loss=306694722682880.0000\n",
      "            step(): A.grad=24766700.0000 A.data=-3662830.0000\n",
      "Ep353: zero_grad(): A.grad= 0.0000 A.data=-3662830.0000 loss=6708165345280.0000\n",
      "            step(): A.grad=-3662831.0000 A.data=-27975938.0000\n",
      "Ep354: zero_grad(): A.grad= 0.0000 A.data=-27975938.0000 loss=391326617042944.0000\n",
      "            step(): A.grad=-27975940.0000 A.data=-1566074.0000\n",
      "Ep355: zero_grad(): A.grad= 0.0000 A.data=-1566074.0000 loss=1226295476224.0000\n",
      "            step(): A.grad=-1566075.0000 A.data=30460318.0000\n",
      "Ep356: zero_grad(): A.grad= 0.0000 A.data=30460318.0000 loss=463915423105024.0000\n",
      "            step(): A.grad=30460316.0000 A.data=7814750.0000\n",
      "Ep357: zero_grad(): A.grad= 0.0000 A.data=7814750.0000 loss=30535151779840.0000\n",
      "            step(): A.grad=7814749.0000 A.data=-31943394.0000\n",
      "Ep358: zero_grad(): A.grad= 0.0000 A.data=-31943394.0000 loss=510190273167360.0000\n",
      "            step(): A.grad=-31943396.0000 A.data=-14984902.0000\n",
      "Ep359: zero_grad(): A.grad= 0.0000 A.data=-14984902.0000 loss=112273657954304.0000\n",
      "            step(): A.grad=-14984903.0000 A.data=32140750.0000\n",
      "Ep360: zero_grad(): A.grad= 0.0000 A.data=32140750.0000 loss=516513840758784.0000\n",
      "            step(): A.grad=32140748.0000 A.data=22911550.0000\n",
      "Ep361: zero_grad(): A.grad= 0.0000 A.data=22911550.0000 loss=262469511544832.0000\n",
      "            step(): A.grad=22911548.0000 A.data=-30772514.0000\n",
      "Ep362: zero_grad(): A.grad= 0.0000 A.data=-30772514.0000 loss=473473872822272.0000\n",
      "            step(): A.grad=-30772516.0000 A.data=-31357204.0000\n",
      "Ep363: zero_grad(): A.grad= 0.0000 A.data=-31357204.0000 loss=491637121744896.0000\n",
      "            step(): A.grad=-31357204.0000 A.data=27578320.0000\n",
      "Ep364: zero_grad(): A.grad= 0.0000 A.data=27578320.0000 loss=380281873760256.0000\n",
      "            step(): A.grad=27578320.0000 A.data=40008592.0000\n",
      "Ep365: zero_grad(): A.grad= 0.0000 A.data=40008592.0000 loss=800343734616064.0000\n",
      "            step(): A.grad=40008592.0000 A.data=-22334436.0000\n",
      "Ep366: zero_grad(): A.grad= 0.0000 A.data=-22334436.0000 loss=249413515608064.0000\n",
      "            step(): A.grad=-22334436.0000 A.data=-48476336.0000\n",
      "Ep367: zero_grad(): A.grad= 0.0000 A.data=-48476336.0000 loss=1174977558609920.0000\n",
      "            step(): A.grad=-48476336.0000 A.data=14872608.0000\n",
      "Ep368: zero_grad(): A.grad= 0.0000 A.data=14872608.0000 loss=110597219811328.0000\n",
      "            step(): A.grad=14872607.0000 A.data=56298488.0000\n",
      "Ep369: zero_grad(): A.grad= 0.0000 A.data=56298488.0000 loss=1584759851450368.0000\n",
      "            step(): A.grad=56298488.0000 A.data=-5100168.0000\n",
      "Ep370: zero_grad(): A.grad= 0.0000 A.data=-5100168.0000 loss=13005861421056.0000\n",
      "            step(): A.grad=-5100169.0000 A.data=-62948372.0000\n",
      "Ep371: zero_grad(): A.grad= 0.0000 A.data=-62948372.0000 loss=1981248817856512.0000\n",
      "            step(): A.grad=-62948372.0000 A.data=-6979496.0000\n",
      "Ep372: zero_grad(): A.grad= 0.0000 A.data=-6979496.0000 loss=24356688232448.0000\n",
      "            step(): A.grad=-6979497.0000 A.data=67847312.0000\n",
      "Ep373: zero_grad(): A.grad= 0.0000 A.data=67847312.0000 loss=2301628950511616.0000\n",
      "            step(): A.grad=67847312.0000 A.data=21246916.0000\n",
      "Ep374: zero_grad(): A.grad= 0.0000 A.data=21246916.0000 loss=225715714785280.0000\n",
      "            step(): A.grad=21246916.0000 A.data=-70382656.0000\n",
      "Ep375: zero_grad(): A.grad= 0.0000 A.data=-70382656.0000 loss=2476859052785664.0000\n",
      "            step(): A.grad=-70382656.0000 A.data=-37448148.0000\n",
      "Ep376: zero_grad(): A.grad= 0.0000 A.data=-37448148.0000 loss=701181865230336.0000\n",
      "            step(): A.grad=-37448148.0000 A.data=69931296.0000\n",
      "Ep377: zero_grad(): A.grad= 0.0000 A.data=69931296.0000 loss=2445193064218624.0000\n",
      "            step(): A.grad=69931296.0000 A.data=55179224.0000\n",
      "Ep378: zero_grad(): A.grad= 0.0000 A.data=55179224.0000 loss=1522373438210048.0000\n",
      "            step(): A.grad=55179224.0000 A.data=-65888576.0000\n",
      "Ep379: zero_grad(): A.grad= 0.0000 A.data=-65888576.0000 loss=2170652177989632.0000\n",
      "            step(): A.grad=-65888576.0000 A.data=-73874872.0000\n",
      "Ep380: zero_grad(): A.grad= 0.0000 A.data=-73874872.0000 loss=2728748415713280.0000\n",
      "            step(): A.grad=-73874872.0000 A.data=57702464.0000\n",
      "Ep381: zero_grad(): A.grad= 0.0000 A.data=57702464.0000 loss=1664787171770368.0000\n",
      "            step(): A.grad=57702464.0000 A.data=92802864.0000\n",
      "Ep382: zero_grad(): A.grad= 0.0000 A.data=92802864.0000 loss=4306185750577152.0000\n",
      "            step(): A.grad=92802864.0000 A.data=-44912128.0000\n",
      "Ep383: zero_grad(): A.grad= 0.0000 A.data=-44912128.0000 loss=1008549589155840.0000\n",
      "            step(): A.grad=-44912128.0000 A.data=-111065584.0000\n",
      "Ep384: zero_grad(): A.grad= 0.0000 A.data=-111065584.0000 loss=6167781879840768.0000\n",
      "            step(): A.grad=-111065584.0000 A.data=27190224.0000\n",
      "Ep385: zero_grad(): A.grad= 0.0000 A.data=27190224.0000 loss=369654144958464.0000\n",
      "            step(): A.grad=27190224.0000 A.data=127610192.0000\n",
      "Ep386: zero_grad(): A.grad= 0.0000 A.data=127610192.0000 loss=8142180493295616.0000\n",
      "            step(): A.grad=127610192.0000 A.data=-4387200.0000\n",
      "Ep387: zero_grad(): A.grad= 0.0000 A.data=-4387200.0000 loss=9623766564864.0000\n",
      "            step(): A.grad=-4387201.0000 A.data=-141248656.0000\n",
      "Ep388: zero_grad(): A.grad= 0.0000 A.data=-141248656.0000 loss=9975590899679232.0000\n",
      "            step(): A.grad=-141248656.0000 A.data=-23423824.0000\n",
      "Ep389: zero_grad(): A.grad= 0.0000 A.data=-23423824.0000 loss=274337764474880.0000\n",
      "            step(): A.grad=-23423824.0000 A.data=150688768.0000\n",
      "Ep390: zero_grad(): A.grad= 0.0000 A.data=150688768.0000 loss=11353552773447680.0000\n",
      "            step(): A.grad=150688768.0000 A.data=55903968.0000\n",
      "Ep391: zero_grad(): A.grad= 0.0000 A.data=55903968.0000 loss=1562626811232256.0000\n",
      "            step(): A.grad=55903968.0000 A.data=-154576848.0000\n",
      "Ep392: zero_grad(): A.grad= 0.0000 A.data=-154576848.0000 loss=11947001289637888.0000\n",
      "            step(): A.grad=-154576848.0000 A.data=-92409744.0000\n",
      "Ep393: zero_grad(): A.grad= 0.0000 A.data=-92409744.0000 loss=4269780265598976.0000\n",
      "            step(): A.grad=-92409744.0000 A.data=151552576.0000\n",
      "Ep394: zero_grad(): A.grad= 0.0000 A.data=151552576.0000 loss=11484091861958656.0000\n",
      "            step(): A.grad=151552576.0000 A.data=131961232.0000\n",
      "Ep395: zero_grad(): A.grad= 0.0000 A.data=131961232.0000 loss=8706883330244608.0000\n",
      "            step(): A.grad=131961232.0000 A.data=-140315568.0000\n",
      "Ep396: zero_grad(): A.grad= 0.0000 A.data=-140315568.0000 loss=9844229324931072.0000\n",
      "            step(): A.grad=-140315568.0000 A.data=-173220496.0000\n",
      "Ep397: zero_grad(): A.grad= 0.0000 A.data=-173220496.0000 loss=15002669731020800.0000\n",
      "            step(): A.grad=-173220496.0000 A.data=119703024.0000\n",
      "Ep398: zero_grad(): A.grad= 0.0000 A.data=119703024.0000 loss=7164407029170176.0000\n",
      "            step(): A.grad=119703024.0000 A.data=214483136.0000\n",
      "Ep399: zero_grad(): A.grad= 0.0000 A.data=214483136.0000 loss=23001508375166976.0000\n",
      "            step(): A.grad=214483136.0000 A.data=-88776672.0000\n",
      "Ep400: zero_grad(): A.grad= 0.0000 A.data=-88776672.0000 loss=3940648868642816.0000\n",
      "            step(): A.grad=-88776672.0000 A.data=-253686784.0000\n",
      "Ep401: zero_grad(): A.grad= 0.0000 A.data=-253686784.0000 loss=32178491982086144.0000\n",
      "            step(): A.grad=-253686784.0000 A.data=46916992.0000\n",
      "Ep402: zero_grad(): A.grad= 0.0000 A.data=46916992.0000 loss=1100602079707136.0000\n",
      "            step(): A.grad=46916992.0000 A.data=288438848.0000\n",
      "Ep403: zero_grad(): A.grad= 0.0000 A.data=288438848.0000 loss=41598484838612992.0000\n",
      "            step(): A.grad=288438848.0000 A.data=6079104.0000\n",
      "Ep404: zero_grad(): A.grad= 0.0000 A.data=6079104.0000 loss=18477746225152.0000\n",
      "            step(): A.grad=6079103.0000 A.data=-316066912.0000\n",
      "Ep405: zero_grad(): A.grad= 0.0000 A.data=-316066912.0000 loss=49949146802552832.0000\n",
      "            step(): A.grad=-316066912.0000 A.data=-69900416.0000\n",
      "Ep406: zero_grad(): A.grad= 0.0000 A.data=-69900416.0000 loss=2443034037846016.0000\n",
      "            step(): A.grad=-69900416.0000 A.data=333693504.0000\n",
      "Ep407: zero_grad(): A.grad= 0.0000 A.data=333693504.0000 loss=55675678237917184.0000\n",
      "            step(): A.grad=333693504.0000 A.data=143629184.0000\n",
      "Ep408: zero_grad(): A.grad= 0.0000 A.data=143629184.0000 loss=10314671051505664.0000\n",
      "            step(): A.grad=143629184.0000 A.data=-338337024.0000\n",
      "Ep409: zero_grad(): A.grad= 0.0000 A.data=-338337024.0000 loss=57235971137077248.0000\n",
      "            step(): A.grad=-338337024.0000 A.data=-225659552.0000\n",
      "Ep410: zero_grad(): A.grad= 0.0000 A.data=-225659552.0000 loss=25461115886501888.0000\n",
      "            step(): A.grad=-225659552.0000 A.data=327038816.0000\n",
      "Ep411: zero_grad(): A.grad= 0.0000 A.data=327038816.0000 loss=53477191853277184.0000\n",
      "            step(): A.grad=327038816.0000 A.data=313633344.0000\n",
      "Ep412: zero_grad(): A.grad= 0.0000 A.data=313633344.0000 loss=49182937521848320.0000\n",
      "            step(): A.grad=313633344.0000 A.data=-297016064.0000\n",
      "Ep413: zero_grad(): A.grad= 0.0000 A.data=-297016064.0000 loss=44109271180247040.0000\n",
      "            step(): A.grad=-297016064.0000 A.data=-404399936.0000\n",
      "Ep414: zero_grad(): A.grad= 0.0000 A.data=-404399936.0000 loss=81769657555484672.0000\n",
      "            step(): A.grad=-404399936.0000 A.data=245837696.0000\n",
      "Ep415: zero_grad(): A.grad= 0.0000 A.data=245837696.0000 loss=30218086372081664.0000\n",
      "            step(): A.grad=245837696.0000 A.data=494007488.0000\n",
      "Ep416: zero_grad(): A.grad= 0.0000 A.data=494007488.0000 loss=122021695916605440.0000\n",
      "            step(): A.grad=494007488.0000 A.data=-171619968.0000\n",
      "Ep417: zero_grad(): A.grad= 0.0000 A.data=-171619968.0000 loss=14726706271092736.0000\n",
      "            step(): A.grad=-171619968.0000 A.data=-577732224.0000\n",
      "Ep418: zero_grad(): A.grad= 0.0000 A.data=-577732224.0000 loss=166887259298070528.0000\n",
      "            step(): A.grad=-577732224.0000 A.data=73235520.0000\n",
      "Ep419: zero_grad(): A.grad= 0.0000 A.data=73235520.0000 loss=2681720671305728.0000\n",
      "            step(): A.grad=73235520.0000 A.data=650152576.0000\n",
      "Ep420: zero_grad(): A.grad= 0.0000 A.data=650152576.0000 loss=211349190242992128.0000\n",
      "            step(): A.grad=650152576.0000 A.data=49471424.0000\n",
      "Ep421: zero_grad(): A.grad= 0.0000 A.data=49471424.0000 loss=1223710941904896.0000\n",
      "            step(): A.grad=49471424.0000 A.data=-705273536.0000\n",
      "Ep422: zero_grad(): A.grad= 0.0000 A.data=-705273536.0000 loss=248705372674588672.0000\n",
      "            step(): A.grad=-705273536.0000 A.data=-195473312.0000\n",
      "Ep423: zero_grad(): A.grad= 0.0000 A.data=-195473312.0000 loss=19104907885805568.0000\n",
      "            step(): A.grad=-195473312.0000 A.data=736706176.0000\n",
      "Ep424: zero_grad(): A.grad= 0.0000 A.data=736706176.0000 loss=271367990950232064.0000\n",
      "            step(): A.grad=736706176.0000 A.data=362361920.0000\n",
      "Ep425: zero_grad(): A.grad= 0.0000 A.data=362361920.0000 loss=65653080540053504.0000\n",
      "            step(): A.grad=362361920.0000 A.data=-737904448.0000\n",
      "Ep426: zero_grad(): A.grad= 0.0000 A.data=-737904448.0000 loss=272251482902888448.0000\n",
      "            step(): A.grad=-737904448.0000 A.data=-546179072.0000\n",
      "Ep427: zero_grad(): A.grad= 0.0000 A.data=-546179072.0000 loss=149155796054179840.0000\n",
      "            step(): A.grad=-546179072.0000 A.data=702459136.0000\n",
      "Ep428: zero_grad(): A.grad= 0.0000 A.data=702459136.0000 loss=246724413498589184.0000\n",
      "            step(): A.grad=702459136.0000 A.data=741288832.0000\n",
      "Ep429: zero_grad(): A.grad= 0.0000 A.data=741288832.0000 loss=274754572663128064.0000\n",
      "            step(): A.grad=741288832.0000 A.data=-624447360.0000\n",
      "Ep430: zero_grad(): A.grad= 0.0000 A.data=-624447360.0000 loss=194967257263112192.0000\n",
      "            step(): A.grad=-624447360.0000 A.data=-940307200.0000\n",
      "Ep431: zero_grad(): A.grad= 0.0000 A.data=-940307200.0000 loss=442088817797627904.0000\n",
      "            step(): A.grad=-940307200.0000 A.data=498830592.0000\n",
      "Ep432: zero_grad(): A.grad= 0.0000 A.data=498830592.0000 loss=124415976975368192.0000\n",
      "            step(): A.grad=498830592.0000 A.data=1134104064.0000\n",
      "Ep433: zero_grad(): A.grad= 0.0000 A.data=1134104064.0000 loss=643096036029497344.0000\n",
      "            step(): A.grad=1134104064.0000 A.data=-321892736.0000\n",
      "Ep434: zero_grad(): A.grad= 0.0000 A.data=-321892736.0000 loss=51807467482382336.0000\n",
      "            step(): A.grad=-321892736.0000 A.data=-1311892992.0000\n",
      "Ep435: zero_grad(): A.grad= 0.0000 A.data=-1311892992.0000 loss=860531607247781888.0000\n",
      "            step(): A.grad=-1311892992.0000 A.data=91703296.0000\n",
      "Ep436: zero_grad(): A.grad= 0.0000 A.data=91703296.0000 loss=4204747212980224.0000\n",
      "            step(): A.grad=91703296.0000 A.data=1461423104.0000\n",
      "Ep437: zero_grad(): A.grad= 0.0000 A.data=1461423104.0000 loss=1067878746964361216.0000\n",
      "            step(): A.grad=1461423104.0000 A.data=191411072.0000\n",
      "Ep438: zero_grad(): A.grad= 0.0000 A.data=191411072.0000 loss=18319098521845760.0000\n",
      "            step(): A.grad=191411072.0000 A.data=-1569283200.0000\n",
      "Ep439: zero_grad(): A.grad= 0.0000 A.data=-1569283200.0000 loss=1231324930004484096.0000\n",
      "            step(): A.grad=-1569283200.0000 A.data=-524408960.0000\n",
      "Ep440: zero_grad(): A.grad= 0.0000 A.data=-524408960.0000 loss=137502381549027328.0000\n",
      "            step(): A.grad=-524408960.0000 A.data=1621329792.0000\n",
      "Ep441: zero_grad(): A.grad= 0.0000 A.data=1621329792.0000 loss=1314355100331802624.0000\n",
      "            step(): A.grad=1621329792.0000 A.data=901115840.0000\n",
      "Ep442: zero_grad(): A.grad= 0.0000 A.data=901115840.0000 loss=406004873398583296.0000\n",
      "            step(): A.grad=901115840.0000 A.data=-1603239424.0000\n",
      "Ep443: zero_grad(): A.grad= 0.0000 A.data=-1603239424.0000 loss=1285188355381788672.0000\n",
      "            step(): A.grad=-1603239424.0000 A.data=-1311875584.0000\n",
      "Ep444: zero_grad(): A.grad= 0.0000 A.data=-1311875584.0000 loss=860508792381505536.0000\n",
      "            step(): A.grad=-1311875584.0000 A.data=1501188096.0000\n",
      "Ep445: zero_grad(): A.grad= 0.0000 A.data=1501188096.0000 loss=1126782883399204864.0000\n",
      "            step(): A.grad=1501188096.0000 A.data=1743300992.0000\n",
      "Ep446: zero_grad(): A.grad= 0.0000 A.data=1743300992.0000 loss=1519549121403289600.0000\n",
      "            step(): A.grad=1743300992.0000 A.data=-1302646656.0000\n",
      "Ep447: zero_grad(): A.grad= 0.0000 A.data=-1302646656.0000 loss=848444126167826432.0000\n",
      "            step(): A.grad=-1302646656.0000 A.data=-2178160640.0000\n",
      "Ep448: zero_grad(): A.grad= 0.0000 A.data=-2178160640.0000 loss=2372191938880208896.0000\n",
      "            step(): A.grad=-2178160640.0000 A.data=997279232.0000\n",
      "Ep449: zero_grad(): A.grad= 0.0000 A.data=997279232.0000 loss=497282927122448384.0000\n",
      "            step(): A.grad=997279232.0000 A.data=2595432704.0000\n",
      "Ep450: zero_grad(): A.grad= 0.0000 A.data=2595432704.0000 loss=3368135343755755520.0000\n",
      "            step(): A.grad=2595432704.0000 A.data=-577920512.0000\n",
      "Ep451: zero_grad(): A.grad= 0.0000 A.data=-577920512.0000 loss=166996059409612800.0000\n",
      "            step(): A.grad=-577920512.0000 A.data=-2970560256.0000\n",
      "Ep452: zero_grad(): A.grad= 0.0000 A.data=-2970560256.0000 loss=4412114212625580032.0000\n",
      "            step(): A.grad=-2970560256.0000 A.data=41600256.0000\n",
      "Ep453: zero_grad(): A.grad= 0.0000 A.data=41600256.0000 loss=865290619453440.0000\n",
      "            step(): A.grad=41600256.0000 A.data=3275936512.0000\n",
      "Ep454: zero_grad(): A.grad= 0.0000 A.data=3275936512.0000 loss=5365880076581732352.0000\n",
      "            step(): A.grad=3275936512.0000 A.data=609426944.0000\n",
      "Ep455: zero_grad(): A.grad= 0.0000 A.data=609426944.0000 loss=185700607623954432.0000\n",
      "            step(): A.grad=609426944.0000 A.data=-3481644544.0000\n",
      "Ep456: zero_grad(): A.grad= 0.0000 A.data=-3481644544.0000 loss=6060924257452425216.0000\n",
      "            step(): A.grad=-3481644544.0000 A.data=-1366698624.0000\n",
      "Ep457: zero_grad(): A.grad= 0.0000 A.data=-1366698624.0000 loss=933932598336421888.0000\n",
      "            step(): A.grad=-1366698624.0000 A.data=3556469760.0000\n",
      "Ep458: zero_grad(): A.grad= 0.0000 A.data=3556469760.0000 loss=6324238600607105024.0000\n",
      "            step(): A.grad=3556469760.0000 A.data=2214662400.0000\n",
      "Ep459: zero_grad(): A.grad= 0.0000 A.data=2214662400.0000 loss=2452364753364844544.0000\n",
      "            step(): A.grad=2214662400.0000 A.data=-3469184256.0000\n",
      "Ep460: zero_grad(): A.grad= 0.0000 A.data=-3469184256.0000 loss=6017619442236653568.0000\n",
      "            step(): A.grad=-3469184256.0000 A.data=-3129965568.0000\n",
      "Ep461: zero_grad(): A.grad= 0.0000 A.data=-3129965568.0000 loss=4898342443683938304.0000\n",
      "            step(): A.grad=-3129965568.0000 A.data=3190109696.0000\n",
      "Ep462: zero_grad(): A.grad= 0.0000 A.data=3190109696.0000 loss=5088400175370600448.0000\n",
      "            step(): A.grad=3190109696.0000 A.data=4080984320.0000\n",
      "Ep463: zero_grad(): A.grad= 0.0000 A.data=4080984320.0000 loss=8327216733903388672.0000\n",
      "            step(): A.grad=4080984320.0000 A.data=-2692923648.0000\n",
      "Ep464: zero_grad(): A.grad= 0.0000 A.data=-2692923648.0000 loss=3625918868544815104.0000\n",
      "            step(): A.grad=-2692923648.0000 A.data=-5027667456.0000\n",
      "Ep465: zero_grad(): A.grad= 0.0000 A.data=-5027667456.0000 loss=12638720135029325824.0000\n",
      "            step(): A.grad=-5027667456.0000 A.data=1956682240.0000\n",
      "Ep466: zero_grad(): A.grad= 0.0000 A.data=1956682240.0000 loss=1914302657955102720.0000\n",
      "            step(): A.grad=1956682240.0000 A.data=5921771520.0000\n",
      "Ep467: zero_grad(): A.grad= 0.0000 A.data=5921771520.0000 loss=17533688727283433472.0000\n",
      "            step(): A.grad=5921771520.0000 A.data=-967996416.0000\n",
      "Ep468: zero_grad(): A.grad= 0.0000 A.data=-967996416.0000 loss=468508536024858624.0000\n",
      "            step(): A.grad=-967996416.0000 A.data=-6707548160.0000\n",
      "Ep469: zero_grad(): A.grad= 0.0000 A.data=-6707548160.0000 loss=22495601084994682880.0000\n",
      "            step(): A.grad=-6707548160.0000 A.data=-276713984.0000\n",
      "Ep470: zero_grad(): A.grad= 0.0000 A.data=-276713984.0000 loss=38285312706740224.0000\n",
      "            step(): A.grad=-276713984.0000 A.data=7322960384.0000\n",
      "Ep471: zero_grad(): A.grad= 0.0000 A.data=7322960384.0000 loss=26812875272899002368.0000\n",
      "            step(): A.grad=7322960384.0000 A.data=1768977920.0000\n",
      "Ep472: zero_grad(): A.grad= 0.0000 A.data=1768977920.0000 loss=1564641467647918080.0000\n",
      "            step(): A.grad=1768977920.0000 A.data=-7701461504.0000\n",
      "Ep473: zero_grad(): A.grad= 0.0000 A.data=-7701461504.0000 loss=29656254123769593856.0000\n",
      "            step(): A.grad=-7701461504.0000 A.data=-3486168320.0000\n",
      "Ep474: zero_grad(): A.grad= 0.0000 A.data=-3486168320.0000 loss=6076684657124966400.0000\n",
      "            step(): A.grad=-3486168320.0000 A.data=7774373888.0000\n",
      "Ep475: zero_grad(): A.grad= 0.0000 A.data=7774373888.0000 loss=30220444326307037184.0000\n",
      "            step(): A.grad=7774373888.0000 A.data=5389660672.0000\n",
      "Ep476: zero_grad(): A.grad= 0.0000 A.data=5389660672.0000 loss=14524220948455882752.0000\n",
      "            step(): A.grad=5389660672.0000 A.data=-7473878528.0000\n",
      "Ep477: zero_grad(): A.grad= 0.0000 A.data=-7473878528.0000 loss=27929429330905530368.0000\n",
      "            step(): A.grad=-7473878528.0000 A.data=-7423403008.0000\n",
      "Ep478: zero_grad(): A.grad= 0.0000 A.data=-7423403008.0000 loss=27553455727834038272.0000\n",
      "            step(): A.grad=-7423403008.0000 A.data=6736585728.0000\n",
      "Ep479: zero_grad(): A.grad= 0.0000 A.data=6736585728.0000 loss=22690792986227245056.0000\n",
      "            step(): A.grad=6736585728.0000 A.data=9513060352.0000\n",
      "Ep480: zero_grad(): A.grad= 0.0000 A.data=9513060352.0000 loss=45249160792005148672.0000\n",
      "            step(): A.grad=9513060352.0000 A.data=-5507631104.0000\n",
      "Ep481: zero_grad(): A.grad= 0.0000 A.data=-5507631104.0000 loss=15166999844100243456.0000\n",
      "            step(): A.grad=-5507631104.0000 A.data=-11565892608.0000\n",
      "Ep482: zero_grad(): A.grad= 0.0000 A.data=-11565892608.0000 loss=66884937187009232896.0000\n",
      "            step(): A.grad=-11565892608.0000 A.data=3745214464.0000\n",
      "Ep483: zero_grad(): A.grad= 0.0000 A.data=3745214464.0000 loss=7013315731873857536.0000\n",
      "            step(): A.grad=3745214464.0000 A.data=13471525888.0000\n",
      "Ep484: zero_grad(): A.grad= 0.0000 A.data=13471525888.0000 loss=90741005790493016064.0000\n",
      "            step(): A.grad=13471525888.0000 A.data=-1425430528.0000\n",
      "Ep485: zero_grad(): A.grad= 0.0000 A.data=-1425430528.0000 loss=1015926066637701120.0000\n",
      "            step(): A.grad=-1425430528.0000 A.data=-15103764480.0000\n",
      "Ep486: zero_grad(): A.grad= 0.0000 A.data=-15103764480.0000 loss=114061849725761486848.0000\n",
      "            step(): A.grad=-15103764480.0000 A.data=-1452780544.0000\n",
      "Ep487: zero_grad(): A.grad= 0.0000 A.data=-1452780544.0000 loss=1055285627974582272.0000\n",
      "            step(): A.grad=-1452780544.0000 A.data=16323587072.0000\n",
      "Ep488: zero_grad(): A.grad= 0.0000 A.data=16323587072.0000 loss=133229750281989783552.0000\n",
      "            step(): A.grad=16323587072.0000 A.data=4862775296.0000\n",
      "Ep489: zero_grad(): A.grad= 0.0000 A.data=4862775296.0000 loss=11823291525545066496.0000\n",
      "            step(): A.grad=4862775296.0000 A.data=-16983390208.0000\n",
      "Ep490: zero_grad(): A.grad= 0.0000 A.data=-16983390208.0000 loss=144217768112680861696.0000\n",
      "            step(): A.grad=-16983390208.0000 A.data=-8745732096.0000\n",
      "Ep491: zero_grad(): A.grad= 0.0000 A.data=-8745732096.0000 loss=38243917124746608640.0000\n",
      "            step(): A.grad=-8745732096.0000 A.data=16932583424.0000\n",
      "Ep492: zero_grad(): A.grad= 0.0000 A.data=16932583424.0000 loss=143356190801155588096.0000\n",
      "            step(): A.grad=16932583424.0000 A.data=13006823424.0000\n",
      "Ep493: zero_grad(): A.grad= 0.0000 A.data=13006823424.0000 loss=84588728059691008000.0000\n",
      "            step(): A.grad=13006823424.0000 A.data=-16024475648.0000\n",
      "Ep494: zero_grad(): A.grad= 0.0000 A.data=-16024475648.0000 loss=128391907915868405760.0000\n",
      "            step(): A.grad=-16024475648.0000 A.data=-17512402944.0000\n",
      "Ep495: zero_grad(): A.grad= 0.0000 A.data=-17512402944.0000 loss=153342131326477664256.0000\n",
      "            step(): A.grad=-17512402944.0000 A.data=14124441600.0000\n",
      "Ep496: zero_grad(): A.grad= 0.0000 A.data=14124441600.0000 loss=99749929079466360832.0000\n",
      "            step(): A.grad=14124441600.0000 A.data=22088534016.0000\n",
      "Ep497: zero_grad(): A.grad= 0.0000 A.data=22088534016.0000 loss=243951670710660759552.0000\n",
      "            step(): A.grad=22088534016.0000 A.data=-11119179776.0000\n",
      "Ep498: zero_grad(): A.grad= 0.0000 A.data=-11119179776.0000 loss=61818079742961647616.0000\n",
      "            step(): A.grad=-11119179776.0000 A.data=-26521227264.0000\n",
      "Ep499: zero_grad(): A.grad= 0.0000 A.data=-26521227264.0000 loss=351687748566850207744.0000\n",
      "            step(): A.grad=-26521227264.0000 A.data=6926850048.0000\n",
      "Ep500: zero_grad(): A.grad= 0.0000 A.data=6926850048.0000 loss=23990626836491010048.0000\n",
      "            step(): A.grad=6926850048.0000 A.data=30558722048.0000\n",
      "Ep501: zero_grad(): A.grad= 0.0000 A.data=30558722048.0000 loss=466917763426426028032.0000\n",
      "            step(): A.grad=30558722048.0000 A.data=-1507788800.0000\n",
      "Ep502: zero_grad(): A.grad= 0.0000 A.data=-1507788800.0000 loss=1136713534982324224.0000\n",
      "            step(): A.grad=-1507788800.0000 A.data=-33916153856.0000\n",
      "Ep503: zero_grad(): A.grad= 0.0000 A.data=-33916153856.0000 loss=575152738086649069568.0000\n",
      "            step(): A.grad=-33916153856.0000 A.data=-5124663296.0000\n",
      "Ep504: zero_grad(): A.grad= 0.0000 A.data=-5124663296.0000 loss=13131086939505557504.0000\n",
      "            step(): A.grad=-5124663296.0000 A.data=36282834944.0000\n",
      "Ep505: zero_grad(): A.grad= 0.0000 A.data=36282834944.0000 loss=658222055425962934272.0000\n",
      "            step(): A.grad=36282834944.0000 A.data=12893700096.0000\n",
      "Ep506: zero_grad(): A.grad= 0.0000 A.data=12893700096.0000 loss=83123747562935287808.0000\n",
      "            step(): A.grad=12893700096.0000 A.data=-37332377600.0000\n",
      "Ep507: zero_grad(): A.grad= 0.0000 A.data=-37332377600.0000 loss=696853229342105272320.0000\n",
      "            step(): A.grad=-37332377600.0000 A.data=-21649551360.0000\n",
      "Ep508: zero_grad(): A.grad= 0.0000 A.data=-21649551360.0000 loss=234351544417478770688.0000\n",
      "            step(): A.grad=-21649551360.0000 A.data=36735709184.0000\n",
      "Ep509: zero_grad(): A.grad= 0.0000 A.data=36735709184.0000 loss=674756177032923578368.0000\n",
      "            step(): A.grad=36735709184.0000 A.data=31161651200.0000\n",
      "Ep510: zero_grad(): A.grad= 0.0000 A.data=31161651200.0000 loss=485524244549418876928.0000\n",
      "            step(): A.grad=31161651200.0000 A.data=-34176950272.0000\n",
      "Ep511: zero_grad(): A.grad= 0.0000 A.data=-34176950272.0000 loss=584031971780102979584.0000\n",
      "            step(): A.grad=-34176950272.0000 A.data=-41113206784.0000\n",
      "Ep512: zero_grad(): A.grad= 0.0000 A.data=-41113206784.0000 loss=845147898609649319936.0000\n",
      "            step(): A.grad=-41113206784.0000 A.data=29372006400.0000\n",
      "Ep513: zero_grad(): A.grad= 0.0000 A.data=29372006400.0000 loss=431357375953080680448.0000\n",
      "            step(): A.grad=29372006400.0000 A.data=51098927104.0000\n",
      "Ep514: zero_grad(): A.grad= 0.0000 A.data=51098927104.0000 loss=1305550184828293349376.0000\n",
      "            step(): A.grad=51098927104.0000 A.data=-22089424896.0000\n",
      "Ep515: zero_grad(): A.grad= 0.0000 A.data=-22089424896.0000 loss=243971338774658416640.0000\n",
      "            step(): A.grad=-22089424896.0000 A.data=-60626702336.0000\n",
      "Ep516: zero_grad(): A.grad= 0.0000 A.data=-60626702336.0000 loss=1837798536639867256832.0000\n",
      "            step(): A.grad=-60626702336.0000 A.data=12173029376.0000\n",
      "Ep517: zero_grad(): A.grad= 0.0000 A.data=12173029376.0000 loss=74091321113406603264.0000\n",
      "            step(): A.grad=12173029376.0000 A.data=69123981312.0000\n",
      "Ep518: zero_grad(): A.grad= 0.0000 A.data=69123981312.0000 loss=2389062493453082951680.0000\n",
      "            step(): A.grad=69123981312.0000 A.data=434466816.0000\n",
      "Ep519: zero_grad(): A.grad= 0.0000 A.data=434466816.0000 loss=94380703738757120.0000\n",
      "            step(): A.grad=434466816.0000 A.data=-75949490176.0000\n",
      "Ep520: zero_grad(): A.grad= 0.0000 A.data=-75949490176.0000 loss=2884162622263314612224.0000\n",
      "            step(): A.grad=-75949490176.0000 A.data=-15667814400.0000\n",
      "Ep521: zero_grad(): A.grad= 0.0000 A.data=-15667814400.0000 loss=122740207042867232768.0000\n",
      "            step(): A.grad=-15667814400.0000 A.data=80410877952.0000\n",
      "Ep522: zero_grad(): A.grad= 0.0000 A.data=80410877952.0000 loss=3232954591454137679872.0000\n",
      "            step(): A.grad=80410877952.0000 A.data=33316773888.0000\n",
      "Ep523: zero_grad(): A.grad= 0.0000 A.data=33316773888.0000 loss=555003703722537648128.0000\n",
      "            step(): A.grad=33316773888.0000 A.data=-81788608512.0000\n",
      "Ep524: zero_grad(): A.grad= 0.0000 A.data=-81788608512.0000 loss=3344688335259246264320.0000\n",
      "            step(): A.grad=-81788608512.0000 A.data=-53006180352.0000\n",
      "Ep525: zero_grad(): A.grad= 0.0000 A.data=-53006180352.0000 loss=1404827535014048563200.0000\n",
      "            step(): A.grad=-53006180352.0000 A.data=79366234112.0000\n",
      "Ep526: zero_grad(): A.grad= 0.0000 A.data=79366234112.0000 loss=3149499512659241861120.0000\n",
      "            step(): A.grad=79366234112.0000 A.data=74180042752.0000\n",
      "Ep527: zero_grad(): A.grad= 0.0000 A.data=74180042752.0000 loss=2751339365828160126976.0000\n",
      "            step(): A.grad=74180042752.0000 A.data=-72466849792.0000\n",
      "Ep528: zero_grad(): A.grad= 0.0000 A.data=-72466849792.0000 loss=2625722150271775145984.0000\n",
      "            step(): A.grad=-72466849792.0000 A.data=-96091414528.0000\n",
      "Ep529: zero_grad(): A.grad= 0.0000 A.data=-96091414528.0000 loss=4616779871080920645632.0000\n",
      "            step(): A.grad=-96091414528.0000 A.data=60495257600.0000\n",
      "Ep530: zero_grad(): A.grad= 0.0000 A.data=60495257600.0000 loss=1829838142823513194496.0000\n",
      "            step(): A.grad=60495257600.0000 A.data=117799600128.0000\n",
      "Ep531: zero_grad(): A.grad= 0.0000 A.data=117799600128.0000 loss=6938372812616459354112.0000\n",
      "            step(): A.grad=117799600128.0000 A.data=-42984849408.0000\n",
      "Ep532: zero_grad(): A.grad= 0.0000 A.data=-42984849408.0000 loss=923848653941669625856.0000\n",
      "            step(): A.grad=-42984849408.0000 A.data=-138176528384.0000\n",
      "Ep533: zero_grad(): A.grad= 0.0000 A.data=-138176528384.0000 loss=9546376957529228836864.0000\n",
      "            step(): A.grad=-138176528384.0000 A.data=19648020480.0000\n",
      "Ep534: zero_grad(): A.grad= 0.0000 A.data=19648020480.0000 loss=193022362480820617216.0000\n",
      "            step(): A.grad=19648020480.0000 A.data=155923791872.0000\n",
      "Ep535: zero_grad(): A.grad= 0.0000 A.data=155923791872.0000 loss=12156114988298535960576.0000\n",
      "            step(): A.grad=155923791872.0000 A.data=9571926016.0000\n",
      "Ep536: zero_grad(): A.grad= 0.0000 A.data=9571926016.0000 loss=45810883690449862656.0000\n",
      "            step(): A.grad=9571926016.0000 A.data=-169601777664.0000\n",
      "Ep537: zero_grad(): A.grad= 0.0000 A.data=-169601777664.0000 loss=14382381643896406933504.0000\n",
      "            step(): A.grad=-169601777664.0000 A.data=-44449488896.0000\n",
      "Ep538: zero_grad(): A.grad= 0.0000 A.data=-44449488896.0000 loss=987878511275065475072.0000\n",
      "            step(): A.grad=-44449488896.0000 A.data=177672060928.0000\n",
      "Ep539: zero_grad(): A.grad= 0.0000 A.data=177672060928.0000 loss=15783681171552364134400.0000\n",
      "            step(): A.grad=177672060928.0000 A.data=84428873728.0000\n",
      "Ep540: zero_grad(): A.grad= 0.0000 A.data=84428873728.0000 loss=3564117219903618940928.0000\n",
      "            step(): A.grad=84428873728.0000 A.data=-178553487360.0000\n",
      "Ep541: zero_grad(): A.grad= 0.0000 A.data=-178553487360.0000 loss=15940674402762685939712.0000\n",
      "            step(): A.grad=-178553487360.0000 A.data=-128582483968.0000\n",
      "Ep542: zero_grad(): A.grad= 0.0000 A.data=-128582483968.0000 loss=8266727537107896631296.0000\n",
      "            step(): A.grad=-128582483968.0000 A.data=170692345856.0000\n",
      "Ep543: zero_grad(): A.grad= 0.0000 A.data=170692345856.0000 loss=14567938955743326109696.0000\n",
      "            step(): A.grad=170692345856.0000 A.data=175579234304.0000\n",
      "Ep544: zero_grad(): A.grad= 0.0000 A.data=175579234304.0000 loss=15414033595437141721088.0000\n",
      "            step(): A.grad=175579234304.0000 A.data=-152645730304.0000\n",
      "Ep545: zero_grad(): A.grad= 0.0000 A.data=-152645730304.0000 loss=11650359624244922417152.0000\n",
      "            step(): A.grad=-152645730304.0000 A.data=-223666290688.0000\n",
      "Ep546: zero_grad(): A.grad= 0.0000 A.data=-223666290688.0000 loss=25013305330589837033472.0000\n",
      "            step(): A.grad=-223666290688.0000 A.data=123177025536.0000\n",
      "Ep547: zero_grad(): A.grad= 0.0000 A.data=123177025536.0000 loss=7586289928407556816896.0000\n",
      "            step(): A.grad=123177025536.0000 A.data=270668349440.0000\n",
      "Ep548: zero_grad(): A.grad= 0.0000 A.data=270668349440.0000 loss=36630678339364084252672.0000\n",
      "            step(): A.grad=270668349440.0000 A.data=-81361027072.0000\n",
      "Ep549: zero_grad(): A.grad= 0.0000 A.data=-81361027072.0000 loss=3309808237620238483456.0000\n",
      "            step(): A.grad=-81361027072.0000 A.data=-314007420928.0000\n",
      "Ep550: zero_grad(): A.grad= 0.0000 A.data=-314007420928.0000 loss=49300328660072516288512.0000\n",
      "            step(): A.grad=-314007420928.0000 A.data=26695663616.0000\n",
      "Ep551: zero_grad(): A.grad= 0.0000 A.data=26695663616.0000 loss=356329235748436836352.0000\n",
      "            step(): A.grad=26695663616.0000 A.data=350747295744.0000\n",
      "Ep552: zero_grad(): A.grad= 0.0000 A.data=350747295744.0000 loss=61511834546087988822016.0000\n",
      "            step(): A.grad=350747295744.0000 A.data=40784232448.0000\n",
      "Ep553: zero_grad(): A.grad= 0.0000 A.data=40784232448.0000 loss=831676787699254034432.0000\n",
      "            step(): A.grad=40784232448.0000 A.data=-377665159168.0000\n",
      "Ep554: zero_grad(): A.grad= 0.0000 A.data=-377665159168.0000 loss=71315486387730198298624.0000\n",
      "            step(): A.grad=-377665159168.0000 A.data=-120395710464.0000\n",
      "Ep555: zero_grad(): A.grad= 0.0000 A.data=-120395710464.0000 loss=7247563504383906807808.0000\n",
      "            step(): A.grad=-120395710464.0000 A.data=391352549376.0000\n",
      "Ep556: zero_grad(): A.grad= 0.0000 A.data=391352549376.0000 loss=76578406423074242035712.0000\n",
      "            step(): A.grad=391352549376.0000 A.data=210705809408.0000\n",
      "Ep557: zero_grad(): A.grad= 0.0000 A.data=210705809408.0000 loss=22198469995090356994048.0000\n",
      "            step(): A.grad=210705809408.0000 A.data=-388346642432.0000\n",
      "Ep558: zero_grad(): A.grad= 0.0000 A.data=-388346642432.0000 loss=75406556289233556865024.0000\n",
      "            step(): A.grad=-388346642432.0000 A.data=-309445754880.0000\n",
      "Ep559: zero_grad(): A.grad= 0.0000 A.data=-309445754880.0000 loss=47878339595728419028992.0000\n",
      "            step(): A.grad=-309445754880.0000 A.data=365292191744.0000\n",
      "Ep560: zero_grad(): A.grad= 0.0000 A.data=365292191744.0000 loss=66719193672829162749952.0000\n",
      "            step(): A.grad=365292191744.0000 A.data=413448732672.0000\n",
      "Ep561: zero_grad(): A.grad= 0.0000 A.data=413448732672.0000 loss=85469926217786595475456.0000\n",
      "            step(): A.grad=413448732672.0000 A.data=-319131615232.0000\n",
      "Ep562: zero_grad(): A.grad= 0.0000 A.data=-319131615232.0000 loss=50922493720653977354240.0000\n",
      "            step(): A.grad=-319131615232.0000 A.data=-518619922432.0000\n",
      "Ep563: zero_grad(): A.grad= 0.0000 A.data=-518619922432.0000 loss=134483312531200827981824.0000\n",
      "            step(): A.grad=-518619922432.0000 A.data=247320739840.0000\n",
      "Ep564: zero_grad(): A.grad= 0.0000 A.data=247320739840.0000 loss=30583773932687198257152.0000\n",
      "            step(): A.grad=247320739840.0000 A.data=619946049536.0000\n",
      "Ep565: zero_grad(): A.grad= 0.0000 A.data=619946049536.0000 loss=192166560472867492855808.0000\n",
      "            step(): A.grad=619946049536.0000 A.data=-148063518720.0000\n",
      "Ep566: zero_grad(): A.grad= 0.0000 A.data=-148063518720.0000 loss=10961402330949504466944.0000\n",
      "            step(): A.grad=-148063518720.0000 A.data=-711553318912.0000\n",
      "Ep567: zero_grad(): A.grad= 0.0000 A.data=-711553318912.0000 loss=253154054425139616940032.0000\n",
      "            step(): A.grad=-711553318912.0000 A.data=20559167488.0000\n",
      "Ep568: zero_grad(): A.grad= 0.0000 A.data=20559167488.0000 loss=211339680841801400320.0000\n",
      "            step(): A.grad=20559167488.0000 A.data=786820497408.0000\n",
      "Ep569: zero_grad(): A.grad= 0.0000 A.data=786820497408.0000 loss=309543247057076898234368.0000\n",
      "            step(): A.grad=786820497408.0000 A.data=134749028352.0000\n",
      "Ep570: zero_grad(): A.grad= 0.0000 A.data=134749028352.0000 loss=9078650551679085182976.0000\n",
      "            step(): A.grad=134749028352.0000 A.data=-838552780800.0000\n",
      "Ep571: zero_grad(): A.grad= 0.0000 A.data=-838552780800.0000 loss=351585394413694028349440.0000\n",
      "            step(): A.grad=-838552780800.0000 A.data=-315934507008.0000\n",
      "Ep572: zero_grad(): A.grad= 0.0000 A.data=-315934507008.0000 loss=49907305803451002257408.0000\n",
      "            step(): A.grad=-315934507008.0000 A.data=859221262336.0000\n",
      "Ep573: zero_grad(): A.grad= 0.0000 A.data=859221262336.0000 loss=369130589899598044594176.0000\n",
      "            step(): A.grad=859221262336.0000 A.data=519372210176.0000\n",
      "Ep574: zero_grad(): A.grad= 0.0000 A.data=519372210176.0000 loss=134873747597296085762048.0000\n",
      "            step(): A.grad=519372210176.0000 A.data=-841268920320.0000\n",
      "Ep575: zero_grad(): A.grad= 0.0000 A.data=-841268920320.0000 loss=353866701812137807839232.0000\n",
      "            step(): A.grad=-841268920320.0000 A.data=-739563274240.0000\n",
      "Ep576: zero_grad(): A.grad= 0.0000 A.data=-739563274240.0000 loss=273476926046017679261696.0000\n",
      "            step(): A.grad=-739563274240.0000 A.data=777483059200.0000\n",
      "Ep577: zero_grad(): A.grad= 0.0000 A.data=777483059200.0000 loss=302239957699753769172992.0000\n",
      "            step(): A.grad=777483059200.0000 A.data=969016279040.0000\n",
      "Ep578: zero_grad(): A.grad= 0.0000 A.data=969016279040.0000 loss=469496261956905102999552.0000\n",
      "            step(): A.grad=969016279040.0000 A.data=-661428174848.0000\n",
      "Ep579: zero_grad(): A.grad= 0.0000 A.data=-661428174848.0000 loss=218743616925071981936640.0000\n",
      "            step(): A.grad=-661428174848.0000 A.data=-1198203600896.0000\n",
      "Ep580: zero_grad(): A.grad= 0.0000 A.data=-1198203600896.0000 loss=717845966371558422216704.0000\n",
      "            step(): A.grad=-1198203600896.0000 A.data=487930200064.0000\n",
      "Ep581: zero_grad(): A.grad= 0.0000 A.data=487930200064.0000 loss=119037938385956814979072.0000\n",
      "            step(): A.grad=487930200064.0000 A.data=1415610105856.0000\n",
      "Ep582: zero_grad(): A.grad= 0.0000 A.data=1415610105856.0000 loss=1001975977192135914946560.0000\n",
      "            step(): A.grad=1415610105856.0000 A.data=-253601120256.0000\n",
      "Ep583: zero_grad(): A.grad= 0.0000 A.data=-253601120256.0000 loss=32156764188937400877056.0000\n",
      "            step(): A.grad=-253601120256.0000 A.data=-1607891419136.0000\n",
      "Ep584: zero_grad(): A.grad= 0.0000 A.data=-1607891419136.0000 loss=1292657464462641815617536.0000\n",
      "            step(): A.grad=-1607891419136.0000 A.data=-42617012224.0000\n",
      "Ep585: zero_grad(): A.grad= 0.0000 A.data=-42617012224.0000 loss=908104843700568326144.0000\n",
      "            step(): A.grad=-42617012224.0000 A.data=1760157106176.0000\n",
      "Ep586: zero_grad(): A.grad= 0.0000 A.data=1760157106176.0000 loss=1549076574574429513711616.0000\n",
      "            step(): A.grad=1760157106176.0000 A.data=398910291968.0000\n",
      "Ep587: zero_grad(): A.grad= 0.0000 A.data=398910291968.0000 loss=79564707307186598969344.0000\n",
      "            step(): A.grad=398910291968.0000 A.data=-1856390823936.0000\n",
      "Ep588: zero_grad(): A.grad= 0.0000 A.data=-1856390823936.0000 loss=1723093502448204341313536.0000\n",
      "            step(): A.grad=-1856390823936.0000 A.data=-810079682560.0000\n",
      "Ep589: zero_grad(): A.grad= 0.0000 A.data=-810079682560.0000 loss=328114542682081008287744.0000\n",
      "            step(): A.grad=-810079682560.0000 A.data=1880013930496.0000\n",
      "Ep590: zero_grad(): A.grad= 0.0000 A.data=1880013930496.0000 loss=1767226184723049836707840.0000\n",
      "            step(): A.grad=1880013930496.0000 A.data=1267090849792.0000\n",
      "Ep591: zero_grad(): A.grad= 0.0000 A.data=1267090849792.0000 loss=802759643992169232990208.0000\n",
      "            step(): A.grad=1267090849792.0000 A.data=-1814596943872.0000\n",
      "Ep592: zero_grad(): A.grad= 0.0000 A.data=-1814596943872.0000 loss=1646380987835426260647936.0000\n",
      "            step(): A.grad=-1814596943872.0000 A.data=-1756719480832.0000\n",
      "Ep593: zero_grad(): A.grad= 0.0000 A.data=-1756719480832.0000 loss=1543031663010587739160576.0000\n",
      "            step(): A.grad=-1756719480832.0000 A.data=1644712689664.0000\n",
      "Ep594: zero_grad(): A.grad= 0.0000 A.data=1644712689664.0000 loss=1352539919181545295839232.0000\n",
      "            step(): A.grad=1644712689664.0000 A.data=2261334360064.0000\n",
      "Ep595: zero_grad(): A.grad= 0.0000 A.data=2261334360064.0000 loss=2556816438713659284258816.0000\n",
      "            step(): A.grad=2261334360064.0000 A.data=-1356917112832.0000\n",
      "Ep596: zero_grad(): A.grad= 0.0000 A.data=-1356917112832.0000 loss=920612000769021510156288.0000\n",
      "            step(): A.grad=-1356917112832.0000 A.data=-2758851166208.0000\n",
      "Ep597: zero_grad(): A.grad= 0.0000 A.data=-2758851166208.0000 loss=3805629883623171543793664.0000\n",
      "            step(): A.grad=-2758851166208.0000 A.data=940838486016.0000\n",
      "Ep598: zero_grad(): A.grad= 0.0000 A.data=940838486016.0000 loss=442588515191262053138432.0000\n",
      "            step(): A.grad=940838486016.0000 A.data=3222903980032.0000\n",
      "Ep599: zero_grad(): A.grad= 0.0000 A.data=3222903980032.0000 loss=5193555125508922111164416.0000\n",
      "            step(): A.grad=3222903980032.0000 A.data=-390341328896.0000\n",
      "Ep600: zero_grad(): A.grad= 0.0000 A.data=-390341328896.0000 loss=76183179526975462047744.0000\n",
      "            step(): A.grad=-390341328896.0000 A.data=-3623262617600.0000\n",
      "Ep601: zero_grad(): A.grad= 0.0000 A.data=-3623262617600.0000 loss=6564016200673264312254464.0000\n",
      "            step(): A.grad=-3623262617600.0000 A.data=-295277166592.0000\n",
      "Ep602: zero_grad(): A.grad= 0.0000 A.data=-295277166592.0000 loss=43594303960991116820480.0000\n",
      "            step(): A.grad=-295277166592.0000 A.data=3926533603328.0000\n",
      "Ep603: zero_grad(): A.grad= 0.0000 A.data=3926533603328.0000 loss=7708833243563477457436672.0000\n",
      "            step(): A.grad=3926533603328.0000 A.data=1110111682560.0000\n",
      "Ep604: zero_grad(): A.grad= 0.0000 A.data=1110111682560.0000 loss=616173998220734787223552.0000\n",
      "            step(): A.grad=1110111682560.0000 A.data=-4097164443648.0000\n",
      "Ep605: zero_grad(): A.grad= 0.0000 A.data=-4097164443648.0000 loss=8393378081080783635742720.0000\n",
      "            step(): A.grad=-4097164443648.0000 A.data=-2040556290048.0000\n",
      "Ep606: zero_grad(): A.grad= 0.0000 A.data=-2040556290048.0000 loss=2081934988495126655926272.0000\n",
      "            step(): A.grad=-2040556290048.0000 A.data=4098769813504.0000\n",
      "Ep607: zero_grad(): A.grad= 0.0000 A.data=4098769813504.0000 loss=8399957227646822608011264.0000\n",
      "            step(): A.grad=4098769813504.0000 A.data=3064365580288.0000\n",
      "Ep608: zero_grad(): A.grad= 0.0000 A.data=3064365580288.0000 loss=4695168199884841238396928.0000\n",
      "            step(): A.grad=3064365580288.0000 A.data=-3895773626368.0000\n",
      "Ep609: zero_grad(): A.grad= 0.0000 A.data=-3895773626368.0000 loss=7588525884557752975491072.0000\n",
      "            step(): A.grad=-3895773626368.0000 A.data=-4149957099520.0000\n",
      "Ep610: zero_grad(): A.grad= 0.0000 A.data=-4149957099520.0000 loss=8611072143119896178327552.0000\n",
      "            step(): A.grad=-4149957099520.0000 A.data=3455359909888.0000\n",
      "Ep611: zero_grad(): A.grad= 0.0000 A.data=3455359909888.0000 loss=5969756069720968017215488.0000\n",
      "            step(): A.grad=3455359909888.0000 A.data=5256024424448.0000\n",
      "Ep612: zero_grad(): A.grad= 0.0000 A.data=5256024424448.0000 loss=13812896598120610899427328.0000\n",
      "            step(): A.grad=5256024424448.0000 A.data=-2749690806272.0000\n",
      "Ep613: zero_grad(): A.grad= 0.0000 A.data=-2749690806272.0000 loss=3780399637646731456282624.0000\n",
      "            step(): A.grad=-2749690806272.0000 A.data=-6331565080576.0000\n",
      "Ep614: zero_grad(): A.grad= 0.0000 A.data=-6331565080576.0000 loss=20044358931858305539112960.0000\n",
      "            step(): A.grad=-6331565080576.0000 A.data=1758346608640.0000\n",
      "Ep615: zero_grad(): A.grad= 0.0000 A.data=1758346608640.0000 loss=1545891340687576947228672.0000\n",
      "            step(): A.grad=1758346608640.0000 A.data=7316390805504.0000\n",
      "Ep616: zero_grad(): A.grad= 0.0000 A.data=7316390805504.0000 loss=26764786804914810049789952.0000\n",
      "            step(): A.grad=7316390805504.0000 A.data=-470903422976.0000\n",
      "Ep617: zero_grad(): A.grad= 0.0000 A.data=-470903422976.0000 loss=110875019946159715123200.0000\n",
      "            step(): A.grad=-470903422976.0000 A.data=-8142209941504.0000\n",
      "Ep618: zero_grad(): A.grad= 0.0000 A.data=-8142209941504.0000 loss=33147791447061037869694976.0000\n",
      "            step(): A.grad=-8142209941504.0000 A.data=-1110448799744.0000\n",
      "Ep619: zero_grad(): A.grad= 0.0000 A.data=-1110448799744.0000 loss=616548265364167784923136.0000\n",
      "            step(): A.grad=-1110448799744.0000 A.data=8734341857280.0000\n",
      "Ep620: zero_grad(): A.grad= 0.0000 A.data=8734341857280.0000 loss=38144364168900357140774912.0000\n",
      "            step(): A.grad=8734341857280.0000 A.data=2968361893888.0000\n",
      "Ep621: zero_grad(): A.grad= 0.0000 A.data=2968361893888.0000 loss=4405586023268977966317568.0000\n",
      "            step(): A.grad=2968361893888.0000 A.data=-9014102458368.0000\n",
      "Ep622: zero_grad(): A.grad= 0.0000 A.data=-9014102458368.0000 loss=40627021525903566358708224.0000\n",
      "            step(): A.grad=-9014102458368.0000 A.data=-5068019466240.0000\n",
      "Ep623: zero_grad(): A.grad= 0.0000 A.data=-5068019466240.0000 loss=12842411086559742175215616.0000\n",
      "            step(): A.grad=-5068019466240.0000 A.data=8901909020672.0000\n",
      "Ep624: zero_grad(): A.grad= 0.0000 A.data=8901909020672.0000 loss=39621992180221667285401600.0000\n",
      "            step(): A.grad=8901909020672.0000 A.data=7355203321856.0000\n",
      "Ep625: zero_grad(): A.grad= 0.0000 A.data=7355203321856.0000 loss=27049507688006498551595008.0000\n",
      "            step(): A.grad=7355203321856.0000 A.data=-8321059258368.0000\n",
      "Ep626: zero_grad(): A.grad= 0.0000 A.data=-8321059258368.0000 loss=34620014562368751115698176.0000\n",
      "            step(): A.grad=-8321059258368.0000 A.data=-9754936082432.0000\n",
      "Ep627: zero_grad(): A.grad= 0.0000 A.data=-9754936082432.0000 loss=47579387229727848702935040.0000\n",
      "            step(): A.grad=-9754936082432.0000 A.data=7202176761856.0000\n",
      "Ep628: zero_grad(): A.grad= 0.0000 A.data=7202176761856.0000 loss=25935674834091842115469312.0000\n",
      "            step(): A.grad=7202176761856.0000 A.data=12170865672192.0000\n",
      "Ep629: zero_grad(): A.grad= 0.0000 A.data=12170865672192.0000 loss=74064987174773083116404736.0000\n",
      "            step(): A.grad=12170865672192.0000 A.data=-5488221618176.0000\n",
      "Ep630: zero_grad(): A.grad= 0.0000 A.data=-5488221618176.0000 loss=15060288490814942916640768.0000\n",
      "            step(): A.grad=-5488221618176.0000 A.data=-14485598240768.0000\n",
      "Ep631: zero_grad(): A.grad= 0.0000 A.data=-14485598240768.0000 loss=104916281194336770135687168.0000\n",
      "            step(): A.grad=-14485598240768.0000 A.data=3139924131840.0000\n",
      "Ep632: zero_grad(): A.grad= 0.0000 A.data=3139924131840.0000 loss=4929562041687807807717376.0000\n",
      "            step(): A.grad=3139924131840.0000 A.data=16562142576640.0000\n",
      "Ep633: zero_grad(): A.grad= 0.0000 A.data=16562142576640.0000 loss=137152280057793464647024640.0000\n",
      "            step(): A.grad=16562142576640.0000 A.data=-141486456832.0000\n",
      "Ep634: zero_grad(): A.grad= 0.0000 A.data=-141486456832.0000 loss=10009208513534374182912.0000\n",
      "            step(): A.grad=-141486456832.0000 A.data=-18246654754816.0000\n",
      "Ep635: zero_grad(): A.grad= 0.0000 A.data=-18246654754816.0000 loss=166470206922974118897778688.0000\n",
      "            step(): A.grad=-18246654754816.0000 A.data=-3493696897024.0000\n",
      "Ep636: zero_grad(): A.grad= 0.0000 A.data=-3493696897024.0000 loss=6102958855755720082587648.0000\n",
      "            step(): A.grad=-3493696897024.0000 A.data=19372580012032.0000\n",
      "Ep637: zero_grad(): A.grad= 0.0000 A.data=19372580012032.0000 loss=187648434178654138659766272.0000\n",
      "            step(): A.grad=19372580012032.0000 A.data=7717584371712.0000\n",
      "Ep638: zero_grad(): A.grad= 0.0000 A.data=7717584371712.0000 loss=29780555065648225309425664.0000\n",
      "            step(): A.grad=7717584371712.0000 A.data=-19766322397184.0000\n",
      "Ep639: zero_grad(): A.grad= 0.0000 A.data=-19766322397184.0000 loss=195353749858707060627079168.0000\n",
      "            step(): A.grad=-19766322397184.0000 A.data=-12442608336896.0000\n",
      "Ep640: zero_grad(): A.grad= 0.0000 A.data=-12442608336896.0000 loss=77409252748128108857524224.0000\n",
      "            step(): A.grad=-12442608336896.0000 A.data=19254432759808.0000\n",
      "Ep641: zero_grad(): A.grad= 0.0000 A.data=19254432759808.0000 loss=185366590383480340834418688.0000\n",
      "            step(): A.grad=19254432759808.0000 A.data=17537756561408.0000\n",
      "Ep642: zero_grad(): A.grad= 0.0000 A.data=17537756561408.0000 loss=153786456282192352576012288.0000\n",
      "            step(): A.grad=17537756561408.0000 A.data=-17672326610944.0000\n",
      "Ep643: zero_grad(): A.grad= 0.0000 A.data=-17672326610944.0000 loss=156155562400206833435279360.0000\n",
      "            step(): A.grad=-17672326610944.0000 A.data=-22825997959168.0000\n",
      "Ep644: zero_grad(): A.grad= 0.0000 A.data=-22825997959168.0000 loss=260513091716550941252517888.0000\n",
      "            step(): A.grad=-22825997959168.0000 A.data=14874360938496.0000\n",
      "Ep645: zero_grad(): A.grad= 0.0000 A.data=14874360938496.0000 loss=110623307205744920650317824.0000\n",
      "            step(): A.grad=14874360938496.0000 A.data=28083472039936.0000\n",
      "Ep646: zero_grad(): A.grad= 0.0000 A.data=28083472039936.0000 loss=394340696643195659702239232.0000\n",
      "            step(): A.grad=28083472039936.0000 A.data=-10745100107776.0000\n",
      "Ep647: zero_grad(): A.grad= 0.0000 A.data=-10745100107776.0000 loss=57728587516584051624378368.0000\n",
      "            step(): A.grad=-10745100107776.0000 A.data=-33040841572352.0000\n",
      "Ep648: zero_grad(): A.grad= 0.0000 A.data=-33040841572352.0000 loss=545848601874031371607867392.0000\n",
      "            step(): A.grad=-33040841572352.0000 A.data=5211441594368.0000\n",
      "Ep649: zero_grad(): A.grad= 0.0000 A.data=5211441594368.0000 loss=13579561426489249567342592.0000\n",
      "            step(): A.grad=5211441594368.0000 A.data=37387212161024.0000\n",
      "Ep650: zero_grad(): A.grad= 0.0000 A.data=37387212161024.0000 loss=698901827749409880071471104.0000\n",
      "            step(): A.grad=37387212161024.0000 A.data=1744864018432.0000\n",
      "Ep651: zero_grad(): A.grad= 0.0000 A.data=1744864018432.0000 loss=1522275184817586445484032.0000\n",
      "            step(): A.grad=1744864018432.0000 A.data=-40776960573440.0000\n",
      "Ep652: zero_grad(): A.grad= 0.0000 A.data=-40776960573440.0000 loss=831380293154735703044653056.0000\n",
      "            step(): A.grad=-40776960573440.0000 A.data=-10074743373824.0000\n",
      "Ep653: zero_grad(): A.grad= 0.0000 A.data=-10074743373824.0000 loss=50750225738673894094536704.0000\n",
      "            step(): A.grad=-10074743373824.0000 A.data=42839706697728.0000\n",
      "Ep654: zero_grad(): A.grad= 0.0000 A.data=42839706697728.0000 loss=917620223651877458775375872.0000\n",
      "            step(): A.grad=42839706697728.0000 A.data=19650161147904.0000\n",
      "Ep655: zero_grad(): A.grad= 0.0000 A.data=19650161147904.0000 loss=193064416685439336723775488.0000\n",
      "            step(): A.grad=19650161147904.0000 A.data=-43193647235072.0000\n",
      "Ep656: zero_grad(): A.grad= 0.0000 A.data=-43193647235072.0000 loss=932845575914506963973570560.0000\n",
      "            step(): A.grad=-43193647235072.0000 A.data=-30253909016576.0000\n",
      "Ep657: zero_grad(): A.grad= 0.0000 A.data=-30253909016576.0000 loss=457649516475797219238215680.0000\n",
      "            step(): A.grad=-30253909016576.0000 A.data=41462221766656.0000\n",
      "Ep658: zero_grad(): A.grad= 0.0000 A.data=41462221766656.0000 loss=859557916088255927652712448.0000\n",
      "            step(): A.grad=41462221766656.0000 A.data=41571751821312.0000\n",
      "Ep659: zero_grad(): A.grad= 0.0000 A.data=41571751821312.0000 loss=864105259863354216640675840.0000\n",
      "            step(): A.grad=41571751821312.0000 A.data=-37294098612224.0000\n",
      "Ep660: zero_grad(): A.grad= 0.0000 A.data=-37294098612224.0000 loss=695424911639420808944680960.0000\n",
      "            step(): A.grad=-37294098612224.0000 A.data=-53187746725888.0000\n",
      "Ep661: zero_grad(): A.grad= 0.0000 A.data=-53187746725888.0000 loss=1414468148947103789720010752.0000\n",
      "            step(): A.grad=-53187746725888.0000 A.data=30385958289408.0000\n",
      "Ep662: zero_grad(): A.grad= 0.0000 A.data=30385958289408.0000 loss=461653234703043287740055552.0000\n",
      "            step(): A.grad=30385958289408.0000 A.data=64583716831232.0000\n",
      "Ep663: zero_grad(): A.grad= 0.0000 A.data=64583716831232.0000 loss=2085528232079087548699246592.0000\n",
      "            step(): A.grad=64583716831232.0000 A.data=-20507814526976.0000\n",
      "Ep664: zero_grad(): A.grad= 0.0000 A.data=-20507814526976.0000 loss=210285227041498298958479360.0000\n",
      "            step(): A.grad=-20507814526976.0000 A.data=-75143648903168.0000\n",
      "Ep665: zero_grad(): A.grad= 0.0000 A.data=-75143648903168.0000 loss=2823283920362117793472053248.0000\n",
      "            step(): A.grad=-75143648903168.0000 A.data=7529857810432.0000\n",
      "Ep666: zero_grad(): A.grad= 0.0000 A.data=7529857810432.0000 loss=28349378261747525029462016.0000\n",
      "            step(): A.grad=7529857810432.0000 A.data=84163986194432.0000\n",
      "Ep667: zero_grad(): A.grad= 0.0000 A.data=84163986194432.0000 loss=3541788143807966981149163520.0000\n",
      "            step(): A.grad=84163986194432.0000 A.data=8549962874880.0000\n",
      "Ep668: zero_grad(): A.grad= 0.0000 A.data=8549962874880.0000 loss=36550932109970316858490880.0000\n",
      "            step(): A.grad=8549962874880.0000 A.data=-90870393077760.0000\n",
      "Ep669: zero_grad(): A.grad= 0.0000 A.data=-90870393077760.0000 loss=4128714250533239961648365568.0000\n",
      "            step(): A.grad=-90870393077760.0000 A.data=-27579041972224.0000\n",
      "Ep670: zero_grad(): A.grad= 0.0000 A.data=-27579041972224.0000 loss=380301765172410858025779200.0000\n",
      "            step(): A.grad=-27579041972224.0000 A.data=94441624829952.0000\n",
      "Ep671: zero_grad(): A.grad= 0.0000 A.data=94441624829952.0000 loss=4459610174840010822419218432.0000\n",
      "            step(): A.grad=94441624829952.0000 A.data=49225278685184.0000\n",
      "Ep672: zero_grad(): A.grad= 0.0000 A.data=49225278685184.0000 loss=1211563999165074819940089856.0000\n",
      "            step(): A.grad=49225278685184.0000 A.data=-94040741642240.0000\n",
      "Ep673: zero_grad(): A.grad= 0.0000 A.data=-94040741642240.0000 loss=4421830652681243302003998720.0000\n",
      "            step(): A.grad=-94040741642240.0000 A.data=-72955958657024.0000\n",
      "Ep674: zero_grad(): A.grad= 0.0000 A.data=-72955958657024.0000 loss=2661286089646326407944470528.0000\n",
      "            step(): A.grad=-72955958657024.0000 A.data=88853629108224.0000\n",
      "Ep675: zero_grad(): A.grad= 0.0000 A.data=88853629108224.0000 loss=3947483696872246407929528320.0000\n",
      "            step(): A.grad=88853629108224.0000 A.data=98022293766144.0000\n",
      "Ep676: zero_grad(): A.grad= 0.0000 A.data=98022293766144.0000 loss=4804185023960223969257193472.0000\n",
      "            step(): A.grad=98022293766144.0000 A.data=-78134531588096.0000\n",
      "Ep677: zero_grad(): A.grad= 0.0000 A.data=-78134531588096.0000 loss=3052502571926222323146817536.0000\n",
      "            step(): A.grad=-78134531588096.0000 A.data=-123451436171264.0000\n",
      "Ep678: zero_grad(): A.grad= 0.0000 A.data=-123451436171264.0000 loss=7620128615920531257950208000.0000\n",
      "            step(): A.grad=-123451436171264.0000 A.data=61257692479488.0000\n",
      "Ep679: zero_grad(): A.grad= 0.0000 A.data=61257692479488.0000 loss=1876252429320046710114746368.0000\n",
      "            step(): A.grad=61257692479488.0000 A.data=148048126672896.0000\n",
      "Ep680: zero_grad(): A.grad= 0.0000 A.data=148048126672896.0000 loss=10959123880706721885334798336.0000\n",
      "            step(): A.grad=148048126672896.0000 A.data=-37773834715136.0000\n",
      "Ep681: zero_grad(): A.grad= 0.0000 A.data=-37773834715136.0000 loss=713431295038602766144503808.0000\n",
      "            step(): A.grad=-37773834715136.0000 A.data=-170407709638656.0000\n",
      "Ep682: zero_grad(): A.grad= 0.0000 A.data=-170407709638656.0000 loss=14519393891189114761086238720.0000\n",
      "            step(): A.grad=-170407709638656.0000 A.data=7469669548032.0000\n",
      "Ep683: zero_grad(): A.grad= 0.0000 A.data=7469669548032.0000 loss=27897981822577833874030592.0000\n",
      "            step(): A.grad=7469669548032.0000 A.data=188942422900736.0000\n",
      "Ep684: zero_grad(): A.grad= 0.0000 A.data=188942422900736.0000 loss=17849619721416616457308995584.0000\n",
      "            step(): A.grad=188942422900736.0000 A.data=29571856465920.0000\n",
      "Ep685: zero_grad(): A.grad= 0.0000 A.data=29571856465920.0000 loss=437247343743298160312713216.0000\n",
      "            step(): A.grad=29571856465920.0000 A.data=-201922300608512.0000\n",
      "Ep686: zero_grad(): A.grad= 0.0000 A.data=-201922300608512.0000 loss=20386307352004047993701400576.0000\n",
      "            step(): A.grad=-201922300608512.0000 A.data=-72913512300544.0000\n",
      "Ep687: zero_grad(): A.grad= 0.0000 A.data=-72913512300544.0000 loss=2658190283268900176154066944.0000\n",
      "            step(): A.grad=-72913512300544.0000 A.data=207531846664192.0000\n",
      "Ep688: zero_grad(): A.grad= 0.0000 A.data=207531846664192.0000 loss=21534732573423392708752310272.0000\n",
      "            step(): A.grad=207531846664192.0000 A.data=121711261384704.0000\n",
      "Ep689: zero_grad(): A.grad= 0.0000 A.data=121711261384704.0000 loss=7406815780114447231413649408.0000\n",
      "            step(): A.grad=121711261384704.0000 A.data=-203942763954176.0000\n",
      "Ep690: zero_grad(): A.grad= 0.0000 A.data=-203942763954176.0000 loss=20796324460695963504557948928.0000\n",
      "            step(): A.grad=-203942763954176.0000 A.data=-174670951219200.0000\n",
      "Ep691: zero_grad(): A.grad= 0.0000 A.data=-174670951219200.0000 loss=15254970945210063612974989312.0000\n",
      "            step(): A.grad=-174670951219200.0000 A.data=189402823262208.0000\n",
      "Ep692: zero_grad(): A.grad= 0.0000 A.data=189402823262208.0000 loss=17936714326460181323985190912.0000\n",
      "            step(): A.grad=189402823262208.0000 A.data=230018634481664.0000\n",
      "Ep693: zero_grad(): A.grad= 0.0000 A.data=230018634481664.0000 loss=26454286191151742827991400448.0000\n",
      "            step(): A.grad=230018634481664.0000 A.data=-162339378692096.0000\n",
      "Ep694: zero_grad(): A.grad= 0.0000 A.data=-162339378692096.0000 loss=13177036426009383043455778816.0000\n",
      "            step(): A.grad=-162339378692096.0000 A.data=-285488388767744.0000\n",
      "Ep695: zero_grad(): A.grad= 0.0000 A.data=-285488388767744.0000 loss=40751807957284572330591256576.0000\n",
      "            step(): A.grad=-285488388767744.0000 A.data=121475633774592.0000\n",
      "Ep696: zero_grad(): A.grad= 0.0000 A.data=121475633774592.0000 loss=7378164592367066735196504064.0000\n",
      "            step(): A.grad=121475633774592.0000 A.data=338332357754880.0000\n",
      "Ep697: zero_grad(): A.grad= 0.0000 A.data=338332357754880.0000 loss=57234392306873601021794320384.0000\n",
      "            step(): A.grad=338332357754880.0000 A.data=-65956705468416.0000\n",
      "Ep698: zero_grad(): A.grad= 0.0000 A.data=-65956705468416.0000 loss=2175143547806836678184665088.0000\n",
      "            step(): A.grad=-65956705468416.0000 A.data=-385356948045824.0000\n",
      "Ep699: zero_grad(): A.grad= 0.0000 A.data=-385356948045824.0000 loss=74249990161384126568267251712.0000\n",
      "            step(): A.grad=-385356948045824.0000 A.data=-4519010238464.0000\n",
      "Ep700: zero_grad(): A.grad= 0.0000 A.data=-4519010238464.0000 loss=10210727095871051917164544.0000\n",
      "            step(): A.grad=-4519010238464.0000 A.data=422988847513600.0000\n",
      "Ep701: zero_grad(): A.grad= 0.0000 A.data=422988847513600.0000 loss=89459783407044197002894114816.0000\n",
      "            step(): A.grad=422988847513600.0000 A.data=89568724385792.0000\n",
      "Ep702: zero_grad(): A.grad= 0.0000 A.data=89568724385792.0000 loss=4011278145689332445121347584.0000\n",
      "            step(): A.grad=89568724385792.0000 A.data=-447373994098688.0000\n",
      "Ep703: zero_grad(): A.grad= 0.0000 A.data=-447373994098688.0000 loss=100071743696354377637755355136.0000\n",
      "            step(): A.grad=-447373994098688.0000 A.data=-188000415776768.0000\n",
      "Ep704: zero_grad(): A.grad= 0.0000 A.data=-188000415776768.0000 loss=17672077631126649275854880768.0000\n",
      "            step(): A.grad=-188000415776768.0000 A.data=454511323774976.0000\n",
      "Ep705: zero_grad(): A.grad= 0.0000 A.data=454511323774976.0000 loss=103290272572754184333149863936.0000\n",
      "            step(): A.grad=454511323774976.0000 A.data=297702772441088.0000\n",
      "Ep706: zero_grad(): A.grad= 0.0000 A.data=297702772441088.0000 loss=44313468704696170316858130432.0000\n",
      "            step(): A.grad=297702772441088.0000 A.data=-440421918441472.0000\n",
      "Ep707: zero_grad(): A.grad= 0.0000 A.data=-440421918441472.0000 loss=96985733868196858926347583488.0000\n",
      "            step(): A.grad=-440421918441472.0000 A.data=-415557446795264.0000\n",
      "Ep708: zero_grad(): A.grad= 0.0000 A.data=-415557446795264.0000 loss=86343994335845702308768776192.0000\n",
      "            step(): A.grad=-415557446795264.0000 A.data=401352580661248.0000\n",
      "Ep709: zero_grad(): A.grad= 0.0000 A.data=401352580661248.0000 loss=80541943759264604370575032320.0000\n",
      "            step(): A.grad=401352580661248.0000 A.data=537383757938688.0000\n",
      "Ep710: zero_grad(): A.grad= 0.0000 A.data=537383757938688.0000 loss=144390652567238813785899663360.0000\n",
      "            step(): A.grad=537383757938688.0000 A.data=-334011083784192.0000\n",
      "Ep711: zero_grad(): A.grad= 0.0000 A.data=-334011083784192.0000 loss=55781702651779723630802370560.0000\n",
      "            step(): A.grad=-334011083784192.0000 A.data=-657924363911168.0000\n",
      "Ep712: zero_grad(): A.grad= 0.0000 A.data=-657924363911168.0000 loss=216432232765275433639627194368.0000\n",
      "            step(): A.grad=-657924363911168.0000 A.data=235827326091264.0000\n",
      "Ep713: zero_grad(): A.grad= 0.0000 A.data=235827326091264.0000 loss=27807263077959827660296159232.0000\n",
      "            step(): A.grad=235827326091264.0000 A.data=770882339340288.0000\n",
      "Ep714: zero_grad(): A.grad= 0.0000 A.data=770882339340288.0000 loss=297129787944883558559283937280.0000\n",
      "            step(): A.grad=770882339340288.0000 A.data=-105233610964992.0000\n",
      "Ep715: zero_grad(): A.grad= 0.0000 A.data=-105233610964992.0000 loss=5537056161986488514438496256.0000\n",
      "            step(): A.grad=-105233610964992.0000 A.data=-869017308889088.0000\n",
      "Ep716: zero_grad(): A.grad= 0.0000 A.data=-869017308889088.0000 loss=377595550488046509813271625728.0000\n",
      "            step(): A.grad=-869017308889088.0000 A.data=-58046550114304.0000\n",
      "Ep717: zero_grad(): A.grad= 0.0000 A.data=-58046550114304.0000 loss=1684700996136788957104963584.0000\n",
      "            step(): A.grad=-58046550114304.0000 A.data=944309763309568.0000\n",
      "Ep718: zero_grad(): A.grad= 0.0000 A.data=944309763309568.0000 loss=445860455870339993864507490304.0000\n",
      "            step(): A.grad=944309763309568.0000 A.data=252713191342080.0000\n",
      "Ep719: zero_grad(): A.grad= 0.0000 A.data=252713191342080.0000 loss=31931978664673697749859827712.0000\n",
      "            step(): A.grad=252713191342080.0000 A.data=-988198155059200.0000\n",
      "Ep720: zero_grad(): A.grad= 0.0000 A.data=-988198155059200.0000 loss=488267798012623626326599794688.0000\n",
      "            step(): A.grad=-988198155059200.0000 A.data=-475624242151424.0000\n",
      "Ep721: zero_grad(): A.grad= 0.0000 A.data=-475624242151424.0000 loss=113109205858596065447275134976.0000\n",
      "            step(): A.grad=-475624242151424.0000 A.data=991893169111040.0000\n",
      "Ep722: zero_grad(): A.grad= 0.0000 A.data=991893169111040.0000 loss=491926045321709357166422392832.0000\n",
      "            step(): A.grad=991893169111040.0000 A.data=721565377363968.0000\n",
      "Ep723: zero_grad(): A.grad= 0.0000 A.data=721565377363968.0000 loss=260328291496550756016046735360.0000\n",
      "            step(): A.grad=721565377363968.0000 A.data=-946769303175168.0000\n",
      "Ep724: zero_grad(): A.grad= 0.0000 A.data=-946769303175168.0000 loss=448186051357959910825025077248.0000\n",
      "            step(): A.grad=-946769303175168.0000 A.data=-983075869687808.0000\n",
      "Ep725: zero_grad(): A.grad= 0.0000 A.data=-983075869687808.0000 loss=483219097116321757150056546304.0000\n",
      "            step(): A.grad=-983075869687808.0000 A.data=844831072976896.0000\n",
      "Ep726: zero_grad(): A.grad= 0.0000 A.data=844831072976896.0000 loss=356869764015505170199270653952.0000\n",
      "            step(): A.grad=844831072976896.0000 A.data=1250349805469696.0000\n",
      "Ep727: zero_grad(): A.grad= 0.0000 A.data=1250349805469696.0000 loss=781687354838178024991548768256.0000\n",
      "            step(): A.grad=1250349805469696.0000 A.data=-679244246024192.0000\n",
      "Ep728: zero_grad(): A.grad= 0.0000 A.data=-679244246024192.0000 loss=230686374872896622581294039040.0000\n",
      "            step(): A.grad=-679244246024192.0000 A.data=-1511233635221504.0000\n",
      "Ep729: zero_grad(): A.grad= 0.0000 A.data=-1511233635221504.0000 loss=1141913550276115761265744281600.0000\n",
      "            step(): A.grad=-1511233635221504.0000 A.data=444921836208128.0000\n",
      "Ep730: zero_grad(): A.grad= 0.0000 A.data=444921836208128.0000 loss=98977722498000932670388830208.0000\n",
      "            step(): A.grad=444921836208128.0000 A.data=1751341365985280.0000\n",
      "Ep731: zero_grad(): A.grad= 0.0000 A.data=1751341365985280.0000 loss=1533598262276337926095497068544.0000\n",
      "            step(): A.grad=1751341365985280.0000 A.data=-139145666101248.0000\n",
      "Ep732: zero_grad(): A.grad= 0.0000 A.data=-139145666101248.0000 loss=9680758023144736012583829504.0000\n",
      "            step(): A.grad=-139145666101248.0000 A.data=-1954304743178240.0000\n",
      "Ep733: zero_grad(): A.grad= 0.0000 A.data=-1954304743178240.0000 loss=1909653527933399809468707897344.0000\n",
      "            step(): A.grad=-1954304743178240.0000 A.data=-237800796454912.0000\n",
      "Ep734: zero_grad(): A.grad= 0.0000 A.data=-237800796454912.0000 loss=28274609715753780664046977024.0000\n",
      "            step(): A.grad=-237800796454912.0000 A.data=2102175232688128.0000\n",
      "Ep735: zero_grad(): A.grad= 0.0000 A.data=2102175232688128.0000 loss=2209570343239677443761630085120.0000\n",
      "            step(): A.grad=2102175232688128.0000 A.data=682015976325120.0000\n",
      "Ep736: zero_grad(): A.grad= 0.0000 A.data=682015976325120.0000 loss=232572903614405251408423026688.0000\n",
      "            step(): A.grad=682015976325120.0000 A.data=-2175989748596736.0000\n",
      "Ep737: zero_grad(): A.grad= 0.0000 A.data=-2175989748596736.0000 loss=2367465726687904931011654320128.0000\n",
      "            step(): A.grad=-2175989748596736.0000 A.data=-1185415537098752.0000\n",
      "Ep738: zero_grad(): A.grad= 0.0000 A.data=-1185415537098752.0000 loss=702605019000904702042203750400.0000\n",
      "            step(): A.grad=-1185415537098752.0000 A.data=2156505495240704.0000\n",
      "Ep739: zero_grad(): A.grad= 0.0000 A.data=2156505495240704.0000 loss=2325258046200517027149197082624.0000\n",
      "            step(): A.grad=2156505495240704.0000 A.data=1735258324074496.0000\n",
      "Ep740: zero_grad(): A.grad= 0.0000 A.data=1735258324074496.0000 loss=1505560703552107994447064596480.0000\n",
      "            step(): A.grad=1735258324074496.0000 A.data=-2025104192045056.0000\n",
      "Ep741: zero_grad(): A.grad= 0.0000 A.data=-2025104192045056.0000 loss=2050523457948267022222698217472.0000\n",
      "            step(): A.grad=-2025104192045056.0000 A.data=-2313805182795776.0000\n",
      "Ep742: zero_grad(): A.grad= 0.0000 A.data=-2313805182795776.0000 loss=2676847071120218045284704321536.0000\n",
      "            step(): A.grad=-2313805182795776.0000 A.data=1764853601533952.0000\n",
      "Ep743: zero_grad(): A.grad= 0.0000 A.data=1764853601533952.0000 loss=1557354107978947744864413941760.0000\n",
      "            step(): A.grad=1764853601533952.0000 A.data=2898156421382144.0000\n",
      "Ep744: zero_grad(): A.grad= 0.0000 A.data=2898156421382144.0000 loss=4199655368225040502338655092736.0000\n",
      "            step(): A.grad=2898156421382144.0000 A.data=-1361707838472192.0000\n",
      "Ep745: zero_grad(): A.grad= 0.0000 A.data=-1361707838472192.0000 loss=927124153252366951281638506496.0000\n",
      "            step(): A.grad=-1361707838472192.0000 A.data=-3460313953337344.0000\n",
      "Ep746: zero_grad(): A.grad= 0.0000 A.data=-3460313953337344.0000 loss=5986886569687982532944801562624.0000\n",
      "            step(): A.grad=-3460313953337344.0000 A.data=805815858495488.0000\n",
      "Ep747: zero_grad(): A.grad= 0.0000 A.data=805815858495488.0000 loss=324669609241819029602192850944.0000\n",
      "            step(): A.grad=805815858495488.0000 A.data=3967508520370176.0000\n",
      "Ep748: zero_grad(): A.grad= 0.0000 A.data=3967508520370176.0000 loss=7870561641799411027906895282176.0000\n",
      "            step(): A.grad=3967508520370176.0000 A.data=-92895579209728.0000\n",
      "Ep749: zero_grad(): A.grad= 0.0000 A.data=-92895579209728.0000 loss=4314794379424235461648842752.0000\n",
      "            step(): A.grad=-92895579209728.0000 A.data=-4382838837215232.0000\n",
      "Ep750: zero_grad(): A.grad= 0.0000 A.data=-4382838837215232.0000 loss=9604638188362627235644973252608.0000\n",
      "            step(): A.grad=-4382838837215232.0000 A.data=-774382603468800.0000\n",
      "Ep751: zero_grad(): A.grad= 0.0000 A.data=-774382603468800.0000 loss=299834211671759278458844217344.0000\n",
      "            step(): A.grad=-774382603468800.0000 A.data=4666246280773632.0000\n",
      "Ep752: zero_grad(): A.grad= 0.0000 A.data=4666246280773632.0000 loss=10886927066877660374503606190080.0000\n",
      "            step(): A.grad=4666246280773632.0000 A.data=1785070549467136.0000\n",
      "Ep753: zero_grad(): A.grad= 0.0000 A.data=1785070549467136.0000 loss=1593238501966841330828570525696.0000\n",
      "            step(): A.grad=1785070549467136.0000 A.data=-4775856530522112.0000\n",
      "Ep754: zero_grad(): A.grad= 0.0000 A.data=-4775856530522112.0000 loss=11404402928260423934219886002176.0000\n",
      "            step(): A.grad=-4775856530522112.0000 A.data=-2918749447389184.0000\n",
      "Ep755: zero_grad(): A.grad= 0.0000 A.data=-2918749447389184.0000 loss=4259549180106208075541123170304.0000\n",
      "            step(): A.grad=-2918749447389184.0000 A.data=4669692455157760.0000\n",
      "Ep756: zero_grad(): A.grad= 0.0000 A.data=4669692455157760.0000 loss=10903014242759272244931421274112.0000\n",
      "            step(): A.grad=4669692455157760.0000 A.data=4144562641567744.0000\n",
      "Ep757: zero_grad(): A.grad= 0.0000 A.data=4144562641567744.0000 loss=8588699846425089196557605011456.0000\n",
      "            step(): A.grad=4144562641567744.0000 A.data=-4307748850237440.0000\n",
      "Ep758: zero_grad(): A.grad= 0.0000 A.data=-4307748850237440.0000 loss=9278350318574458436020951056384.0000\n",
      "            step(): A.grad=-4307748850237440.0000 A.data=-5420568702615552.0000\n",
      "Ep759: zero_grad(): A.grad= 0.0000 A.data=-5420568702615552.0000 loss=14691282430438921163465549676544.0000\n",
      "            step(): A.grad=-5420568702615552.0000 A.data=3654409431023616.0000\n",
      "Ep760: zero_grad(): A.grad= 0.0000 A.data=3654409431023616.0000 loss=6677354275691411261730248982528.0000\n",
      "            step(): A.grad=3654409431023616.0000 A.data=6693507351707648.0000\n",
      "Ep761: zero_grad(): A.grad= 0.0000 A.data=6693507351707648.0000 loss=22401521165744318528739610722304.0000\n",
      "            step(): A.grad=6693507351707648.0000 A.data=-2681148903784448.0000\n",
      "Ep762: zero_grad(): A.grad= 0.0000 A.data=-2681148903784448.0000 loss=3594279719423916869958663929856.0000\n",
      "            step(): A.grad=-2681148903784448.0000 A.data=-7899088028696576.0000\n",
      "Ep763: zero_grad(): A.grad= 0.0000 A.data=-7899088028696576.0000 loss=31197795993248878783852615565312.0000\n",
      "            step(): A.grad=-7899088028696576.0000 A.data=1369444953620480.0000\n",
      "Ep764: zero_grad(): A.grad= 0.0000 A.data=1369444953620480.0000 loss=937689711568616454782629969920.0000\n",
      "            step(): A.grad=1369444953620480.0000 A.data=8962886466535424.0000\n",
      "Ep765: zero_grad(): A.grad= 0.0000 A.data=8962886466535424.0000 loss=40166666742168180416980071743488.0000\n",
      "            step(): A.grad=8962886466535424.0000 A.data=286188703318016.0000\n",
      "Ep766: zero_grad(): A.grad= 0.0000 A.data=286188703318016.0000 loss=40951989072493416591199830016.0000\n",
      "            step(): A.grad=286188703318016.0000 A.data=-9801936781967360.0000\n",
      "Ep767: zero_grad(): A.grad= 0.0000 A.data=-9801936781967360.0000 loss=48038983744257671886448640393216.0000\n",
      "            step(): A.grad=-9801936781967360.0000 A.data=-2275195574288384.0000\n",
      "Ep768: zero_grad(): A.grad= 0.0000 A.data=-2275195574288384.0000 loss=2588257591521767826676823097344.0000\n",
      "            step(): A.grad=-2275195574288384.0000 A.data=10327091023183872.0000\n",
      "Ep769: zero_grad(): A.grad= 0.0000 A.data=10327091023183872.0000 loss=53324402591909552179747343040512.0000\n",
      "            step(): A.grad=10327091023183872.0000 A.data=4568133658476544.0000\n",
      "Ep770: zero_grad(): A.grad= 0.0000 A.data=4568133658476544.0000 loss=10433922010529125374045232037888.0000\n",
      "            step(): A.grad=4568133658476544.0000 A.data=-10446174360174592.0000\n",
      "Ep771: zero_grad(): A.grad= 0.0000 A.data=-10446174360174592.0000 loss=54561278776473671580972725829632.0000\n",
      "            step(): A.grad=-10446174360174592.0000 A.data=-7114181681610752.0000\n",
      "Ep772: zero_grad(): A.grad= 0.0000 A.data=-7114181681610752.0000 loss=25305791362050440614370040872960.0000\n",
      "            step(): A.grad=-7114181681610752.0000 A.data=10067954171379712.0000\n",
      "Ep773: zero_grad(): A.grad= 0.0000 A.data=10067954171379712.0000 loss=50681850328440161934890703519744.0000\n",
      "            step(): A.grad=10067954171379712.0000 A.data=9839192402034688.0000\n",
      "Ep774: zero_grad(): A.grad= 0.0000 A.data=9839192402034688.0000 loss=48404853054305843259881717497856.0000\n",
      "            step(): A.grad=9839192402034688.0000 A.data=-9106911215484928.0000\n",
      "Ep775: zero_grad(): A.grad= 0.0000 A.data=-9106911215484928.0000 loss=41467915808775139592500208992256.0000\n",
      "            step(): A.grad=-9106911215484928.0000 A.data=-12644493241090048.0000\n",
      "Ep776: zero_grad(): A.grad= 0.0000 A.data=-12644493241090048.0000 loss=79941602833154993313221751865344.0000\n",
      "            step(): A.grad=-12644493241090048.0000 A.data=7488702829821952.0000\n",
      "Ep777: zero_grad(): A.grad= 0.0000 A.data=7488702829821952.0000 loss=28040335626837586451646772150272.0000\n",
      "            step(): A.grad=7488702829821952.0000 A.data=15406683345911808.0000\n",
      "Ep778: zero_grad(): A.grad= 0.0000 A.data=15406683345911808.0000 loss=118682946033997525933223242104832.0000\n",
      "            step(): A.grad=15406683345911808.0000 A.data=-5156237087866880.0000\n",
      "Ep779: zero_grad(): A.grad= 0.0000 A.data=-5156237087866880.0000 loss=13293390624886148917090226012160.0000\n",
      "            step(): A.grad=-5156237087866880.0000 A.data=-17978597809586176.0000\n",
      "Ep780: zero_grad(): A.grad= 0.0000 A.data=-17978597809586176.0000 loss=161614987067204664460693402025984.0000\n",
      "            step(): A.grad=-17978597809586176.0000 A.data=2076139946246144.0000\n",
      "Ep781: zero_grad(): A.grad= 0.0000 A.data=2076139946246144.0000 loss=2155178503879303499761896652800.0000\n",
      "            step(): A.grad=2076139946246144.0000 A.data=20191684935548928.0000\n",
      "Ep782: zero_grad(): A.grad= 0.0000 A.data=20191684935548928.0000 loss=203852069839451415719308668436480.0000\n",
      "            step(): A.grad=20191684935548928.0000 A.data=1754586482212864.0000\n",
      "Ep783: zero_grad(): A.grad= 0.0000 A.data=1754586482212864.0000 loss=1539286862720534563677076979712.0000\n",
      "            step(): A.grad=1754586482212864.0000 A.data=-21859938280144896.0000\n",
      "Ep784: zero_grad(): A.grad= 0.0000 A.data=-21859938280144896.0000 loss=238928443196956795450304313163776.0000\n",
      "            step(): A.grad=-21859938280144896.0000 A.data=-6302033430708224.0000\n",
      "Ep785: zero_grad(): A.grad= 0.0000 A.data=-6302033430708224.0000 loss=19857813095138259594465297563648.0000\n",
      "            step(): A.grad=-6302033430708224.0000 A.data=22785527354753024.0000\n",
      "Ep786: zero_grad(): A.grad= 0.0000 A.data=22785527354753024.0000 loss=259590126680144524255973783633920.0000\n",
      "            step(): A.grad=22785527354753024.0000 A.data=11489343533219840.0000\n",
      "Ep787: zero_grad(): A.grad= 0.0000 A.data=11489343533219840.0000 loss=66002509211977015963741686071296.0000\n",
      "            step(): A.grad=11489343533219840.0000 A.data=-22766212886822912.0000\n",
      "Ep788: zero_grad(): A.grad= 0.0000 A.data=-22766212886822912.0000 loss=259150232424309709908915097960448.0000\n",
      "            step(): A.grad=-22766212886822912.0000 A.data=-17191519282790400.0000\n",
      "Ep789: zero_grad(): A.grad= 0.0000 A.data=-17191519282790400.0000 loss=147774169443754799546083550691328.0000\n",
      "            step(): A.grad=-17191519282790400.0000 A.data=21604529459953664.0000\n",
      "Ep790: zero_grad(): A.grad= 0.0000 A.data=21604529459953664.0000 loss=233377848888624085476792160223232.0000\n",
      "            step(): A.grad=21604529459953664.0000 A.data=23231576888311808.0000\n",
      "Ep791: zero_grad(): A.grad= 0.0000 A.data=23231576888311808.0000 loss=269853075147708831084357020024832.0000\n",
      "            step(): A.grad=23231576888311808.0000 A.data=-19118664666054656.0000\n",
      "Ep792: zero_grad(): A.grad= 0.0000 A.data=-19118664666054656.0000 loss=182761672246408668657188195008512.0000\n",
      "            step(): A.grad=-19118664666054656.0000 A.data=-29378469657837568.0000\n",
      "Ep793: zero_grad(): A.grad= 0.0000 A.data=-29378469657837568.0000 loss=431547251691801532214510375403520.0000\n",
      "            step(): A.grad=-29378469657837568.0000 A.data=15154839348576256.0000\n",
      "Ep794: zero_grad(): A.grad= 0.0000 A.data=15154839348576256.0000 loss=114834577308121555339897770868736.0000\n",
      "            step(): A.grad=15154839348576256.0000 A.data=35347284493336576.0000\n",
      "Ep795: zero_grad(): A.grad= 0.0000 A.data=35347284493336576.0000 loss=624715260679387359637235356925952.0000\n",
      "            step(): A.grad=35347284493336576.0000 A.data=-9600862519296000.0000\n",
      "Ep796: zero_grad(): A.grad= 0.0000 A.data=-9600862519296000.0000 loss=46088280384540620084209550098432.0000\n",
      "            step(): A.grad=-9600862519296000.0000 A.data=-40802189312000000.0000\n",
      "Ep797: zero_grad(): A.grad= 0.0000 A.data=-40802189312000000.0000 loss=832409296773574066873760252690432.0000\n",
      "            step(): A.grad=-40802189312000000.0000 A.data=2400513056309248.0000\n",
      "Ep798: zero_grad(): A.grad= 0.0000 A.data=2400513056309248.0000 loss=2881231394730271193790760878080.0000\n",
      "            step(): A.grad=2400513056309248.0000 A.data=45362512572448768.0000\n",
      "Ep799: zero_grad(): A.grad= 0.0000 A.data=45362512572448768.0000 loss=1028878737103429092817106836127744.0000\n",
      "            step(): A.grad=45362512572448768.0000 A.data=6431941159026688.0000\n",
      "Ep800: zero_grad(): A.grad= 0.0000 A.data=6431941159026688.0000 loss=20684933544550557353957419646976.0000\n",
      "            step(): A.grad=6431941159026688.0000 A.data=-48612376886378496.0000\n",
      "Ep801: zero_grad(): A.grad= 0.0000 A.data=-48612376886378496.0000 loss=1181581581056874052504066694578176.0000\n",
      "            step(): A.grad=-48612376886378496.0000 A.data=-16797612799688704.0000\n",
      "Ep802: zero_grad(): A.grad= 0.0000 A.data=-16797612799688704.0000 loss=141079902295846979622199162306560.0000\n",
      "            step(): A.grad=-16797612799688704.0000 A.data=50114090726588416.0000\n",
      "Ep803: zero_grad(): A.grad= 0.0000 A.data=50114090726588416.0000 loss=1255711055405584185426637058211840.0000\n",
      "            step(): A.grad=50114090726588416.0000 A.data=28500200385413120.0000\n",
      "Ep804: zero_grad(): A.grad= 0.0000 A.data=28500200385413120.0000 loss=406130717888971113109220549984256.0000\n",
      "            step(): A.grad=28500200385413120.0000 A.data=-49425461440151552.0000\n",
      "Ep805: zero_grad(): A.grad= 0.0000 A.data=-49425461440151552.0000 loss=1221438143819201245162184535703552.0000\n",
      "            step(): A.grad=-49425461440151552.0000 A.data=-41235310993997824.0000\n",
      "Ep806: zero_grad(): A.grad= 0.0000 A.data=-41235310993997824.0000 loss=850175438504873291216440671600640.0000\n",
      "            step(): A.grad=-41235310993997824.0000 A.data=46120939372412928.0000\n",
      "Ep807: zero_grad(): A.grad= 0.0000 A.data=46120939372412928.0000 loss=1063570536650605223632078349795328.0000\n",
      "            step(): A.grad=46120939372412928.0000 A.data=54583034262847488.0000\n",
      "Ep808: zero_grad(): A.grad= 0.0000 A.data=54583034262847488.0000 loss=1489653791808384680990119125057536.0000\n",
      "            step(): A.grad=54583034262847488.0000 A.data=-39816421303123968.0000\n",
      "Ep809: zero_grad(): A.grad= 0.0000 A.data=-39816421303123968.0000 loss=792673665278834564801246619762688.0000\n",
      "            step(): A.grad=-39816421303123968.0000 A.data=-68004622379253760.0000\n",
      "Ep810: zero_grad(): A.grad= 0.0000 A.data=-68004622379253760.0000 loss=2312314310772586378199048774483968.0000\n",
      "            step(): A.grad=-68004622379253760.0000 A.data=30197141964062720.0000\n",
      "Ep811: zero_grad(): A.grad= 0.0000 A.data=30197141964062720.0000 loss=455933703325067831926683357675520.0000\n",
      "            step(): A.grad=30197141964062720.0000 A.data=80844513009991680.0000\n",
      "Ep812: zero_grad(): A.grad= 0.0000 A.data=80844513009991680.0000 loss=3267917520040654884474464099106816.0000\n",
      "            step(): A.grad=80844513009991680.0000 A.data=-17047953558470656.0000\n",
      "Ep813: zero_grad(): A.grad= 0.0000 A.data=-17047953558470656.0000 loss=145316355552632360014672111337472.0000\n",
      "            step(): A.grad=-17047953558470656.0000 A.data=-92338549868724224.0000\n",
      "Ep814: zero_grad(): A.grad= 0.0000 A.data=-92338549868724224.0000 loss=4263203980465550630169511387463680.0000\n",
      "            step(): A.grad=-92338549868724224.0000 A.data=285031209631744.0000\n",
      "Ep815: zero_grad(): A.grad= 0.0000 A.data=285031209631744.0000 loss=40621395084493644208369827840.0000\n",
      "            step(): A.grad=285031209631744.0000 A.data=101629405943562240.0000\n",
      "Ep816: zero_grad(): A.grad= 0.0000 A.data=101629405943562240.0000 loss=5164268039135347691036367808102400.0000\n",
      "            step(): A.grad=101629405943562240.0000 A.data=20012357166039040.0000\n",
      "Ep817: zero_grad(): A.grad= 0.0000 A.data=20012357166039040.0000 loss=200247227130678616026936009293824.0000\n",
      "            step(): A.grad=20012357166039040.0000 A.data=-107789883694645248.0000\n",
      "Ep818: zero_grad(): A.grad= 0.0000 A.data=-107789883694645248.0000 loss=5809329502016036145849841311809536.0000\n",
      "            step(): A.grad=-107789883694645248.0000 A.data=-43571579929493504.0000\n",
      "Ep819: zero_grad(): A.grad= 0.0000 A.data=-43571579929493504.0000 loss=949241280663676026370174362845184.0000\n",
      "            step(): A.grad=-43571579929493504.0000 A.data=109854560373178368.0000\n",
      "Ep820: zero_grad(): A.grad= 0.0000 A.data=109854560373178368.0000 loss=6034012524296234452293345110654976.0000\n",
      "            step(): A.grad=109854560373178368.0000 A.data=69899652144562176.0000\n",
      "Ep821: zero_grad(): A.grad= 0.0000 A.data=69899652144562176.0000 loss=2442980738829217194285063685013504.0000\n",
      "            step(): A.grad=69899652144562176.0000 A.data=-106860091994537984.0000\n",
      "Ep822: zero_grad(): A.grad= 0.0000 A.data=-106860091994537984.0000 loss=5709539774419261284580360357675008.0000\n",
      "            step(): A.grad=-106860091994537984.0000 A.data=-98261636187422720.0000\n",
      "Ep823: zero_grad(): A.grad= 0.0000 A.data=-98261636187422720.0000 loss=4827674618333731365051845729845248.0000\n",
      "            step(): A.grad=-98261636187422720.0000 A.data=97893780828454912.0000\n",
      "Ep824: zero_grad(): A.grad= 0.0000 A.data=97893780828454912.0000 loss=4791596093828798242320185653460992.0000\n",
      "            step(): A.grad=97893780828454912.0000 A.data=127666545663934464.0000\n",
      "Ep825: zero_grad(): A.grad= 0.0000 A.data=127666545663934464.0000 loss=8149373514926110131534096106520576.0000\n",
      "            step(): A.grad=127666545663934464.0000 A.data=-82149839470592000.0000\n",
      "Ep826: zero_grad(): A.grad= 0.0000 A.data=-82149839470592000.0000 loss=3374298040406585110327507990609920.0000\n",
      "            step(): A.grad=-82149839470592000.0000 A.data=-156863166406459392.0000\n",
      "Ep827: zero_grad(): A.grad= 0.0000 A.data=-156863166406459392.0000 loss=12303026920070130498311615670124544.0000\n",
      "            step(): A.grad=-156863166406459392.0000 A.data=58992183264411648.0000\n",
      "Ep828: zero_grad(): A.grad= 0.0000 A.data=58992183264411648.0000 loss=1740038770441721142028544178651136.0000\n",
      "            step(): A.grad=58992183264411648.0000 A.data=184347916264013824.0000\n",
      "Ep829: zero_grad(): A.grad= 0.0000 A.data=184347916264013824.0000 loss=16992077631774704622870394918928384.0000\n",
      "            step(): A.grad=184347916264013824.0000 A.data=-28021809748115456.0000\n",
      "Ep830: zero_grad(): A.grad= 0.0000 A.data=-28021809748115456.0000 loss=392610903920551881450112080150528.0000\n",
      "            step(): A.grad=-28021809748115456.0000 A.data=-208387088737894400.0000\n",
      "Ep831: zero_grad(): A.grad= 0.0000 A.data=-208387088737894400.0000 loss=21712589351637677401735460510760960.0000\n",
      "            step(): A.grad=-208387088737894400.0000 A.data=-10853433896599552.0000\n",
      "Ep832: zero_grad(): A.grad= 0.0000 A.data=-10853433896599552.0000 loss=58898512925285405919965880516608.0000\n",
      "            step(): A.grad=-10853433896599552.0000 A.data=227055112550350848.0000\n",
      "Ep833: zero_grad(): A.grad= 0.0000 A.data=227055112550350848.0000 loss=25777011419779987915608113750736896.0000\n",
      "            step(): A.grad=227055112550350848.0000 A.data=57349822130159616.0000\n",
      "Ep834: zero_grad(): A.grad= 0.0000 A.data=57349822130159616.0000 loss=1644501057394881740658272991444992.0000\n",
      "            step(): A.grad=57349822130159616.0000 A.data=-238290678277210112.0000\n",
      "Ep835: zero_grad(): A.grad= 0.0000 A.data=-238290678277210112.0000 loss=28391223870100653998844689936023552.0000\n",
      "            step(): A.grad=-238290678277210112.0000 A.data=-110742939998617600.0000\n",
      "Ep836: zero_grad(): A.grad= 0.0000 A.data=-110742939998617600.0000 loss=6131999192225790157192435490357248.0000\n",
      "            step(): A.grad=-110742939998617600.0000 A.data=239971161541181440.0000\n",
      "Ep837: zero_grad(): A.grad= 0.0000 A.data=239971161541181440.0000 loss=28793079013893317002161344045121536.0000\n",
      "            step(): A.grad=239971161541181440.0000 A.data=169811462011748352.0000\n",
      "Ep838: zero_grad(): A.grad= 0.0000 A.data=169811462011748352.0000 loss=14417965773146609862878626064629760.0000\n",
      "            step(): A.grad=169811462011748352.0000 A.data=-230005978421002240.0000\n",
      "Ep839: zero_grad(): A.grad= 0.0000 A.data=-230005978421002240.0000 loss=26451374304420541678838312075264000.0000\n",
      "            step(): A.grad=-230005978421002240.0000 A.data=-232793807333097472.0000\n",
      "Ep840: zero_grad(): A.grad= 0.0000 A.data=-232793807333097472.0000 loss=27096477238292546193890994699632640.0000\n",
      "            step(): A.grad=-232793807333097472.0000 A.data=206447790744666112.0000\n",
      "Ep841: zero_grad(): A.grad= 0.0000 A.data=206447790744666112.0000 loss=21310345494672678789012488076656640.0000\n",
      "            step(): A.grad=206447790744666112.0000 A.data=297362782293065728.0000\n",
      "Ep842: zero_grad(): A.grad= 0.0000 A.data=297362782293065728.0000 loss=44212310497854570997462780168110080.0000\n",
      "            step(): A.grad=297362782293065728.0000 A.data=-167619997898637312.0000\n",
      "Ep843: zero_grad(): A.grad= 0.0000 A.data=-167619997898637312.0000 loss=14048231459553284621554779531771904.0000\n",
      "            step(): A.grad=-167619997898637312.0000 A.data=-360623115077681152.0000\n",
      "Ep844: zero_grad(): A.grad= 0.0000 A.data=-360623115077681152.0000 loss=65024513872478969905376960253526016.0000\n",
      "            step(): A.grad=-360623115077681152.0000 A.data=112257388416860160.0000\n",
      "Ep845: zero_grad(): A.grad= 0.0000 A.data=112257388416860160.0000 loss=6300860403284512453590050530131968.0000\n",
      "            step(): A.grad=112257388416860160.0000 A.data=419136924884664320.0000\n",
      "Ep846: zero_grad(): A.grad= 0.0000 A.data=419136924884664320.0000 loss=87837877031169128876792172226019328.0000\n",
      "            step(): A.grad=419136924884664320.0000 A.data=-39655742281613312.0000\n",
      "Ep847: zero_grad(): A.grad= 0.0000 A.data=-39655742281613312.0000 loss=786288912154967760697187205382144.0000\n",
      "            step(): A.grad=-39655742281613312.0000 A.data=-468981807061139456.0000\n",
      "Ep848: zero_grad(): A.grad= 0.0000 A.data=-468981807061139456.0000 loss=109971967635022928266806935947313152.0000\n",
      "            step(): A.grad=-468981807061139456.0000 A.data=-50175044902453248.0000\n",
      "Ep849: zero_grad(): A.grad= 0.0000 A.data=-50175044902453248.0000 loss=1258767529362579789325362995920896.0000\n",
      "            step(): A.grad=-50175044902453248.0000 A.data=505844927247155200.0000\n",
      "Ep850: zero_grad(): A.grad= 0.0000 A.data=505844927247155200.0000 loss=127939548207454708973199215250374656.0000\n",
      "            step(): A.grad=505844927247155200.0000 A.data=156361582945763328.0000\n",
      "Ep851: zero_grad(): A.grad= 0.0000 A.data=156361582945763328.0000 loss=12224472196937237407587616843366400.0000\n",
      "            step(): A.grad=156361582945763328.0000 A.data=-525157093074796544.0000\n",
      "Ep852: zero_grad(): A.grad= 0.0000 A.data=-525157093074796544.0000 loss=137894982775225222879600378715832320.0000\n",
      "            step(): A.grad=-525157093074796544.0000 A.data=-277029187343089664.0000\n",
      "Ep853: zero_grad(): A.grad= 0.0000 A.data=-277029187343089664.0000 loss=38372585854053960909723340659228672.0000\n",
      "            step(): A.grad=-277029187343089664.0000 A.data=522266992401448960.0000\n",
      "Ep854: zero_grad(): A.grad= 0.0000 A.data=522266992401448960.0000 loss=136381407958552716974213315088613376.0000\n",
      "            step(): A.grad=522266992401448960.0000 A.data=409185520019570688.0000\n",
      "Ep855: zero_grad(): A.grad= 0.0000 A.data=409185520019570688.0000 loss=83716398306616155186049418350559232.0000\n",
      "            step(): A.grad=409185520019570688.0000 A.data=-492656594509627392.0000\n",
      "Ep856: zero_grad(): A.grad= 0.0000 A.data=-492656594509627392.0000 loss=121355262051145828348361147678720000.0000\n",
      "            step(): A.grad=-492656594509627392.0000 A.data=-548635445899034624.0000\n",
      "Ep857: zero_grad(): A.grad= 0.0000 A.data=-548635445899034624.0000 loss=150500431019252536066788201039134720.0000\n",
      "            step(): A.grad=-548635445899034624.0000 A.data=432195171652730880.0000\n",
      "Ep858: zero_grad(): A.grad= 0.0000 A.data=432195171652730880.0000 loss=93396336746283943424553431114711040.0000\n",
      "            step(): A.grad=432195171652730880.0000 A.data=689938117590777856.0000\n",
      "Ep859: zero_grad(): A.grad= 0.0000 A.data=689938117590777856.0000 loss=238007302690957382824156745833644032.0000\n",
      "            step(): A.grad=689938117590777856.0000 A.data=-337427061863874560.0000\n",
      "Ep860: zero_grad(): A.grad= 0.0000 A.data=-337427061863874560.0000 loss=56928509809556511445564177740988416.0000\n",
      "            step(): A.grad=-337427061863874560.0000 A.data=-826417334850682880.0000\n",
      "Ep861: zero_grad(): A.grad= 0.0000 A.data=-826417334850682880.0000 loss=341482808587818492484348057678249984.0000\n",
      "            step(): A.grad=-826417334850682880.0000 A.data=205886301080125440.0000\n",
      "Ep862: zero_grad(): A.grad= 0.0000 A.data=205886301080125440.0000 loss=21194585721599102879506670970470400.0000\n",
      "            step(): A.grad=205886301080125440.0000 A.data=950236225472561152.0000\n",
      "Ep863: zero_grad(): A.grad= 0.0000 A.data=950236225472561152.0000 loss=451474434710665272589920391718240256.0000\n",
      "            step(): A.grad=950236225472561152.0000 A.data=-36427644861939712.0000\n",
      "Ep864: zero_grad(): A.grad= 0.0000 A.data=-36427644861939712.0000 loss=663486652940402233480003343876096.0000\n",
      "            step(): A.grad=-36427644861939712.0000 A.data=-1052545370120257536.0000\n",
      "Ep865: zero_grad(): A.grad= 0.0000 A.data=-1052545370120257536.0000 loss=553925876992948258554546351232778240.0000\n",
      "            step(): A.grad=-1052545370120257536.0000 A.data=-170438733395394560.0000\n",
      "Ep866: zero_grad(): A.grad= 0.0000 A.data=-170438733395394560.0000 loss=14524681156293166784096030169235456.0000\n",
      "            step(): A.grad=-170438733395394560.0000 A.data=1123712153581256704.0000\n",
      "Ep867: zero_grad(): A.grad= 0.0000 A.data=1123712153581256704.0000 loss=631364513000502907554864924362539008.0000\n",
      "            step(): A.grad=1123712153581256704.0000 A.data=412225119914557440.0000\n",
      "Ep868: zero_grad(): A.grad= 0.0000 A.data=412225119914557440.0000 loss=84964776656312789787426491990016000.0000\n",
      "            step(): A.grad=412225119914557440.0000 A.data=-1153638386188156928.0000\n",
      "Ep869: zero_grad(): A.grad= 0.0000 A.data=-1153638386188156928.0000 loss=665440743768294284814692161261928448.0000\n",
      "            step(): A.grad=-1153638386188156928.0000 A.data=-684175370991173632.0000\n",
      "Ep870: zero_grad(): A.grad= 0.0000 A.data=-684175370991173632.0000 loss=234047974304510165383341378845540352.0000\n",
      "            step(): A.grad=-684175370991173632.0000 A.data=1132167123120947200.0000\n",
      "Ep871: zero_grad(): A.grad= 0.0000 A.data=1132167123120947200.0000 loss=640901206922344905870999809664483328.0000\n",
      "            step(): A.grad=1132167123120947200.0000 A.data=979026387690061824.0000\n",
      "Ep872: zero_grad(): A.grad= 0.0000 A.data=979026387690061824.0000 loss=479246322130871607978754149401493504.0000\n",
      "            step(): A.grad=979026387690061824.0000 A.data=-1049578475431657472.0000\n",
      "Ep873: zero_grad(): A.grad= 0.0000 A.data=-1049578475431657472.0000 loss=550807496130468071359033258119528448.0000\n",
      "            step(): A.grad=-1049578475431657472.0000 A.data=-1286844907087986688.0000\n",
      "Ep874: zero_grad(): A.grad= 0.0000 A.data=-1286844907087986688.0000 loss=827984925069908942764256555595464704.0000\n",
      "            step(): A.grad=-1286844907087986688.0000 A.data=897167265965801472.0000\n",
      "Ep875: zero_grad(): A.grad= 0.0000 A.data=897167265965801472.0000 loss=402454544456164670162718065854251008.0000\n",
      "            step(): A.grad=897167265965801472.0000 A.data=1594963112123957248.0000\n",
      "Ep876: zero_grad(): A.grad= 0.0000 A.data=1594963112123957248.0000 loss=1271953640190704126200083764406648832.0000\n",
      "            step(): A.grad=1594963112123957248.0000 A.data=-667891603783811072.0000\n",
      "Ep877: zero_grad(): A.grad= 0.0000 A.data=-667891603783811072.0000 loss=223039597456925078430322016280117248.0000\n",
      "            step(): A.grad=-667891603783811072.0000 A.data=-1888037661629743104.0000\n",
      "Ep878: zero_grad(): A.grad= 0.0000 A.data=-1888037661629743104.0000 loss=1782343047670845274264445763350167552.0000\n",
      "            step(): A.grad=-1888037661629743104.0000 A.data=357073135628976128.0000\n",
      "Ep879: zero_grad(): A.grad= 0.0000 A.data=357073135628976128.0000 loss=63750614150932427904252566269067264.0000\n",
      "            step(): A.grad=357073135628976128.0000 A.data=2148256329796419584.0000\n",
      "Ep880: zero_grad(): A.grad= 0.0000 A.data=2148256329796419584.0000 loss=2307502644575870665729021786779025408.0000\n",
      "            step(): A.grad=2148256329796419584.0000 A.data=36870885486886912.0000\n",
      "Ep881: zero_grad(): A.grad= 0.0000 A.data=36870885486886912.0000 loss=679731134249662359456031195529216.0000\n",
      "            step(): A.grad=36870885486886912.0000 A.data=-2355708060556591104.0000\n",
      "Ep882: zero_grad(): A.grad= 0.0000 A.data=-2355708060556591104.0000 loss=2774680219939106901426488979769786368.0000\n",
      "            step(): A.grad=-2355708060556591104.0000 A.data=-511699517427417088.0000\n",
      "Ep883: zero_grad(): A.grad= 0.0000 A.data=-511699517427417088.0000 loss=130918200301820676726323894414213120.0000\n",
      "            step(): A.grad=-511699517427417088.0000 A.data=2488939183029092352.0000\n",
      "Ep884: zero_grad(): A.grad= 0.0000 A.data=2488939183029092352.0000 loss=3097409167862661425352082029595852800.0000\n",
      "            step(): A.grad=2488939183029092352.0000 A.data=1060657360751558656.0000\n",
      "Ep885: zero_grad(): A.grad= 0.0000 A.data=1060657360751558656.0000 loss=562497017298228917388428716411977728.0000\n",
      "            step(): A.grad=1060657360751558656.0000 A.data=-2525701629181689856.0000\n",
      "Ep886: zero_grad(): A.grad= 0.0000 A.data=-2525701629181689856.0000 loss=3189584479782356783937812558119960576.0000\n",
      "            step(): A.grad=-2525701629181689856.0000 A.data=-1671863642565378048.0000\n",
      "Ep887: zero_grad(): A.grad= 0.0000 A.data=-1671863642565378048.0000 loss=1397564028973044177893665775412051968.0000\n",
      "            step(): A.grad=-1671863642565378048.0000 A.data=2443898788708876288.0000\n",
      "Ep888: zero_grad(): A.grad= 0.0000 A.data=2443898788708876288.0000 loss=2986320508987560884096020689247535104.0000\n",
      "            step(): A.grad=2443898788708876288.0000 A.data=2327829943234330624.0000\n",
      "Ep889: zero_grad(): A.grad= 0.0000 A.data=2327829943234330624.0000 loss=2709396214027353087249408764692922368.0000\n",
      "            step(): A.grad=2327829943234330624.0000 A.data=-2222722678932897792.0000\n",
      "Ep890: zero_grad(): A.grad= 0.0000 A.data=-2222722678932897792.0000 loss=2470248065410271555096073782746415104.0000\n",
      "            step(): A.grad=-2222722678932897792.0000 A.data=-3005157418368761856.0000\n",
      "Ep891: zero_grad(): A.grad= 0.0000 A.data=-3005157418368761856.0000 loss=4515485526566973668777391520837795840.0000\n",
      "            step(): A.grad=-3005157418368761856.0000 A.data=1843963188274528256.0000\n",
      "Ep892: zero_grad(): A.grad= 0.0000 A.data=1843963188274528256.0000 loss=1700100095116588150096792278615982080.0000\n",
      "            step(): A.grad=1843963188274528256.0000 A.data=3674466155201822720.0000\n",
      "Ep893: zero_grad(): A.grad= 0.0000 A.data=3674466155201822720.0000 loss=6750850453804631569541465979572191232.0000\n",
      "            step(): A.grad=3674466155201822720.0000 A.data=-1293466028671500288.0000\n",
      "Ep894: zero_grad(): A.grad= 0.0000 A.data=-1293466028671500288.0000 loss=836527147095871895114917277232791552.0000\n",
      "            step(): A.grad=-1293466028671500288.0000 A.data=-4300606416260956160.0000\n",
      "Ep895: zero_grad(): A.grad= 0.0000 A.data=-4300606416260956160.0000 loss=9247607470110550829352963632661528576.0000\n",
      "            step(): A.grad=-4300606416260956160.0000 A.data=562691293310877696.0000\n",
      "Ep896: zero_grad(): A.grad= 0.0000 A.data=562691293310877696.0000 loss=158310743279908714466759763663257600.0000\n",
      "            step(): A.grad=562691293310877696.0000 A.data=4843205234085855232.0000\n",
      "Ep897: zero_grad(): A.grad= 0.0000 A.data=4843205234085855232.0000 loss=11728318565118781161400316016577216512.0000\n",
      "            step(): A.grad=4843205234085855232.0000 A.data=349680706638577664.0000\n",
      "Ep898: zero_grad(): A.grad= 0.0000 A.data=349680706638577664.0000 loss=61138298224751947023597135542091776.0000\n",
      "            step(): A.grad=349680706638577664.0000 A.data=-5257589726117888000.0000\n",
      "Ep899: zero_grad(): A.grad= 0.0000 A.data=-5257589726117888000.0000 loss=13821124773505372611901472729538232320.0000\n",
      "            step(): A.grad=-5257589726117888000.0000 A.data=-1436167244794036224.0000\n",
      "Ep900: zero_grad(): A.grad= 0.0000 A.data=-1436167244794036224.0000 loss=1031288193033311917717494822267256832.0000\n",
      "            step(): A.grad=-1436167244794036224.0000 A.data=5496115579624357888.0000\n",
      "Ep901: zero_grad(): A.grad= 0.0000 A.data=5496115579624357888.0000 loss=15103643647623678916048330766810087424.0000\n",
      "            step(): A.grad=5496115579624357888.0000 A.data=2679007085198311424.0000\n",
      "Ep902: zero_grad(): A.grad= 0.0000 A.data=2679007085198311424.0000 loss=3588539476686185141176854990915895296.0000\n",
      "            step(): A.grad=2679007085198311424.0000 A.data=-5509925445669224448.0000\n",
      "Ep903: zero_grad(): A.grad= 0.0000 A.data=-5509925445669224448.0000 loss=15179639301107361268668058123972378624.0000\n",
      "            step(): A.grad=-5509925445669224448.0000 A.data=-4048893295168847872.0000\n",
      "Ep904: zero_grad(): A.grad= 0.0000 A.data=-4048893295168847872.0000 loss=8196768222641756415311847563313807360.0000\n",
      "            step(): A.grad=-4048893295168847872.0000 A.data=5251139990909353984.0000\n",
      "Ep905: zero_grad(): A.grad= 0.0000 A.data=5251139990909353984.0000 loss=13787235402358871127081859866045710336.0000\n",
      "            step(): A.grad=5251139990909353984.0000 A.data=5504010622867603456.0000\n",
      "Ep906: zero_grad(): A.grad= 0.0000 A.data=5504010622867603456.0000 loss=15147067018934496914196600335110242304.0000\n",
      "            step(): A.grad=5504010622867603456.0000 A.data=-4675451645524443136.0000\n",
      "Ep907: zero_grad(): A.grad= 0.0000 A.data=-4675451645524443136.0000 loss=10929924328781837263290859410360893440.0000\n",
      "            step(): A.grad=-4675451645524443136.0000 A.data=-6989501959283671040.0000\n",
      "Ep908: zero_grad(): A.grad= 0.0000 A.data=-6989501959283671040.0000 loss=24426569845797390223520622517122236416.0000\n",
      "            step(): A.grad=-6989501959283671040.0000 A.data=3745096033391083520.0000\n",
      "Ep909: zero_grad(): A.grad= 0.0000 A.data=3745096033391083520.0000 loss=7012871931395906244486732287068602368.0000\n",
      "            step(): A.grad=3745096033391083520.0000 A.data=8437471361890254848.0000\n",
      "Ep910: zero_grad(): A.grad= 0.0000 A.data=8437471361890254848.0000 loss=35595461524529451467746428442134970368.0000\n",
      "            step(): A.grad=8437471361890254848.0000 A.data=-2432110374791675904.0000\n",
      "Ep911: zero_grad(): A.grad= 0.0000 A.data=-2432110374791675904.0000 loss=2957580334579186467105287434175250432.0000\n",
      "            step(): A.grad=-2432110374791675904.0000 A.data=-9767641287720173568.0000\n",
      "Ep912: zero_grad(): A.grad= 0.0000 A.data=-9767641287720173568.0000 loss=47703405950199780944471765161032548352.0000\n",
      "            step(): A.grad=-9767641287720173568.0000 A.data=721792000239599616.0000\n",
      "Ep913: zero_grad(): A.grad= 0.0000 A.data=721792000239599616.0000 loss=260491839264652401945432152833261568.0000\n",
      "            step(): A.grad=721792000239599616.0000 A.data=10888764916051738624.0000\n",
      "Ep914: zero_grad(): A.grad= 0.0000 A.data=10888764916051738624.0000 loss=59282603169310114111997614999342153728.0000\n",
      "            step(): A.grad=10888764916051738624.0000 A.data=1383781563044462592.0000\n",
      "Ep915: zero_grad(): A.grad= 0.0000 A.data=1383781563044462592.0000 loss=957425678597163618123136042423812096.0000\n",
      "            step(): A.grad=1383781563044462592.0000 A.data=-11700885095048019968.0000\n",
      "Ep916: zero_grad(): A.grad= 0.0000 A.data=-11700885095048019968.0000 loss=68455358406778388451651001301132640256.0000\n",
      "            step(): A.grad=-11700885095048019968.0000 A.data=-3862338057772466176.0000\n",
      "Ep917: zero_grad(): A.grad= 0.0000 A.data=-3862338057772466176.0000 loss=7458827609604396663245067984610263040.0000\n",
      "            step(): A.grad=-3862338057772466176.0000 A.data=12098506982558793728.0000\n",
      "Ep918: zero_grad(): A.grad= 0.0000 A.data=12098506982558793728.0000 loss=73186935260563867473583929830578061312.0000\n",
      "            step(): A.grad=12098506982558793728.0000 A.data=6668273589914959872.0000\n",
      "Ep919: zero_grad(): A.grad= 0.0000 A.data=6668273589914959872.0000 loss=22232935976319245634653819527908818944.0000\n",
      "            step(): A.grad=6668273589914959872.0000 A.data=-11974704172294471680.0000\n",
      "Ep920: zero_grad(): A.grad= 0.0000 A.data=-11974704172294471680.0000 loss=71696771415176376508783707318156001280.0000\n",
      "            step(): A.grad=-11974704172294471680.0000 A.data=-9730042388096745472.0000\n",
      "Ep921: zero_grad(): A.grad= 0.0000 A.data=-9730042388096745472.0000 loss=47336862243842587956570190435791667200.0000\n",
      "            step(): A.grad=-9730042388096745472.0000 A.data=11226165452197593088.0000\n",
      "Ep922: zero_grad(): A.grad= 0.0000 A.data=11226165452197593088.0000 loss=63013395227227410586036926282207330304.0000\n",
      "            step(): A.grad=11226165452197593088.0000 A.data=12948282136271519744.0000\n",
      "Ep923: zero_grad(): A.grad= 0.0000 A.data=12948282136271519744.0000 loss=83829004026347078860841720870468583424.0000\n",
      "            step(): A.grad=12948282136271519744.0000 A.data=-9759124470651420672.0000\n",
      "Ep924: zero_grad(): A.grad= 0.0000 A.data=-9759124470651420672.0000 loss=47620253141427210008651187417572704256.0000\n",
      "            step(): A.grad=-9759124470651420672.0000 A.data=-16194937662954536960.0000\n",
      "Ep925: zero_grad(): A.grad= 0.0000 A.data=-16194937662954536960.0000 loss=131137998239394249422249407782993264640.0000\n",
      "            step(): A.grad=-16194937662954536960.0000 A.data=7496050264734957568.0000\n",
      "Ep926: zero_grad(): A.grad= 0.0000 A.data=7496050264734957568.0000 loss=28095383930568328105661956986870169600.0000\n",
      "            step(): A.grad=7496050264734957568.0000 A.data=19313643021513261056.0000\n",
      "Ep927: zero_grad(): A.grad= 0.0000 A.data=19313643021513261056.0000 loss=    inf\n",
      "            step(): A.grad=19313643021513261056.0000 A.data=-4382926027198824448.0000\n",
      "Ep928: zero_grad(): A.grad= 0.0000 A.data=-4382926027198824448.0000 loss=9605020289194299880875557604714086400.0000\n",
      "            step(): A.grad=-4382926027198824448.0000 A.data=-22121593408713654272.0000\n",
      "Ep929: zero_grad(): A.grad= 0.0000 A.data=-22121593408713654272.0000 loss=    inf\n",
      "            step(): A.grad=-22121593408713654272.0000 A.data=396899508371324928.0000\n",
      "Ep930: zero_grad(): A.grad= 0.0000 A.data=396899508371324928.0000 loss=78764608438913691237326323875577856.0000\n",
      "            step(): A.grad=396899508371324928.0000 A.data=24413133970673238016.0000\n",
      "Ep931: zero_grad(): A.grad= 0.0000 A.data=24413133970673238016.0000 loss=    inf\n",
      "            step(): A.grad=24413133970673238016.0000 A.data=4446037994633166848.0000\n",
      "Ep932: zero_grad(): A.grad= 0.0000 A.data=4446037994633166848.0000 loss=9883627073413660595195306028598034432.0000\n",
      "            step(): A.grad=4446037994633166848.0000 A.data=-25965239768813928448.0000\n",
      "Ep933: zero_grad(): A.grad= 0.0000 A.data=-25965239768813928448.0000 loss=    inf\n",
      "            step(): A.grad=-25965239768813928448.0000 A.data=-10083691507077873664.0000\n",
      "Ep934: zero_grad(): A.grad= 0.0000 A.data=-10083691507077873664.0000 loss=50840415255162972028097202702061142016.0000\n",
      "            step(): A.grad=-10083691507077873664.0000 A.data=26545023245256491008.0000\n",
      "Ep935: zero_grad(): A.grad= 0.0000 A.data=26545023245256491008.0000 loss=    inf\n",
      "            step(): A.grad=26545023245256491008.0000 A.data=16401069704883470336.0000\n",
      "Ep936: zero_grad(): A.grad= 0.0000 A.data=16401069704883470336.0000 loss=134497546142528706633766394565132025856.0000\n",
      "            step(): A.grad=16401069704883470336.0000 A.data=-25919308770075213824.0000\n",
      "Ep937: zero_grad(): A.grad= 0.0000 A.data=-25919308770075213824.0000 loss=    inf\n",
      "            step(): A.grad=-25919308770075213824.0000 A.data=-23225041288117092352.0000\n",
      "Ep938: zero_grad(): A.grad= 0.0000 A.data=-23225041288117092352.0000 loss=    inf\n",
      "            step(): A.grad=-23225041288117092352.0000 A.data=23866232488970944512.0000\n",
      "Ep939: zero_grad(): A.grad= 0.0000 A.data=23866232488970944512.0000 loss=    inf\n",
      "            step(): A.grad=23866232488970944512.0000 A.data=30320792354527641600.0000\n",
      "Ep940: zero_grad(): A.grad= 0.0000 A.data=30320792354527641600.0000 loss=    inf\n",
      "            step(): A.grad=30320792354527641600.0000 A.data=-20188695947548557312.0000\n",
      "Ep941: zero_grad(): A.grad= 0.0000 A.data=-20188695947548557312.0000 loss=    inf\n",
      "            step(): A.grad=-20188695947548557312.0000 A.data=-37390612538708721664.0000\n",
      "Ep942: zero_grad(): A.grad= 0.0000 A.data=-37390612538708721664.0000 loss=    inf\n",
      "            step(): A.grad=-37390612538708721664.0000 A.data=14729440395733762048.0000\n",
      "Ep943: zero_grad(): A.grad= 0.0000 A.data=14729440395733762048.0000 loss=108478206276460152101229603384738185216.0000\n",
      "            step(): A.grad=14729440395733762048.0000 A.data=44075564070749601792.0000\n",
      "Ep944: zero_grad(): A.grad= 0.0000 A.data=44075564070749601792.0000 loss=    inf\n",
      "            step(): A.grad=44075564070749601792.0000 A.data=-7387271181352566784.0000\n",
      "Ep945: zero_grad(): A.grad= 0.0000 A.data=-7387271181352566784.0000 loss=27285887610274585374454192253981163520.0000\n",
      "            step(): A.grad=-7387271181352566784.0000 A.data=-49960576913118330880.0000\n",
      "Ep946: zero_grad(): A.grad= 0.0000 A.data=-49960576913118330880.0000 loss=    inf\n",
      "            step(): A.grad=-49960576913118330880.0000 A.data=-1866117522940493824.0000\n",
      "Ep947: zero_grad(): A.grad= 0.0000 A.data=-1866117522940493824.0000 loss=1741197327575987347293315396534272000.0000\n",
      "            step(): A.grad=-1866117522940493824.0000 A.data=54583407581404856320.0000\n",
      "Ep948: zero_grad(): A.grad= 0.0000 A.data=54583407581404856320.0000 loss=    inf\n",
      "            step(): A.grad=54583407581404856320.0000 A.data=12969412550734118912.0000\n",
      "Ep949: zero_grad(): A.grad= 0.0000 A.data=12969412550734118912.0000 loss=84102831767803579150317826723268263936.0000\n",
      "            step(): A.grad=12969412550734118912.0000 A.data=-57447868468226424832.0000\n",
      "Ep950: zero_grad(): A.grad= 0.0000 A.data=-57447868468226424832.0000 loss=    inf\n",
      "            step(): A.grad=-57447868468226424832.0000 A.data=-25755925740234211328.0000\n",
      "Ep951: zero_grad(): A.grad= 0.0000 A.data=-25755925740234211328.0000 loss=    inf\n",
      "            step(): A.grad=-25755925740234211328.0000 A.data=58041468407783620608.0000\n",
      "Ep952: zero_grad(): A.grad= 0.0000 A.data=58041468407783620608.0000 loss=    inf\n",
      "            step(): A.grad=58041468407783620608.0000 A.data=39939817053567844352.0000\n",
      "Ep953: zero_grad(): A.grad= 0.0000 A.data=39939817053567844352.0000 loss=    inf\n",
      "            step(): A.grad=39939817053567844352.0000 A.data=-55857649199020507136.0000\n",
      "Ep954: zero_grad(): A.grad= 0.0000 A.data=-55857649199020507136.0000 loss=    inf\n",
      "            step(): A.grad=-55857649199020507136.0000 A.data=-55105332556970590208.0000\n",
      "Ep955: zero_grad(): A.grad= 0.0000 A.data=-55105332556970590208.0000 loss=    inf\n",
      "            step(): A.grad=-55105332556970590208.0000 A.data=50422345408505184256.0000\n",
      "Ep956: zero_grad(): A.grad= 0.0000 A.data=50422345408505184256.0000 loss=    inf\n",
      "            step(): A.grad=50422345408505184256.0000 A.data=70700339292415197184.0000\n",
      "Ep957: zero_grad(): A.grad= 0.0000 A.data=70700339292415197184.0000 loss=    inf\n",
      "            step(): A.grad=70700339292415197184.0000 A.data=-41324511211263361024.0000\n",
      "Ep958: zero_grad(): A.grad= 0.0000 A.data=-41324511211263361024.0000 loss=    inf\n",
      "            step(): A.grad=-41324511211263361024.0000 A.data=-86035280741565202432.0000\n",
      "Ep959: zero_grad(): A.grad= 0.0000 A.data=-86035280741565202432.0000 loss=    inf\n",
      "            step(): A.grad=-86035280741565202432.0000 A.data=28249892989937123328.0000\n",
      "Ep960: zero_grad(): A.grad= 0.0000 A.data=28249892989937123328.0000 loss=    inf\n",
      "            step(): A.grad=28249892989937123328.0000 A.data=100288795330192867328.0000\n",
      "Ep961: zero_grad(): A.grad= 0.0000 A.data=100288795330192867328.0000 loss=    inf\n",
      "            step(): A.grad=100288795330192867328.0000 A.data=-11017124102501564416.0000\n",
      "Ep962: zero_grad(): A.grad= 0.0000 A.data=-11017124102501564416.0000 loss=60688513885204036037856760629922103296.0000\n",
      "            step(): A.grad=-11017124102501564416.0000 A.data=-112521099683712466944.0000\n",
      "Ep963: zero_grad(): A.grad= 0.0000 A.data=-112521099683712466944.0000 loss=    inf\n",
      "            step(): A.grad=-112521099683712466944.0000 A.data=-10385388701646585856.0000\n",
      "Ep964: zero_grad(): A.grad= 0.0000 A.data=-10385388701646585856.0000 loss=53928148445994091378427660396090359808.0000\n",
      "            step(): A.grad=-10385388701646585856.0000 A.data=121696137189410209792.0000\n",
      "Ep965: zero_grad(): A.grad= 0.0000 A.data=121696137189410209792.0000 loss=    inf\n",
      "            step(): A.grad=121696137189410209792.0000 A.data=35763163805786308608.0000\n",
      "Ep966: zero_grad(): A.grad= 0.0000 A.data=35763163805786308608.0000 loss=    inf\n",
      "            step(): A.grad=35763163805786308608.0000 A.data=-126713129582114897920.0000\n",
      "Ep967: zero_grad(): A.grad= 0.0000 A.data=-126713129582114897920.0000 loss=    inf\n",
      "            step(): A.grad=-126713129582114897920.0000 A.data=-64682114019271639040.0000\n",
      "Ep968: zero_grad(): A.grad= 0.0000 A.data=-64682114019271639040.0000 loss=    inf\n",
      "            step(): A.grad=-64682114019271639040.0000 A.data=126448032930611593216.0000\n",
      "Ep969: zero_grad(): A.grad= 0.0000 A.data=126448032930611593216.0000 loss=    inf\n",
      "            step(): A.grad=126448032930611593216.0000 A.data=96439932886930423808.0000\n",
      "Ep970: zero_grad(): A.grad= 0.0000 A.data=96439932886930423808.0000 loss=    inf\n",
      "            step(): A.grad=96439932886930423808.0000 A.data=-119804854044333178880.0000\n",
      "Ep971: zero_grad(): A.grad= 0.0000 A.data=-119804854044333178880.0000 loss=    inf\n",
      "            step(): A.grad=-119804854044333178880.0000 A.data=-130044904900973821952.0000\n",
      "Ep972: zero_grad(): A.grad= 0.0000 A.data=-130044904900973821952.0000 loss=    inf\n",
      "            step(): A.grad=-130044904900973821952.0000 A.data=105776361107399639040.0000\n",
      "Ep973: zero_grad(): A.grad= 0.0000 A.data=105776361107399639040.0000 loss=    inf\n",
      "            step(): A.grad=105776361107399639040.0000 A.data=164204690482392989696.0000\n",
      "Ep974: zero_grad(): A.grad= 0.0000 A.data=164204690482392989696.0000 loss=    inf\n",
      "            step(): A.grad=164204690482392989696.0000 A.data=-83513062640098213888.0000\n",
      "Ep975: zero_grad(): A.grad= 0.0000 A.data=-83513062640098213888.0000 loss=    inf\n",
      "            step(): A.grad=-83513062640098213888.0000 A.data=-197327786132400766976.0000\n",
      "Ep976: zero_grad(): A.grad= 0.0000 A.data=-197327786132400766976.0000 loss=    inf\n",
      "            step(): A.grad=-197327786132400766976.0000 A.data=52398818714502299648.0000\n",
      "Ep977: zero_grad(): A.grad= 0.0000 A.data=52398818714502299648.0000 loss=    inf\n",
      "            step(): A.grad=52398818714502299648.0000 A.data=227540342562290139136.0000\n",
      "Ep978: zero_grad(): A.grad= 0.0000 A.data=227540342562290139136.0000 loss=    inf\n",
      "            step(): A.grad=227540342562290139136.0000 A.data=-12130621518182875136.0000\n",
      "Ep979: zero_grad(): A.grad= 0.0000 A.data=-12130621518182875136.0000 loss=73575987370978712902738480017933598720.0000\n",
      "            step(): A.grad=-12130621518182875136.0000 A.data=-252720513436685959168.0000\n",
      "Ep980: zero_grad(): A.grad= 0.0000 A.data=-252720513436685959168.0000 loss=    inf\n",
      "            step(): A.grad=-252720513436685959168.0000 A.data=-37200419017336029184.0000\n",
      "Ep981: zero_grad(): A.grad= 0.0000 A.data=-37200419017336029184.0000 loss=    inf\n",
      "            step(): A.grad=-37200419017336029184.0000 A.data=270552498569073393664.0000\n",
      "Ep982: zero_grad(): A.grad= 0.0000 A.data=270552498569073393664.0000 loss=    inf\n",
      "            step(): A.grad=270552498569073393664.0000 A.data=95030965910540124160.0000\n",
      "Ep983: zero_grad(): A.grad= 0.0000 A.data=95030965910540124160.0000 loss=    inf\n",
      "            step(): A.grad=95030965910540124160.0000 A.data=-278601557003091312640.0000\n",
      "Ep984: zero_grad(): A.grad= 0.0000 A.data=-278601557003091312640.0000 loss=    inf\n",
      "            step(): A.grad=-278601557003091312640.0000 A.data=-160254400290491465728.0000\n",
      "Ep985: zero_grad(): A.grad= 0.0000 A.data=-160254400290491465728.0000 loss=    inf\n",
      "            step(): A.grad=-160254400290491465728.0000 A.data=274410834404520755200.0000\n",
      "Ep986: zero_grad(): A.grad= 0.0000 A.data=274410834404520755200.0000 loss=    inf\n",
      "            step(): A.grad=274410834404520755200.0000 A.data=231162028311068016640.0000\n",
      "Ep987: zero_grad(): A.grad= 0.0000 A.data=231162028311068016640.0000 loss=    inf\n",
      "            step(): A.grad=231162028311068016640.0000 A.data=-255619494590573182976.0000\n",
      "Ep988: zero_grad(): A.grad= 0.0000 A.data=-255619494590573182976.0000 loss=    inf\n",
      "            step(): A.grad=-255619494590573182976.0000 A.data=-305402144134038290432.0000\n",
      "Ep989: zero_grad(): A.grad= 0.0000 A.data=-305402144134038290432.0000 loss=    inf\n",
      "            step(): A.grad=-305402144134038290432.0000 A.data=220100976520013545472.0000\n",
      "Ep990: zero_grad(): A.grad= 0.0000 A.data=220100976520013545472.0000 loss=    inf\n",
      "            step(): A.grad=220100976520013545472.0000 A.data=379962578480505290752.0000\n",
      "Ep991: zero_grad(): A.grad= 0.0000 A.data=379962578480505290752.0000 loss=    inf\n",
      "            step(): A.grad=379962578480505290752.0000 A.data=-166118509217792917504.0000\n",
      "Ep992: zero_grad(): A.grad= 0.0000 A.data=-166118509217792917504.0000 loss=    inf\n",
      "            step(): A.grad=-166118509217792917504.0000 A.data=-451182573356486492160.0000\n",
      "Ep993: zero_grad(): A.grad= 0.0000 A.data=-451182573356486492160.0000 loss=    inf\n",
      "            step(): A.grad=-451182573356486492160.0000 A.data=92493803247028404224.0000\n",
      "Ep994: zero_grad(): A.grad= 0.0000 A.data=92493803247028404224.0000 loss=    inf\n",
      "            step(): A.grad=92493803247028404224.0000 A.data=514799612452164075520.0000\n",
      "Ep995: zero_grad(): A.grad= 0.0000 A.data=514799612452164075520.0000 loss=    inf\n",
      "            step(): A.grad=514799612452164075520.0000 A.data=1216781139948077056.0000\n",
      "Ep996: zero_grad(): A.grad= 0.0000 A.data=1216781139948077056.0000 loss=740278160744180607083139499414257664.0000\n",
      "            step(): A.grad=1216781139948077056.0000 A.data=-566036220987828076544.0000\n",
      "Ep997: zero_grad(): A.grad= 0.0000 A.data=-566036220987828076544.0000 loss=    inf\n",
      "            step(): A.grad=-566036220987828076544.0000 A.data=-114545714006820126720.0000\n",
      "Ep998: zero_grad(): A.grad= 0.0000 A.data=-114545714006820126720.0000 loss=    inf\n",
      "            step(): A.grad=-114545714006820126720.0000 A.data=599730710840558485504.0000\n",
      "Ep999: zero_grad(): A.grad= 0.0000 A.data=599730710840558485504.0000 loss=    inf\n",
      "            step(): A.grad=599730710840558485504.0000 A.data=245946466278423134208.0000\n",
      "Ep1000: zero_grad(): A.grad= 0.0000 A.data=245946466278423134208.0000 loss=    inf\n",
      "            step(): A.grad=245946466278423134208.0000 A.data=-610514580148297138176.0000\n"
     ]
    }
   ],
   "source": [
    "m = MyModel().to('cpu')\n",
    "optimizer = torch.optim.SGD(m.parameters(), lr=lr, momentum=1.1)\n",
    "epoch = train(m, optimizer, epochs, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy example 2: train a two-layer NN to fit XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Minimum, loss = 0.34804069995880127\n",
      "Local Minimum, loss = 0.3483721911907196\n",
      "Local Minimum, loss = 0.4790840446949005\n",
      "Local Minimum, loss = 0.01830323413014412\n",
      "epoch=6981, Global Minimum, loss = 0.009997597895562649\n",
      "epoch=3758, Global Minimum, loss = 0.009998379275202751\n",
      "epoch=3459, Global Minimum, loss = 0.009998274967074394\n",
      "epoch=3216, Global Minimum, loss = 0.009996936656534672\n",
      "epoch=3538, Global Minimum, loss = 0.009998416528105736\n",
      "Local Minimum, loss = 0.4790799021720886\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class XORModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.in_hid = torch.nn.Linear(2, 2)\n",
    "        self.hid_out = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        l1 = torch.tanh(self.in_hid(X))\n",
    "        pred = torch.sigmoid(self.hid_out(l1))\n",
    "        return pred\n",
    "\n",
    "\n",
    "X = torch.Tensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = torch.Tensor([[0], [1], [1], [0]])\n",
    "\n",
    "xor_dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "train_loader = torch.utils.data.DataLoader(xor_dataset, batch_size=4)\n",
    "\n",
    "xor_m = XORModel().to('cpu')\n",
    "\n",
    "\n",
    "def train(lr, mon, init, epochs=10000, verbose=False):\n",
    "    # init weights\n",
    "    torch.manual_seed(random.randint(0, 100000))\n",
    "    xor_m.in_hid.weight.data.normal_(0, init)\n",
    "    xor_m.hid_out.weight.data.normal_(0, init)\n",
    "    optimizer = torch.optim.SGD(xor_m.parameters(), lr=lr, momentum=mon)\n",
    "    for epoch in range(1, epochs):\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = xor_m(x)\n",
    "            loss = F.binary_cross_entropy(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if verbose and epoch % 100 == 0:\n",
    "                print('ep%3d: loss = %7.4f' % (epoch, loss.item()))\n",
    "            if loss < 0.01:\n",
    "                print(f\"{epoch=}, Global Minimum, loss = {loss.item()}\")\n",
    "                return\n",
    "    print(f\"Local Minimum, loss = {loss.item()}\")\n",
    "\n",
    "lr = 0.1\n",
    "mon = .0\n",
    "init = 1.\n",
    "for _ in range(10):\n",
    "    train(lr, mon, init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=650, Global Minimum, loss = 0.00997321866452694\n",
      "epoch=472, Global Minimum, loss = 0.009986438788473606\n",
      "epoch=419, Global Minimum, loss = 0.009983159601688385\n",
      "epoch=429, Global Minimum, loss = 0.00999308843165636\n",
      "epoch=2459, Global Minimum, loss = 0.009996993467211723\n",
      "epoch=901, Global Minimum, loss = 0.00997856818139553\n",
      "epoch=498, Global Minimum, loss = 0.009987319819629192\n",
      "epoch=424, Global Minimum, loss = 0.00999538879841566\n",
      "epoch=422, Global Minimum, loss = 0.00997464545071125\n",
      "epoch=431, Global Minimum, loss = 0.00997532531619072\n"
     ]
    }
   ],
   "source": [
    "mon = 0.9\n",
    "init = 0.01\n",
    "for _ in range(10):\n",
    "    train(lr, mon, init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('COMP9417')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2d5407234b8938a089e01a60a45d2aae9d8d06e85dfd57bc61d76669c88389c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
